{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, csv\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import xlsxwriter\n",
    "import random\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import unicodedata as udata\n",
    "# import pause, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from distutils.dir_util import copy_tree\n",
    "import sklearn\n",
    "from sklearn.metrics import *\n",
    "import itertools as it\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import functools\n",
    "import spacy\n",
    "\n",
    "import tensorflow.keras.preprocessing.text as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_1 = (stopwords.words('english')) #Stopword\n",
    "with open('data/preprocess/stop_words.txt') as f:\n",
    "    stop_words_2 = f.read().splitlines() #stop_list1\n",
    "stop_words_3 = pickle.load(open('data/preprocess/stop_list2.pkl','rb'))\n",
    "stop_words_all = set(stop_words_1 + stop_words_2 + stop_words_3)\n",
    "len(stop_words_all) # stopwords#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', \"'s\", '(', 'beautiful', ')', '4toys', 'does', \"n't\"]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenizer = spacy.load(\"en\")\n",
    "kk = sp_tokenizer(\"It's (beautiful) 4toys doesn't\")\n",
    "token_li=[]\n",
    "for token in kk:\n",
    "    token_li.append(token.text)\n",
    "token_li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196009"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenizer = spacy.load(\"en\")\n",
    "dil= r\"[!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~]+\\ *\" \n",
    "dil_filter = '!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_\\`{|}~\\t\\n'\n",
    "glove = pd.read_table('./data/preprocess/glove.840B.300d.txt', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "glove_words = set(glove.index.tolist())\n",
    "len(glove_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprcess(text,tokenizers='nltk'):\n",
    "    '''\n",
    "    Input: string\n",
    "    Return: preprocessed list\n",
    "    '''\n",
    "    tokens = [i for i in text.split() if i not in stop_words_all]\n",
    "    tokens_str = ' '.join(tokens)\n",
    "    if tokenizers == 'keras':\n",
    "        tokens = T.text_to_word_sequence(tokens_str,filters=dil_filter) # lower casse & tokenize\n",
    "        tokens = list(filter(None, tokens))\n",
    "    elif tokenizers == 'nltk':\n",
    "#         tokens = re.sub(dil,\" \",tokens_str.lower())\n",
    "        tokens = [i for i in tokens_str.lower() if i not in dil_filter]\n",
    "        tokens = ''.join(tokens)\n",
    "        tokens = word_tokenize(tokens)\n",
    "        tokens = list(filter(None, tokens))\n",
    "#         tokens = [x.lower() for x in tokens]\n",
    "    elif tokenizers == 'spacy':\n",
    "#         tokens = re.sub(dil,\" \",tokens_str.lower())\n",
    "        tokens = [i for i in tokens_str.lower() if i not in dil_filter]\n",
    "        tokens = ''.join(tokens)\n",
    "        tokens = sp_tokenizer(tokens)\n",
    "        token_li = []\n",
    "        for token in tokens:\n",
    "            token_li.append(token.text)\n",
    "        tokens = list(filter(None, token_li))\n",
    "#         tokens = [x.lower() for x in token_li]\n",
    "    tokens_li = []\n",
    "    for token in tokens:\n",
    "        tokens_li.append(''.join([i for i in token if not i.isdigit()]))\n",
    "    tokens_str = ' '.join(tokens_li)\n",
    "    tokens_str = re.sub(dil,\" \",tokens_str.lower())\n",
    "    tokens_li = tokens_str.split()\n",
    "    tokens_li = list(filter(None, tokens_li))\n",
    "    ori_tokens_num = len(tokens_li)\n",
    "    tokens_li = [i for i in tokens_li if i in glove_words]\n",
    "    try:\n",
    "        rate = len(tokens_li)/ori_tokens_num\n",
    "    except ZeroDivisionError:\n",
    "        rate = np.nan\n",
    "    return tokens_li , rate#ori_tokens_num/len(tokens_li)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:53<00:00, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595051529116455 0.959558811893717 0.9605178211130393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>original_tokenize_num</th>\n",
       "      <th>text_keras</th>\n",
       "      <th>rate_keras</th>\n",
       "      <th>text_nltk</th>\n",
       "      <th>rate_nltk</th>\n",
       "      <th>text_apacy</th>\n",
       "      <th>rate_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19305</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>My votes (FWIW): Team MVP: Pat Verbeek. He fan...</td>\n",
       "      <td>453</td>\n",
       "      <td>[my, votes, fwiw, team, mvp, pat, he, fans, go...</td>\n",
       "      <td>0.950207</td>\n",
       "      <td>[my, votes, fwiw, team, mvp, pat, he, fans, go...</td>\n",
       "      <td>0.950207</td>\n",
       "      <td>[my, votes, fwiw, team, mvp, pat, he, fans, go...</td>\n",
       "      <td>0.950207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19313</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Kovalev is too talented a player to play for R...</td>\n",
       "      <td>160</td>\n",
       "      <td>[kovalev, talented, player, play, roger, niels...</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>[kovalev, talented, player, play, roger, needs...</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>[kovalev, talented, player, play, roger, needs...</td>\n",
       "      <td>0.938272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19122</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>New Jersey 1 0 2--3 Pittsburgh 2 3 1--6 First ...</td>\n",
       "      <td>919</td>\n",
       "      <td>[new, jersey, pittsburgh, first, period, pitts...</td>\n",
       "      <td>0.867299</td>\n",
       "      <td>[new, jersey, pittsburgh, first, period, pitts...</td>\n",
       "      <td>0.867299</td>\n",
       "      <td>[new, jersey, pittsburgh, first, period, pitts...</td>\n",
       "      <td>0.867299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19312</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>First of all, the Penguins WILL win the cup ag...</td>\n",
       "      <td>66</td>\n",
       "      <td>[first, all, penguins, will, win, cup, again, ...</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>[first, all, penguins, will, win, cup, again, ...</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>[first, all, penguins, will, win, cup, again, ...</td>\n",
       "      <td>0.981481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>19115</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>You can't. But good luck trying.</td>\n",
       "      <td>6</td>\n",
       "      <td>[you, can't, but, good, luck, trying]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[you, ca, n't, but, good, luck, trying]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[you, ca, n't, but, good, luck, trying]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>20797</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>: When the object of their belief is said to b...</td>\n",
       "      <td>269</td>\n",
       "      <td>[when, object, belief, said, perfect, make, be...</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>[when, object, belief, said, perfect, make, be...</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>[when, object, belief, said, perfect, make, be...</td>\n",
       "      <td>0.978102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>20537</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Koff! You mean that as long as I put you to sl...</td>\n",
       "      <td>23</td>\n",
       "      <td>[koff, you, mean, long, i, sleep, first, i, ki...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[koff, you, mean, long, i, sleep, first, i, ki...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[koff, you, mean, long, i, sleep, first, i, ki...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>20757</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>: Mr Connor's assertion that \"more complex\" ==...</td>\n",
       "      <td>135</td>\n",
       "      <td>[mr, assertion, more, complex, later, paleonto...</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>[mr, connor, 's, assertion, more, complex, lat...</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>[mr, connor, 's, assertion, more, complex, lat...</td>\n",
       "      <td>0.985915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>20677</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>(excess stuff deleted...) I know of a similar ...</td>\n",
       "      <td>139</td>\n",
       "      <td>[excess, stuff, deleted, i, know, similar, inc...</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>[excess, stuff, deleted, i, know, similar, inc...</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>[excess, stuff, deleted, i, know, similar, inc...</td>\n",
       "      <td>0.987952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>20719</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>The \"System\" refered to a \"moral system\". You ...</td>\n",
       "      <td>40</td>\n",
       "      <td>[the, system, refered, moral, system, you, sho...</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>[the, system, refered, moral, system, you, hav...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[the, system, refered, moral, system, you, sho...</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id          category  \\\n",
       "0      19305  rec.sport.hockey   \n",
       "1      19313  rec.sport.hockey   \n",
       "2      19122  rec.sport.hockey   \n",
       "3      19312  rec.sport.hockey   \n",
       "4      19115  rec.sport.hockey   \n",
       "...      ...               ...   \n",
       "11309  20797       alt.atheism   \n",
       "11310  20537       alt.atheism   \n",
       "11311  20757       alt.atheism   \n",
       "11312  20677       alt.atheism   \n",
       "11313  20719       alt.atheism   \n",
       "\n",
       "                                                    text  \\\n",
       "0      My votes (FWIW): Team MVP: Pat Verbeek. He fan...   \n",
       "1      Kovalev is too talented a player to play for R...   \n",
       "2      New Jersey 1 0 2--3 Pittsburgh 2 3 1--6 First ...   \n",
       "3      First of all, the Penguins WILL win the cup ag...   \n",
       "4                       You can't. But good luck trying.   \n",
       "...                                                  ...   \n",
       "11309  : When the object of their belief is said to b...   \n",
       "11310  Koff! You mean that as long as I put you to sl...   \n",
       "11311  : Mr Connor's assertion that \"more complex\" ==...   \n",
       "11312  (excess stuff deleted...) I know of a similar ...   \n",
       "11313  The \"System\" refered to a \"moral system\". You ...   \n",
       "\n",
       "      original_tokenize_num  \\\n",
       "0                       453   \n",
       "1                       160   \n",
       "2                       919   \n",
       "3                        66   \n",
       "4                         6   \n",
       "...                     ...   \n",
       "11309                   269   \n",
       "11310                    23   \n",
       "11311                   135   \n",
       "11312                   139   \n",
       "11313                    40   \n",
       "\n",
       "                                              text_keras  rate_keras  \\\n",
       "0      [my, votes, fwiw, team, mvp, pat, he, fans, go...    0.950207   \n",
       "1      [kovalev, talented, player, play, roger, niels...    0.951220   \n",
       "2      [new, jersey, pittsburgh, first, period, pitts...    0.867299   \n",
       "3      [first, all, penguins, will, win, cup, again, ...    0.981481   \n",
       "4                  [you, can't, but, good, luck, trying]    1.000000   \n",
       "...                                                  ...         ...   \n",
       "11309  [when, object, belief, said, perfect, make, be...    0.930233   \n",
       "11310  [koff, you, mean, long, i, sleep, first, i, ki...    1.000000   \n",
       "11311  [mr, assertion, more, complex, later, paleonto...    0.971014   \n",
       "11312  [excess, stuff, deleted, i, know, similar, inc...    0.987805   \n",
       "11313  [the, system, refered, moral, system, you, sho...    0.950000   \n",
       "\n",
       "                                               text_nltk  rate_nltk  \\\n",
       "0      [my, votes, fwiw, team, mvp, pat, he, fans, go...   0.950207   \n",
       "1      [kovalev, talented, player, play, roger, needs...   0.938272   \n",
       "2      [new, jersey, pittsburgh, first, period, pitts...   0.867299   \n",
       "3      [first, all, penguins, will, win, cup, again, ...   0.981481   \n",
       "4                [you, ca, n't, but, good, luck, trying]   1.000000   \n",
       "...                                                  ...        ...   \n",
       "11309  [when, object, belief, said, perfect, make, be...   0.978102   \n",
       "11310  [koff, you, mean, long, i, sleep, first, i, ki...   1.000000   \n",
       "11311  [mr, connor, 's, assertion, more, complex, lat...   0.985915   \n",
       "11312  [excess, stuff, deleted, i, know, similar, inc...   0.987952   \n",
       "11313  [the, system, refered, moral, system, you, hav...   1.000000   \n",
       "\n",
       "                                              text_apacy  rate_spacy  \n",
       "0      [my, votes, fwiw, team, mvp, pat, he, fans, go...    0.950207  \n",
       "1      [kovalev, talented, player, play, roger, needs...    0.938272  \n",
       "2      [new, jersey, pittsburgh, first, period, pitts...    0.867299  \n",
       "3      [first, all, penguins, will, win, cup, again, ...    0.981481  \n",
       "4                [you, ca, n't, but, good, luck, trying]    1.000000  \n",
       "...                                                  ...         ...  \n",
       "11309  [when, object, belief, said, perfect, make, be...    0.978102  \n",
       "11310  [koff, you, mean, long, i, sleep, first, i, ki...    1.000000  \n",
       "11311  [mr, connor, 's, assertion, more, complex, lat...    0.985915  \n",
       "11312  [excess, stuff, deleted, i, know, similar, inc...    0.987952  \n",
       "11313  [the, system, refered, moral, system, you, sho...    0.950000  \n",
       "\n",
       "[11314 rows x 10 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df = pd.DataFrame(columns=['id','category','text','original_tokenize_num','preprocess_text','preprocss_tokenize_num'])\n",
    "train_df = pd.DataFrame(columns=['id','category','text','original_tokenize_num',\n",
    "                                 'text_keras','rate_keras','text_nltk','rate_nltk','text_apacy','rate_spacy'])\n",
    "train_dir = './data/20news-bydate-v3/20news-bydate-train/'\n",
    "cat_dir = next(os.walk(train_dir))[1]\n",
    "count = 0\n",
    "for cat in tqdm(cat_dir):\n",
    "    in_dir = train_dir + cat + '/'\n",
    "    news_list = next(os.walk(in_dir))[2]\n",
    "    news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "    news_list = list(filter(lambda f: f.endswith(\".txt\"), news_list))\n",
    "    for news_path in news_list:\n",
    "        id_num = news_path.split('/')[-1].split('.')[0]\n",
    "        with open(news_path,'r',encoding='latin1') as f:\n",
    "            news_text = f.read()\n",
    "            news_text = re.sub('\\n',' ',news_text)\n",
    "            news_text = re.sub('\\t',' ',news_text)\n",
    "            news_text_tok_ori = T.text_to_word_sequence(news_text,filters='',lower=False)\n",
    "            news_text_ori = \" \".join(news_text_tok_ori)\n",
    "            news_text_tok_num_ori = len(news_text_tok_ori)\n",
    "            news_text_tok_keras,rate_keras = text_preprcess(news_text,tokenizers='keras')\n",
    "            news_text_tok_nltk,rate_nltk = text_preprcess(news_text,tokenizers='nltk')\n",
    "            news_text_tok_spacy,rate_spacy = text_preprcess(news_text,tokenizers='spacy')\n",
    "        train_df.loc[count] = [id_num,cat,news_text_ori,news_text_tok_num_ori,\n",
    "                               news_text_tok_keras,rate_keras,news_text_tok_nltk,rate_nltk,news_text_tok_spacy,rate_spacy]\n",
    "        count+=1\n",
    "#         break\n",
    "    \n",
    "# train_df.loc[0] = [0,0,0]\n",
    "print(train_df.rate_keras.mean() , train_df.rate_nltk.mean(),train_df.rate_spacy.mean()  )\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* keras tokenizer: 95.95% tokens in glove\n",
    "* nltk tokenizer: 95.96% tokens in glove\n",
    "* spacy tokenizer: 96.05% tokens in glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 42.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>original_tokenize_num</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>preprocss_tokenize_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19305</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>My votes (FWIW): Team MVP: Pat Verbeek. He fan...</td>\n",
       "      <td>453</td>\n",
       "      <td>my votes fwiw team mvp pat he fans goal mouth ...</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21002</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>I have posted disp135.zip to alt.binaries.pict...</td>\n",
       "      <td>1447</td>\n",
       "      <td>i posted you distribute program freely noncomm...</td>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>21586</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>Why don't you call the City and ask? Oak Park ...</td>\n",
       "      <td>327</td>\n",
       "      <td>why city ask oak park illegal handgun ban well...</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>23146</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>-&gt; &gt;Now let me get this straight. After a nice...</td>\n",
       "      <td>301</td>\n",
       "      <td>now let straight after nice long rant how peop...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>23229</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>NEWS YOU MAY HAVE MISSED, APR 19, 1993 Not bec...</td>\n",
       "      <td>615</td>\n",
       "      <td>news you may have missed apr not busy us media...</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>19586</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>Is this the one that had the {wrench|pliers} f...</td>\n",
       "      <td>12</td>\n",
       "      <td>is inside recovery</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>22675</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>Hmm, followup on my own posting... Well, who c...</td>\n",
       "      <td>973</td>\n",
       "      <td>hmm followup posting well cares first let try ...</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>27525</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>: I'm looking to buy a 17\" monitor soon, and i...</td>\n",
       "      <td>111</td>\n",
       "      <td>i 'm looking buy monitor soon i ca n't decide ...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>26073</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>Methinks you recall wrong. Mitchell hit close ...</td>\n",
       "      <td>84</td>\n",
       "      <td>methinks recall wrong mitchell hit close atlan...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>28486</td>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>Fortunately, wire-wrapping is a better wiring ...</td>\n",
       "      <td>125</td>\n",
       "      <td>fortunately wirewrapping better wiring techniq...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>25314</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>Forsale: SONY MHC-3600 HI-FI Bookshelf stereo ...</td>\n",
       "      <td>120</td>\n",
       "      <td>forsale sony mhc hifi bookshelf stereo months ...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>17681</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>Paraphrase of initial post: \\tCan I fight a sp...</td>\n",
       "      <td>49</td>\n",
       "      <td>paraphrase initial post can i fight speeding t...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>25854</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>Please, please don't make Barney to a modern m...</td>\n",
       "      <td>33</td>\n",
       "      <td>please make barney modern mythical figure i de...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>23931</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>A relative of mine has recently been diagnosed...</td>\n",
       "      <td>43</td>\n",
       "      <td>a relative recently diagnosed stage papillary ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>26597</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>18684</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>I apologize if this post isn't entirely approp...</td>\n",
       "      <td>52</td>\n",
       "      <td>i apologize post entirely appropriate newsgrou...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>20307</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>I have heard of two packages for the PC that s...</td>\n",
       "      <td>52</td>\n",
       "      <td>i heard packages pc support xwin the linux fre...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>24375</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>What exactly does the windows bitmap format lo...</td>\n",
       "      <td>43</td>\n",
       "      <td>what exactly windows bitmap format look like i...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>28065</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>I'm wondering if anybody else out there is a c...</td>\n",
       "      <td>166</td>\n",
       "      <td>i 'm wondering anybody clutchless shifter i 'v...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>20556</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Just what do gay people do that straight peopl...</td>\n",
       "      <td>128</td>\n",
       "      <td>just gay people straight people do n't absolut...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                  category  \\\n",
       "0   19305          rec.sport.hockey   \n",
       "1   21002             comp.graphics   \n",
       "2   21586        talk.politics.guns   \n",
       "3   23146        talk.politics.misc   \n",
       "4   23229     talk.politics.mideast   \n",
       "5   19586                 sci.space   \n",
       "6   22675                 sci.crypt   \n",
       "7   27525  comp.sys.ibm.pc.hardware   \n",
       "8   26073        rec.sport.baseball   \n",
       "9   28486           sci.electronics   \n",
       "10  25314              misc.forsale   \n",
       "11  17681           rec.motorcycles   \n",
       "12  25854        talk.religion.misc   \n",
       "13  23931                   sci.med   \n",
       "14  26597     comp.sys.mac.hardware   \n",
       "15  18684    soc.religion.christian   \n",
       "16  20307            comp.windows.x   \n",
       "17  24375   comp.os.ms-windows.misc   \n",
       "18  28065                 rec.autos   \n",
       "19  20556               alt.atheism   \n",
       "\n",
       "                                                 text original_tokenize_num  \\\n",
       "0   My votes (FWIW): Team MVP: Pat Verbeek. He fan...                   453   \n",
       "1   I have posted disp135.zip to alt.binaries.pict...                  1447   \n",
       "2   Why don't you call the City and ask? Oak Park ...                   327   \n",
       "3   -> >Now let me get this straight. After a nice...                   301   \n",
       "4   NEWS YOU MAY HAVE MISSED, APR 19, 1993 Not bec...                   615   \n",
       "5   Is this the one that had the {wrench|pliers} f...                    12   \n",
       "6   Hmm, followup on my own posting... Well, who c...                   973   \n",
       "7   : I'm looking to buy a 17\" monitor soon, and i...                   111   \n",
       "8   Methinks you recall wrong. Mitchell hit close ...                    84   \n",
       "9   Fortunately, wire-wrapping is a better wiring ...                   125   \n",
       "10  Forsale: SONY MHC-3600 HI-FI Bookshelf stereo ...                   120   \n",
       "11  Paraphrase of initial post: \\tCan I fight a sp...                    49   \n",
       "12  Please, please don't make Barney to a modern m...                    33   \n",
       "13  A relative of mine has recently been diagnosed...                    43   \n",
       "14                                                                        0   \n",
       "15  I apologize if this post isn't entirely approp...                    52   \n",
       "16  I have heard of two packages for the PC that s...                    52   \n",
       "17  What exactly does the windows bitmap format lo...                    43   \n",
       "18  I'm wondering if anybody else out there is a c...                   166   \n",
       "19  Just what do gay people do that straight peopl...                   128   \n",
       "\n",
       "                                      preprocess_text preprocss_tokenize_num  \n",
       "0   my votes fwiw team mvp pat he fans goal mouth ...                    229  \n",
       "1   i posted you distribute program freely noncomm...                    912  \n",
       "2   why city ask oak park illegal handgun ban well...                    180  \n",
       "3   now let straight after nice long rant how peop...                    150  \n",
       "4   news you may have missed apr not busy us media...                    368  \n",
       "5                                  is inside recovery                      3  \n",
       "6   hmm followup posting well cares first let try ...                    481  \n",
       "7   i 'm looking buy monitor soon i ca n't decide ...                     56  \n",
       "8   methinks recall wrong mitchell hit close atlan...                     39  \n",
       "9   fortunately wirewrapping better wiring techniq...                     71  \n",
       "10  forsale sony mhc hifi bookshelf stereo months ...                     80  \n",
       "11  paraphrase initial post can i fight speeding t...                     35  \n",
       "12  please make barney modern mythical figure i de...                     17  \n",
       "13  a relative recently diagnosed stage papillary ...                     23  \n",
       "14                                                                         0  \n",
       "15  i apologize post entirely appropriate newsgrou...                     26  \n",
       "16  i heard packages pc support xwin the linux fre...                     23  \n",
       "17  what exactly windows bitmap format look like i...                     24  \n",
       "18  i 'm wondering anybody clutchless shifter i 'v...                     96  \n",
       "19  just gay people straight people do n't absolut...                     76  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(columns=['id','category','text','original_tokenize_num','preprocess_text','preprocss_tokenize_num'])\n",
    "train_dir = './data/20news-bydate-v3/20news-bydate-train/'\n",
    "cat_dir = next(os.walk(train_dir))[1]\n",
    "count = 0\n",
    "for cat in tqdm(cat_dir):\n",
    "    in_dir = train_dir + cat + '/'\n",
    "    news_list = next(os.walk(in_dir))[2]\n",
    "    news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "    news_list = list(filter(lambda f: f.endswith(\".txt\"), news_list))\n",
    "    for news_path in news_list:\n",
    "        id_num = news_path.split('/')[-1].split('.')[0]\n",
    "        with open(news_path,'r',encoding='latin1') as f:\n",
    "            news_text = f.read()\n",
    "            news_text = re.sub('\\n',' ',news_text)\n",
    "            news_text_tok_ori = T.text_to_word_sequence(news_text,filters='',lower=False)\n",
    "            news_text_ori = \" \".join(news_text_tok_ori)\n",
    "            news_text_tok_num_ori = len(news_text_tok_ori)\n",
    "#             news_text_tok_pre,rate_keras = text_preprcess(news_text,tokenizers='keras')\n",
    "#             news_text_tok_pre,rate_nltk = text_preprcess(news_text,tokenizers='nltk')\n",
    "            news_text_tok_pre,rate_spacy = text_preprcess(news_text,tokenizers='spacy')\n",
    "            news_text_pre = \" \".join(news_text_tok_pre)\n",
    "        train_df.loc[count] = [id_num,cat,news_text_ori,news_text_tok_num_ori,news_text_pre,len(news_text_tok_pre)]\n",
    "        count+=1\n",
    "        break\n",
    "    \n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprcess2(text):\n",
    "    '''\n",
    "    Goal: filter out punctuations、words not in GloVe\n",
    "    * cased\n",
    "    * spacy\n",
    "    * for RoBERTa + GloVe embedding\n",
    "    Input: text string\n",
    "    Return: preprcessed text string\n",
    "    '''\n",
    "    tokens = [i for i in text if i not in dil_filter] #濾掉符號\n",
    "    tokens = sp_tokenizer(tokens)\n",
    "    token_li = []\n",
    "    for token in tokens:\n",
    "        token_li.append(token.text)\n",
    "    tokens_li = [i for i in token_li if i in glove_words] #只留有在glove的字\n",
    "    tokens = list(filter(None, tokens_li))\n",
    "    tokens_str = ' '.join(tokens)\n",
    "    tokens_str = re.sub(dil,\" \",tokens_str) #在確認濾一次符號\n",
    "    return tokens_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My votes (FWIW): Team MVP: Pat Verbeek. He fans on 25% of goal mouth feeds, but he still has 36 goals after a terrible start and has been an examplary (sp?) team captain throughout a tough couple of seasons. Honorable mention: Nick Kypreos and Mark Janssens. Probably more appropriate in the unsung heroes category than MVP, but Kypreos (17 goals, 320+ PIM) has been the hardest working player on the team and Janssens is underrated as a defensive center and checker. I guess I place a greater emphasis on hard work than skill when determining value. Biggest surprise: Geoff Sanderson. He had 13 goals and 31 points last season as a center, then moved to left wing and has so far put up 45 goals and 80+ points. He now has a new Whaler record 21 power play goals, most all coming from the right wing faceoff circle, his garden spot. Honorable mention: Andrew Cassels and Terry Yake. The kiddie quartet of Sanderson, Poulin, Nylander, and Petrovicky have been attracting the most attention, but Cassels is just 23 and will score close to 90 points this season. He has quite nicely assumed the role of number one center on the team and works very well with Sanderson. Yake bounced around the minors for a number of seasons but is still 24 and will put up about 20 goals and 50 points this season. Yake, like Sanderson, started performing better offensively once he was converted from center to wing, although lefty Sanderson went to the left wing and righty Yake went to the right side. Biggest disappointment: Hands down, John Cullen. Cullen had a disasterous 77 point season last year, his first full season after The Trade. Cullen started the season off of summer back surgery, and fell flat on his face (appropriate, since he spent all of his Whaler career flat on his ass, and whining about it). Cullen scored just 9 point on 19 games, was a clubhouse malcontent, commanded the powerplay to a 9% success percentage (>21% with Sanderson), and sulked his way out of town. Worst of all, his 4 year, $4M contract had three years left to run, so no one would give up any more than the 2nd round draft pick the Maple Leafs offered to Hartford. Honorable mention: Steve Konroyd, also subpar after signing a 3 year, $2,1M contract; Eric Weinrich, who showed flashes of competence, but overall has played poorly; Jim McKenzie, who was a much better hockey player two seasons ago than he is now; and Frank Pietrangelo, who only seemed to play well when Sean Burke was out for an extended period and he got to make a number of starts in a row.'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[0,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(all_length):\n",
    "    '''\n",
    "    input: length list of elements e.g.[1,1,1,3,5,9,4,2,1,3,54,78,5...]\n",
    "    output1: mean、std、mode、min、q1、median(q2)、q3、max、iqr、outlier、far out\n",
    "    output2: statistics graph、10%~90% form\n",
    "    '''\n",
    "    stat_dict = {}\n",
    "    stat_dict['mean'] = np.mean(all_length)\n",
    "    stat_dict['std'] = np.std(all_length)\n",
    "    stat_dict['mode'] = np.argmax(np.bincount(all_length))\n",
    "    stat_dict['min'] = np.min(all_length)\n",
    "    stat_dict['q1'] = np.quantile(all_length,0.25)\n",
    "    stat_dict['median'] = np.quantile(all_length,0.5)\n",
    "    stat_dict['q3'] = np.quantile(all_length,0.75)\n",
    "    stat_dict['max'] = np.max(all_length)\n",
    "    stat_dict['iqr'] = stat_dict['q3'] - stat_dict['q1']\n",
    "    stat_dict['outlier'] = stat_dict['q3'] + 1.5*stat_dict['iqr']\n",
    "    stat_dict['far_out'] = stat_dict['q3'] + 3*stat_dict['iqr']\n",
    "    for i in [10,20,30,40,50,60,70,80,90,100]:\n",
    "        stat_dict[str(i)+'%'] = np.percentile(all_length,i)\n",
    "    return pd.DataFrame.from_dict(stat_dict,orient='index',columns=['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>144.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>214.881008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mode</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q1</td>\n",
       "      <td>23.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>median</td>\n",
       "      <td>63.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q3</td>\n",
       "      <td>157.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>912.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iqr</td>\n",
       "      <td>133.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>outlier</td>\n",
       "      <td>358.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>far_out</td>\n",
       "      <td>558.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10%</td>\n",
       "      <td>15.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20%</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30%</td>\n",
       "      <td>25.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40%</td>\n",
       "      <td>37.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>63.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60%</td>\n",
       "      <td>77.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70%</td>\n",
       "      <td>112.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80%</td>\n",
       "      <td>189.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90%</td>\n",
       "      <td>379.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100%</td>\n",
       "      <td>912.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "mean     144.450000\n",
       "std      214.881008\n",
       "mode      23.000000\n",
       "min        0.000000\n",
       "q1        23.750000\n",
       "median    63.500000\n",
       "q3       157.500000\n",
       "max      912.000000\n",
       "iqr      133.750000\n",
       "outlier  358.125000\n",
       "far_out  558.750000\n",
       "10%       15.600000\n",
       "20%       23.000000\n",
       "30%       25.400000\n",
       "40%       37.400000\n",
       "50%       63.500000\n",
       "60%       77.600000\n",
       "70%      112.200000\n",
       "80%      189.800000\n",
       "90%      379.300000\n",
       "100%     912.000000"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_num_li = train_df.preprocss_tokenize_num.tolist()\n",
    "basic_statistics(tokens_num_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just',\n",
       " 'what',\n",
       " 'do',\n",
       " 'gay',\n",
       " 'people',\n",
       " 'do',\n",
       " 'that',\n",
       " 'straight',\n",
       " 'people',\n",
       " \"don't?\",\n",
       " 'absolutely',\n",
       " 'nothing.',\n",
       " \"i'm\",\n",
       " 'a',\n",
       " 'very',\n",
       " 'straight(as',\n",
       " 'an',\n",
       " 'arrow),',\n",
       " '17-year',\n",
       " 'old',\n",
       " 'male',\n",
       " 'that',\n",
       " 'is',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bsa.',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'care',\n",
       " 'what',\n",
       " 'gay',\n",
       " 'people',\n",
       " 'do',\n",
       " 'among',\n",
       " 'each',\n",
       " 'other,',\n",
       " 'as',\n",
       " 'long',\n",
       " 'as',\n",
       " 'they',\n",
       " \"don't\",\n",
       " 'make',\n",
       " 'passes',\n",
       " 'at',\n",
       " 'me',\n",
       " 'or',\n",
       " 'anything.',\n",
       " 'at',\n",
       " 'my',\n",
       " 'summer',\n",
       " 'camp',\n",
       " 'where',\n",
       " 'i',\n",
       " 'work,',\n",
       " 'my',\n",
       " 'boss',\n",
       " 'is',\n",
       " 'gay.',\n",
       " 'not',\n",
       " 'in',\n",
       " 'a',\n",
       " \"'pansy'\",\n",
       " 'way',\n",
       " 'of',\n",
       " 'gay',\n",
       " '(i',\n",
       " 'know',\n",
       " 'a',\n",
       " 'few),',\n",
       " 'but',\n",
       " 'just',\n",
       " \"'one\",\n",
       " 'of',\n",
       " 'the',\n",
       " \"guys'.\",\n",
       " 'he',\n",
       " \"doesn't\",\n",
       " 'push',\n",
       " 'anything',\n",
       " 'on',\n",
       " 'me,',\n",
       " 'and',\n",
       " 'we',\n",
       " 'give',\n",
       " 'him',\n",
       " 'the',\n",
       " 'same',\n",
       " 'respect',\n",
       " 'back,',\n",
       " 'due',\n",
       " 'to',\n",
       " 'his',\n",
       " 'position.',\n",
       " 'if',\n",
       " 'anything,',\n",
       " 'the',\n",
       " 'bsa',\n",
       " 'has',\n",
       " 'taught',\n",
       " 'me,',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know,',\n",
       " 'tolerance',\n",
       " 'or',\n",
       " 'something.',\n",
       " 'before',\n",
       " 'i',\n",
       " 'met',\n",
       " 'this',\n",
       " 'guy,',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'all',\n",
       " 'gays',\n",
       " 'were',\n",
       " \"'faries'.\",\n",
       " 'so,',\n",
       " 'the',\n",
       " 'bsa',\n",
       " 'has',\n",
       " 'taught',\n",
       " 'me',\n",
       " 'to',\n",
       " 'be',\n",
       " 'an',\n",
       " 'antibigot.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/20news-bydate-v3/20news-bydate-train/alt.atheism/20556.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20748.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20810.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20574.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20692.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20705.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20519.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20565.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20835.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20649.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20684.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20872.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20739.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20547.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20578.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20704.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20932.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20528.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20769.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20869.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20549.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20776.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20936.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20980.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20780.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20515.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20575.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20766.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20750.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20562.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20700.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20594.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20972.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20801.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20910.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20756.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20883.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20539.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20687.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20852.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20945.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20734.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20865.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20694.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20897.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20833.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20966.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20808.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20816.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20793.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20893.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20926.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20975.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20914.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20952.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20913.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20879.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20925.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20666.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20958.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20760.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20881.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20640.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20834.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20600.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20727.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20842.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20768.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20829.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20871.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20741.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20886.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20927.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20652.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20679.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20586.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20527.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20976.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20613.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20794.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20559.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20595.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20959.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20744.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20541.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20717.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20589.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20725.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20526.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20598.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20802.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20674.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20901.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20667.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20538.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20577.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20693.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20853.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20938.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20545.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20823.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20859.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20803.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20921.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20861.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20832.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20622.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20680.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20696.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20988.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20747.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20805.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20627.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20858.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20977.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20686.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20800.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20664.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20792.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20698.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20795.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20770.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20688.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20683.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20807.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20531.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20764.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20553.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20954.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20827.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20767.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20916.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20896.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20782.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20970.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20939.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20955.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20894.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20646.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20951.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20819.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20946.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20979.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20905.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20847.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20864.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20570.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20611.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20870.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20779.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20635.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20900.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20665.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20580.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20670.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20840.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20787.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20962.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20701.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20542.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20956.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20737.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20612.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20743.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20522.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20918.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20618.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20814.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20763.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20851.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20728.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20662.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20745.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20517.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20609.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20676.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20839.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20848.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20891.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20825.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20950.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20758.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20911.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20751.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20902.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20775.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20548.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20535.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20623.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20604.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20762.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20566.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20690.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20711.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20552.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20655.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20581.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20984.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20681.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20607.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20788.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20532.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20774.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20544.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20678.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20812.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20981.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20944.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20536.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20620.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20647.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20785.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20804.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20878.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20843.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20659.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20935.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20672.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20551.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20629.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20703.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20986.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20720.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20621.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20749.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20917.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20746.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20930.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20707.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20880.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20657.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20540.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20671.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20656.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20636.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20815.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20772.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20862.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20963.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20850.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20663.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20571.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20799.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20860.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20929.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20601.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20875.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20854.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20765.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20989.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20637.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20985.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20948.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20730.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20715.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20919.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20863.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20983.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20721.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20754.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20633.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20709.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20759.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20806.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20550.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20777.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20937.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20895.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20661.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20658.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20568.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20898.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20723.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20753.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20731.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20824.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20738.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20923.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20564.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20884.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20726.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20903.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20610.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20855.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20874.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20922.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20521.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20625.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20596.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20836.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20887.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20933.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20818.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20890.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20991.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20628.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20543.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20994.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20591.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20642.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20826.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20588.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20654.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20987.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20846.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20668.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20561.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20587.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20602.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20899.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20811.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20648.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20634.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20892.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20572.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20554.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20724.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20638.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20789.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20632.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20906.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20590.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20877.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20742.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20710.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20631.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20817.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20888.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20830.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20771.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20733.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20518.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20904.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20828.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20982.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20974.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20555.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20920.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20856.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20644.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20606.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20608.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20845.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20524.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20821.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20791.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20953.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20689.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20708.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20626.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20643.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20691.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20786.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20943.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20530.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20809.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20957.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20908.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20978.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20928.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20716.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20961.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20605.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20857.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20889.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20660.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20712.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20567.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20624.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20844.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20702.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20778.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20569.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20560.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20582.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20729.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20682.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20735.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20614.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20573.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20558.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20973.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20713.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20740.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20675.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20736.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20523.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20617.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20706.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20576.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20525.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20968.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20866.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20603.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20534.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20516.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20924.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20971.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20761.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20599.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20650.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20820.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20563.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20990.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20597.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20585.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20755.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20868.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20992.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20949.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20876.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20993.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20615.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20630.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20645.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20533.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20867.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20685.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20639.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20907.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20882.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20813.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20695.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20641.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20838.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20722.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20699.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20967.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20520.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20781.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20885.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20837.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20960.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20796.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20841.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20969.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20546.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20557.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20942.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20784.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20912.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20790.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20940.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20529.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20909.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20941.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20583.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20752.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20965.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20593.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20915.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20616.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20592.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20653.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20697.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20714.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20579.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20831.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20783.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20773.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20873.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20651.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20931.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20947.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20718.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20798.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20669.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20619.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20732.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20584.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20673.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20964.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20822.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20934.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20849.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20797.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20537.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20757.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20677.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20719.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_list = next(os.walk(in_dir))[2]\n",
    "news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/leoqaz12/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/leoqaz12/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "| dictionary: 50264 types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (decoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerSentenceEncoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\n",
    "roberta.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ori = \"now let straight after nice long rant how people need personal responsibility economic social lives sudden 's radicals such me i guess responsible poor people 's lifestyles tell think poor people dumb think themselves there reasons disintegration family and support systems general nation 's poor somehow i think murphy janis the sane person 's list you want generation 's vaunted cultural revolution lasting change worse try socalled relevant values education hey like good idea time how know needed real education mean took granted the 's generation spoiled irresponsible the depression create mothers fathers determined kids want going overboard creating nation brats consider contrast famous events july apollo woodstock which group large numbers people feed reverted cultural level primitives defecation public etc and group assembled took care itself dispersed damage deaths large numbers drug problems was n't woodstock called biggest parking lot history they rejected society went nature parent 's cars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 tensor([    0,  8310,   905,  1359,    71,  2579,   251, 25693,   141,    82,\n",
      "          240,  1081,  2640,   776,   592,  1074,  7207,   128,    29, 35842,\n",
      "          215,   162,   939,  4443,  2149,  2129,    82,   128,    29, 28182,\n",
      "         1137,   206,  2129,    82, 16881,   206,  1235,    89,  2188, 32654,\n",
      "         8475,   284,     8,   323,  1743,   937,  1226,   128,    29,  2129,\n",
      "         7421,   939,   206, 22802, 16628, 10408,   354,     5, 37091,   621,\n",
      "          128,    29,   889,    47,   236,  2706,   128,    29,   748, 19264,\n",
      "         4106,  7977,  9735,   464,  3007,   860, 17380,  9315,  4249,  3266,\n",
      "         1265, 17232,   101,   205,  1114,    86,   141,   216,   956,   588,\n",
      "         1265,  1266,   362,  4159,     5,   128,    29,  2706, 29136, 21573,\n",
      "            5,  6943,  1045,  8826, 17850,  3030,  1159,   236,   164, 35912,\n",
      "         2351,  1226,  5378,  2923,  1701,  5709,  3395,  1061,  1236, 21347,\n",
      "         6256, 38520,  5627,  9607,    61,   333,   739,  1530,    82,  3993,\n",
      "        41411,  4106,   672,  9156, 35143,  3816,  3204,  1258,   285,  4753,\n",
      "            8,   333, 14525,   362,   575,  1495, 32807,  1880,  3257,   739,\n",
      "         1530,  1262,  1272,    21,   295,    75,  5627,  9607,   373,   934,\n",
      "         2932,   319,   750,    51,  3946,  2313,   439,  2574,  4095,   128,\n",
      "           29,  1677,     2])\n",
      "152\n",
      "<s>       tensor([ 0.0302,  0.0098,  0.1224, -0.0345,  0.0752], grad_fn=<SliceBackward>) (...)\n",
      "now       tensor([ 0.0393, -0.0690, -0.3313, -0.1227,  0.1800], grad_fn=<SliceBackward>) (...)\n",
      "let       tensor([ 0.1010, -0.0228, -0.1646, -0.1543,  0.0447], grad_fn=<SliceBackward>) (...)\n",
      "straight  tensor([ 0.2091, -0.2438, -0.2680,  0.1781,  0.0975], grad_fn=<SliceBackward>) (...)\n",
      "after     tensor([-0.1753, -0.0346, -0.3692,  0.1023,  0.2032], grad_fn=<SliceBackward>) (...)\n",
      "nice      tensor([ 0.1725, -0.0696, -0.2781, -0.1379,  0.2678], grad_fn=<SliceBackward>) (...)\n",
      "long      tensor([ 0.1106,  0.1460, -0.4175, -0.3472, -0.0420], grad_fn=<SliceBackward>) (...)\n",
      "rant      tensor([ 0.2148,  0.0055, -0.2569,  0.2737, -0.2485], grad_fn=<SliceBackward>) (...)\n",
      "how       tensor([ 0.2709, -0.2224, -0.2750,  0.2639, -0.2607], grad_fn=<SliceBackward>) (...)\n",
      "people    tensor([ 0.0228, -0.1473, -0.2250, -0.0179, -0.3463], grad_fn=<SliceBackward>) (...)\n",
      "need      tensor([ 0.0726, -0.2178, -0.1451, -0.2164, -0.1891], grad_fn=<SliceBackward>) (...)\n",
      "personal  tensor([ 0.0040, -0.3281, -0.1735, -0.0379,  0.2510], grad_fn=<SliceBackward>) (...)\n",
      "responsibilitytensor([ 0.0626,  0.0004, -0.0598, -0.1265, -0.0399], grad_fn=<SliceBackward>) (...)\n",
      "economic  tensor([ 0.2742, -0.2316, -0.1548,  0.3878,  0.2656], grad_fn=<SliceBackward>) (...)\n",
      "social    tensor([ 0.1093, -0.3464, -0.2793,  0.0736,  0.3081], grad_fn=<SliceBackward>) (...)\n",
      "lives     tensor([ 0.1688, -0.1624, -0.3362,  0.1650, -0.0933], grad_fn=<SliceBackward>) (...)\n",
      "sudden    tensor([ 0.0971, -0.1188, -0.1441, -0.1043, -0.3079], grad_fn=<SliceBackward>) (...)\n",
      "'s        tensor([ 0.0697, -0.0195, -0.4105, -0.4592, -0.0165], grad_fn=<SliceBackward>) (...)\n",
      "radicals  tensor([ 0.2561, -0.0652,  0.0799, -0.1866,  0.0009], grad_fn=<SliceBackward>) (...)\n",
      "such      tensor([ 0.0185,  0.0325, -0.0014, -0.3010, -0.0371], grad_fn=<SliceBackward>) (...)\n",
      "me        tensor([ 0.1259, -0.0328, -0.0250, -0.1060, -0.1509], grad_fn=<SliceBackward>) (...)\n",
      "i         tensor([ 0.4878,  0.4521, -0.2119, -0.1300,  0.2328], grad_fn=<SliceBackward>) (...)\n",
      "guess     tensor([ 0.0221,  0.4480, -0.3052, -0.1920,  0.2779], grad_fn=<SliceBackward>) (...)\n",
      "responsibletensor([-0.1174,  0.1388, -0.2813, -0.1282,  0.1310], grad_fn=<SliceBackward>) (...)\n",
      "poor      tensor([-0.4279, -0.4089, -0.5635, -0.2078,  0.0963], grad_fn=<SliceBackward>) (...)\n",
      "people    tensor([-0.2817, -0.1348, -0.7782, -0.3482, -0.5964], grad_fn=<SliceBackward>) (...)\n",
      "'s        tensor([ 0.8738,  0.1939, -0.1980, -0.6656, -0.1582], grad_fn=<SliceBackward>) (...)\n",
      "lifestylestensor([ 0.2870,  0.0388, -0.3782,  0.0448,  0.2793], grad_fn=<SliceBackward>) (...)\n",
      "tell      tensor([ 0.1128,  0.0592, -0.0101,  0.0106, -0.0272], grad_fn=<SliceBackward>) (...)\n",
      "think     tensor([-0.0429, -0.0918, -0.1659, -0.2200, -0.1436], grad_fn=<SliceBackward>) (...)\n",
      "poor      tensor([-0.3747, -0.5261, -0.2702, -0.1240,  0.2178], grad_fn=<SliceBackward>) (...)\n",
      "people    tensor([ 0.0306, -0.3108, -0.5681, -0.2846, -0.3918], grad_fn=<SliceBackward>) (...)\n",
      "dumb      tensor([-0.0560,  0.0034,  0.2129,  0.1401,  0.1343], grad_fn=<SliceBackward>) (...)\n",
      "think     tensor([ 0.2083, -0.1123,  0.0862, -0.4893, -0.6004], grad_fn=<SliceBackward>) (...)\n",
      "themselvestensor([ 0.3776,  0.0932, -0.1470,  0.0308, -0.5223], grad_fn=<SliceBackward>) (...)\n",
      "there     tensor([ 0.1321,  0.0367, -0.2702, -0.1829, -0.0329], grad_fn=<SliceBackward>) (...)\n",
      "reasons   tensor([ 0.1534,  0.1389, -0.1494,  0.0869,  0.1670], grad_fn=<SliceBackward>) (...)\n",
      "disintegrationtensor([ 0.3227,  0.0292, -0.1048, -0.0987,  0.2708], grad_fn=<SliceBackward>) (...)\n",
      "family    tensor([ 0.2528, -0.1436, -0.1760, -0.0248, -0.0550], grad_fn=<SliceBackward>) (...)\n",
      "and       tensor([ 0.2188, -0.1166,  0.1049,  0.0177,  0.1797], grad_fn=<SliceBackward>) (...)\n",
      "support   tensor([-0.1370, -0.3007, -0.2155, -0.3850,  0.2240], grad_fn=<SliceBackward>) (...)\n",
      "systems   tensor([-0.0512, -0.3115, -0.4420,  0.1333,  0.0244], grad_fn=<SliceBackward>) (...)\n",
      "general   tensor([ 0.1138,  0.2203, -0.2359,  0.0554,  0.3133], grad_fn=<SliceBackward>) (...)\n",
      "nation    tensor([ 0.0079,  0.3810, -0.1366, -0.0039, -0.1719], grad_fn=<SliceBackward>) (...)\n",
      "'s        tensor([ 0.7944,  0.1546, -0.1128, -0.5273, -0.0769], grad_fn=<SliceBackward>) (...)\n",
      "poor      tensor([-0.0361, -0.2257, -0.2822, -0.0711,  0.0103], grad_fn=<SliceBackward>) (...)\n",
      "somehow   tensor([ 0.0164, -0.0665, -0.3250, -0.0846, -0.0689], grad_fn=<SliceBackward>) (...)\n",
      "i         tensor([ 0.4469,  0.3359,  0.0985, -0.1033,  0.2245], grad_fn=<SliceBackward>) (...)\n",
      "think     tensor([ 0.1697,  0.0346, -0.2599, -0.2131, -0.3157], grad_fn=<SliceBackward>) (...)\n",
      "murphy    tensor([ 0.2621,  0.2780, -0.3397, -0.1270,  0.0277], grad_fn=<SliceBackward>) (...)\n",
      "janis     tensor([ 0.2712, -0.0955, -0.5443, -0.1329, -0.4716], grad_fn=<SliceBackward>) (...)\n",
      "the       tensor([ 0.2030,  0.2633, -0.3861, -0.1215,  0.0215], grad_fn=<SliceBackward>) (...)\n",
      "sane      tensor([-0.1762,  0.0646, -0.2519, -0.0019,  0.2106], grad_fn=<SliceBackward>) (...)\n",
      "person    tensor([-0.2528,  0.0408, -0.5174, -0.2997, -0.5688], grad_fn=<SliceBackward>) (...)\n",
      "'s        tensor([ 0.7345,  0.1804,  0.0104, -0.7484, -0.2470], grad_fn=<SliceBackward>) (...)\n",
      "list      tensor([ 0.1474,  0.1611, -0.0169, -0.0661, -0.3744], grad_fn=<SliceBackward>) (...)\n",
      "you       tensor([ 0.0698,  0.1894,  0.0164, -0.3534, -0.2746], grad_fn=<SliceBackward>) (...)\n",
      "want      tensor([ 0.1888, -0.0809, -0.1177, -0.0484, -0.2011], grad_fn=<SliceBackward>) (...)\n",
      "generationtensor([ 0.1861,  0.0077,  0.1303, -0.2836, -0.4716], grad_fn=<SliceBackward>) (...)\n",
      "'s        tensor([ 0.5934,  0.1858,  0.1361, -0.6332, -0.2213], grad_fn=<SliceBackward>) (...)\n",
      "vaunted   tensor([ 0.7042, -0.4999,  0.0031,  0.3202,  0.0717], grad_fn=<SliceBackward>) (...)\n",
      "cultural  tensor([ 0.1421, -0.2810, -0.0670, -0.0115,  0.2806], grad_fn=<SliceBackward>) (...)\n",
      "revolutiontensor([0.1289, 0.0103, 0.2324, 0.0992, 0.0030], grad_fn=<SliceBackward>) (...)\n",
      "lasting   tensor([ 0.0036, -0.1329,  0.0747, -0.1247,  0.0983], grad_fn=<SliceBackward>) (...)\n",
      "change    tensor([ 0.1185, -0.2742,  0.0976, -0.2009, -0.1206], grad_fn=<SliceBackward>) (...)\n",
      "worse     tensor([ 0.0881,  0.0152,  0.0608, -0.0453,  0.0049], grad_fn=<SliceBackward>) (...)\n",
      "try       tensor([-0.0852, -0.0122,  0.0175, -0.0084,  0.1194], grad_fn=<SliceBackward>) (...)\n",
      "socalled  tensor([ 0.1282, -0.2997,  0.0340, -0.0220,  0.5855], grad_fn=<SliceBackward>) (...)\n",
      "relevant  tensor([-0.0633, -0.1934, -0.1392,  0.0227,  0.2234], grad_fn=<SliceBackward>) (...)\n",
      "values    tensor([ 0.1986, -0.1613, -0.1050, -0.0071,  0.1781], grad_fn=<SliceBackward>) (...)\n",
      "education tensor([ 0.2864, -0.2105, -0.0536, -0.0959,  0.0207], grad_fn=<SliceBackward>) (...)\n",
      "hey       tensor([ 0.1086,  0.1511, -0.1609,  0.2328, -0.0142], grad_fn=<SliceBackward>) (...)\n",
      "like      tensor([-0.0525,  0.1108, -0.0355,  0.0858,  0.0593], grad_fn=<SliceBackward>) (...)\n",
      "good      tensor([ 0.0981,  0.0201, -0.1802, -0.2422,  0.0537], grad_fn=<SliceBackward>) (...)\n",
      "idea      tensor([ 0.0564,  0.1619, -0.1799, -0.0653, -0.0822], grad_fn=<SliceBackward>) (...)\n",
      "time      tensor([ 0.1834, -0.1228,  0.0255, -0.1433, -0.0754], grad_fn=<SliceBackward>) (...)\n",
      "how       tensor([ 0.1827, -0.1175, -0.1559,  0.1818, -0.0912], grad_fn=<SliceBackward>) (...)\n",
      "know      tensor([-0.0378,  0.0626,  0.0278,  0.3709, -0.1048], grad_fn=<SliceBackward>) (...)\n",
      "needed    tensor([ 0.1508, -0.0273, -0.0592, -0.0614, -0.0453], grad_fn=<SliceBackward>) (...)\n",
      "real      tensor([ 0.1993, -0.4478, -0.1379,  0.0558,  0.2642], grad_fn=<SliceBackward>) (...)\n",
      "education tensor([ 0.2287, -0.0952,  0.0585, -0.0139,  0.0507], grad_fn=<SliceBackward>) (...)\n",
      "mean      tensor([0.1301, 0.0060, 0.1000, 0.2084, 0.0182], grad_fn=<SliceBackward>) (...)\n",
      "took      tensor([ 0.1516, -0.2648, -0.1047,  0.0216,  0.1827], grad_fn=<SliceBackward>) (...)\n",
      "granted   tensor([ 0.0508, -0.2128,  0.0744,  0.1456,  0.1408], grad_fn=<SliceBackward>) (...)\n",
      "the       tensor([ 0.0799, -0.0581,  0.1062, -0.3915,  0.0444], grad_fn=<SliceBackward>) (...)\n",
      "'s        tensor([ 0.3020, -0.0621,  0.3108, -0.3681,  0.2548], grad_fn=<SliceBackward>) (...)\n",
      "generationtensor([ 0.2929,  0.2526,  0.2569, -0.1692, -0.1054], grad_fn=<SliceBackward>) (...)\n",
      "spoiled   tensor([ 0.2562, -0.1692, -0.0018,  0.2048,  0.2465], grad_fn=<SliceBackward>) (...)\n",
      "irresponsibletensor([ 0.0558,  0.0269,  0.1324, -0.0180,  0.0079], grad_fn=<SliceBackward>) (...)\n",
      "the       tensor([ 0.3663,  0.0006,  0.0142, -0.1825,  0.0903], grad_fn=<SliceBackward>) (...)\n",
      "depressiontensor([ 0.0093, -0.1158,  0.2884, -0.0223,  0.1805], grad_fn=<SliceBackward>) (...)\n",
      "create    tensor([-0.0036, -0.0303,  0.1231, -0.0272, -0.1095], grad_fn=<SliceBackward>) (...)\n",
      "mothers   tensor([ 0.3070, -0.2431,  0.2015, -0.0148,  0.2740], grad_fn=<SliceBackward>) (...)\n",
      "fathers   tensor([ 0.2888, -0.5918,  0.0325, -0.4558,  0.2241], grad_fn=<SliceBackward>) (...)\n",
      "determinedtensor([ 0.1701, -0.0039,  0.1501,  0.0181,  0.0541], grad_fn=<SliceBackward>) (...)\n",
      "kids      tensor([ 0.2814, -0.2322,  0.2203, -0.0552, -0.1838], grad_fn=<SliceBackward>) (...)\n",
      "want      tensor([ 0.3121, -0.0491,  0.1599,  0.0592, -0.2612], grad_fn=<SliceBackward>) (...)\n",
      "going     tensor([-0.0927, -0.0683,  0.1429, -0.0016,  0.0014], grad_fn=<SliceBackward>) (...)\n",
      "overboard tensor([-0.1466,  0.0620,  0.0697,  0.0319,  0.1328], grad_fn=<SliceBackward>) (...)\n",
      "creating  tensor([-0.0730,  0.1119, -0.0058, -0.1537,  0.1315], grad_fn=<SliceBackward>) (...)\n",
      "nation    tensor([ 0.0918,  0.2164,  0.1045, -0.0126,  0.1381], grad_fn=<SliceBackward>) (...)\n",
      "brats     tensor([-0.3484, -0.0368,  0.2015, -0.1526,  0.2712], grad_fn=<SliceBackward>) (...)\n",
      "consider  tensor([ 0.0685, -0.1386,  0.3058,  0.0012,  0.2177], grad_fn=<SliceBackward>) (...)\n",
      "contrast  tensor([ 0.0967, -0.1925,  0.2454,  0.0215,  0.0301], grad_fn=<SliceBackward>) (...)\n",
      "famous    tensor([ 0.2415, -0.1875,  0.3958,  0.0478, -0.1595], grad_fn=<SliceBackward>) (...)\n",
      "events    tensor([ 0.3961, -0.2663,  0.3288,  0.0408, -0.0358], grad_fn=<SliceBackward>) (...)\n",
      "july      tensor([ 0.7646, -0.0963,  0.7744,  0.0280,  0.4202], grad_fn=<SliceBackward>) (...)\n",
      "apollo    tensor([ 0.7703, -0.1626,  0.3554,  0.0164,  0.3912], grad_fn=<SliceBackward>) (...)\n",
      "woodstock tensor([ 0.8058,  0.2145,  1.1339, -0.1413,  0.3818], grad_fn=<SliceBackward>) (...)\n",
      "which     tensor([ 0.2972,  0.1001,  0.3946, -0.0651,  0.0151], grad_fn=<SliceBackward>) (...)\n",
      "group     tensor([ 0.1769,  0.2321,  0.4462, -0.1367, -0.0519], grad_fn=<SliceBackward>) (...)\n",
      "large     tensor([ 0.2779,  0.1812,  0.4576, -0.5415, -0.0725], grad_fn=<SliceBackward>) (...)\n",
      "numbers   tensor([ 0.4795,  0.1842,  0.2696,  0.1067, -0.1113], grad_fn=<SliceBackward>) (...)\n",
      "people    tensor([ 0.2389, -0.2040,  0.4118,  0.0531, -0.2897], grad_fn=<SliceBackward>) (...)\n",
      "feed      tensor([ 0.1076, -0.0773,  0.4601,  0.0091, -0.0141], grad_fn=<SliceBackward>) (...)\n",
      "reverted  tensor([-0.0203, -0.0892,  0.2884,  0.0163,  0.1053], grad_fn=<SliceBackward>) (...)\n",
      "cultural  tensor([-0.0174, -0.1531,  0.4035, -0.0161,  0.4765], grad_fn=<SliceBackward>) (...)\n",
      "level     tensor([-0.1021, -0.1589,  0.4141, -0.0910,  0.1587], grad_fn=<SliceBackward>) (...)\n",
      "primitivestensor([ 0.4806, -0.3644,  0.8873, -0.2161,  1.0296], grad_fn=<SliceBackward>) (...)\n",
      "defecationtensor([ 0.4979,  0.7453,  0.8267, -0.2067,  0.7851], grad_fn=<SliceBackward>) (...)\n",
      "public    tensor([-0.0460, -0.3544,  0.3243, -0.0078,  0.1741], grad_fn=<SliceBackward>) (...)\n",
      "etc       tensor([ 0.0786, -0.2999,  0.2566, -0.3332,  0.0560], grad_fn=<SliceBackward>) (...)\n",
      "and       tensor([ 0.3502,  0.0908,  0.2811,  0.1302, -0.1490], grad_fn=<SliceBackward>) (...)\n",
      "group     tensor([ 0.1428,  0.1259,  0.3157, -0.2575,  0.1070], grad_fn=<SliceBackward>) (...)\n",
      "assembled tensor([0.1180, 0.1498, 0.2824, 0.0543, 0.1742], grad_fn=<SliceBackward>) (...)\n",
      "took      tensor([ 0.2217, -0.2544,  0.1726, -0.2103, -0.0819], grad_fn=<SliceBackward>) (...)\n",
      "care      tensor([0.1412, 0.1785, 0.3790, 0.0092, 0.1088], grad_fn=<SliceBackward>) (...)\n",
      "itself    tensor([0.1067, 0.0271, 0.4208, 0.0561, 0.0295], grad_fn=<SliceBackward>) (...)\n",
      "dispersed tensor([ 0.3107, -0.1578,  0.3624, -0.0088,  0.0532], grad_fn=<SliceBackward>) (...)\n",
      "damage    tensor([ 0.1480, -0.1668,  0.4174, -0.0729,  0.0914], grad_fn=<SliceBackward>) (...)\n",
      "deaths    tensor([ 0.3496, -0.1068,  0.2988,  0.1976,  0.0099], grad_fn=<SliceBackward>) (...)\n",
      "large     tensor([ 0.3212,  0.4200,  0.3828, -1.0290,  0.0549], grad_fn=<SliceBackward>) (...)\n",
      "numbers   tensor([ 0.3337,  0.0190,  0.1312,  0.2529, -0.0327], grad_fn=<SliceBackward>) (...)\n",
      "drug      tensor([ 0.1846, -0.5488,  0.5022, -0.0330,  0.0029], grad_fn=<SliceBackward>) (...)\n",
      "problems  tensor([-0.0704, -0.5175,  0.3860, -0.2169, -0.2688], grad_fn=<SliceBackward>) (...)\n",
      "was       tensor([ 0.1196, -0.3164,  0.2018, -0.1731, -0.0407], grad_fn=<SliceBackward>) (...)\n",
      "n't       tensor([ 0.1103, -0.1283,  0.5824, -0.2519,  0.2847], grad_fn=<SliceBackward>) (...)\n",
      "woodstock tensor([ 1.0914,  0.3956,  1.0484, -0.1045,  0.1822], grad_fn=<SliceBackward>) (...)\n",
      "called    tensor([ 0.1995,  0.0230,  0.1699, -0.2556, -0.0244], grad_fn=<SliceBackward>) (...)\n",
      "biggest   tensor([ 0.2747,  0.0644,  0.4136, -0.0471, -0.0334], grad_fn=<SliceBackward>) (...)\n",
      "parking   tensor([-0.1480, -0.0267,  0.3689, -0.4443,  0.1135], grad_fn=<SliceBackward>) (...)\n",
      "lot       tensor([ 0.1868,  0.1621,  0.4695, -0.6649,  0.0204], grad_fn=<SliceBackward>) (...)\n",
      "history   tensor([-2.1054e-04, -4.6503e-02,  4.3817e-01, -3.1679e-01, -4.3834e-01],\n",
      "       grad_fn=<SliceBackward>) (...)\n",
      "they      tensor([ 0.3250, -0.0946,  0.3217, -0.0473, -0.1501], grad_fn=<SliceBackward>) (...)\n",
      "rejected  tensor([ 0.2303, -0.1733,  0.3419,  0.1600,  0.1109], grad_fn=<SliceBackward>) (...)\n",
      "society   tensor([ 0.1545, -0.2539,  0.1840, -0.0614,  0.2232], grad_fn=<SliceBackward>) (...)\n",
      "went      tensor([ 0.1304, -0.2261,  0.2946,  0.0414,  0.0745], grad_fn=<SliceBackward>) (...)\n",
      "nature    tensor([ 0.0891,  0.1716,  0.2499, -0.2776,  0.0799], grad_fn=<SliceBackward>) (...)\n",
      "parent    tensor([ 0.2184, -0.0305,  0.2023, -0.3797, -0.0585], grad_fn=<SliceBackward>) (...)\n",
      "'s        tensor([ 0.6701,  0.2390,  0.4390, -0.7167, -0.0340], grad_fn=<SliceBackward>) (...)\n",
      "cars      tensor([ 0.2558,  0.0101,  0.2862, -0.2187, -0.1624], grad_fn=<SliceBackward>) (...)\n",
      "</s>      tensor([ 0.0454,  0.0074,  0.0973, -0.0148,  0.0102], grad_fn=<SliceBackward>) (...)\n"
     ]
    }
   ],
   "source": [
    "# tokens_ori = train_df.loc[3,'preprocess_text']#'how\\'s are you'#\n",
    "tokens = roberta.encode(tokens_ori)\n",
    "# assert tokens.tolist() == [0, 31414, 232, 328, 2]\n",
    "# assert roberta.decode(tokens) == 'Hello world!'\n",
    "print(len(tokens) , tokens)\n",
    "roberta.decode(tokens)\n",
    "doc = roberta.extract_features_aligned_to_words(tokens_ori)\n",
    "print(len(doc))\n",
    "for tok in doc:\n",
    "    print('{:10}{} (...)'.format(str(tok), tok.vector[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, ['I', 'said', ',', '\"', 'hello', 'RoBERTa', '.', '\"'])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = 'I said, \"hello RoBERTa.\"'\n",
    "tokens = sp_tokenizer(tokens)\n",
    "token_li = []\n",
    "for token in tokens:\n",
    "    token_li.append(token.text)\n",
    "len(token_li) , token_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tokens exceeds maximum length: 592 > 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-7a2d631178ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast_layer_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# == torch.Size([1, 5, 1024])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/roberta/hub_interface.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, tokens, return_all_hiddens)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             raise ValueError('tokens exceeds maximum length: {} > {}'.format(\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             ))\n\u001b[1;32m     84\u001b[0m         features, extra = self.model(\n",
      "\u001b[0;31mValueError\u001b[0m: tokens exceeds maximum length: 592 > 512"
     ]
    }
   ],
   "source": [
    "last_layer_features = roberta.extract_features(tokens)\n",
    "print(last_layer_features.size())# == torch.Size([1, 5, 1024])\n",
    "\n",
    "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)\n",
    "assert len(all_layers) == 25\n",
    "assert torch.all(all_layers[-1] == last_layer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 19, 1024)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer_features.detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 898823/898823 [00:06<00:00, 134563.64B/s]\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 545019.70B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0925 15:40:29.729804 139833327294272 tokenization_utils.py:665] Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "590"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(train_df.loc[0,'text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
