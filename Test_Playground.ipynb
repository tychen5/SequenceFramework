{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, csv\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import xlsxwriter\n",
    "import random\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import unicodedata as udata\n",
    "# import pause, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from distutils.dir_util import copy_tree\n",
    "import sklearn\n",
    "from sklearn.metrics import *\n",
    "import itertools as it\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import functools\n",
    "import spacy\n",
    "\n",
    "import tensorflow.keras.preprocessing.text as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_1 = (stopwords.words('english')) #Stopword\n",
    "with open('data/preprocess/stop_words.txt') as f:\n",
    "    stop_words_2 = f.read().splitlines() #stop_list1\n",
    "stop_words_3 = pickle.load(open('data/preprocess/stop_list2.pkl','rb'))\n",
    "stop_words_all = set(stop_words_1 + stop_words_2 + stop_words_3)\n",
    "len(stop_words_all) # stopwords#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk = ['A','B']\n",
    "[x.lower() for x in kk]\n",
    "'A5555'.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A', 'a'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['A','a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', \"'s\", '(', 'beautiful', ')', '4toys', 'does', \"n't\"]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenizer = spacy.load(\"en\")\n",
    "kk = sp_tokenizer(\"It's (beautiful) 4toys doesn't\")\n",
    "token_li=[]\n",
    "for token in kk:\n",
    "    token_li.append(token.text)\n",
    "token_li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196009"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenizer = spacy.load(\"en\")\n",
    "dil= r\"[!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~]+\\ *\" \n",
    "dil_filter = '!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_\\`{|}~\\t\\n'\n",
    "glove = pd.read_table('./data/preprocess/glove.840B.300d.txt', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "glove_words = set(glove.index.tolist())\n",
    "len(glove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprcess(text,tokenizers='nltk'):\n",
    "    '''\n",
    "    Input: string\n",
    "    Return: preprocessed list\n",
    "    '''\n",
    "    tokens = [i for i in text.split() if i not in stop_words_all]\n",
    "    tokens_str = ' '.join(tokens)\n",
    "    if tokenizers == 'keras':\n",
    "        tokens = T.text_to_word_sequence(tokens_str,filters=dil_filter) # lower casse & tokenize\n",
    "        tokens = list(filter(None, tokens))\n",
    "    elif tokenizers == 'nltk':\n",
    "#         tokens = re.sub(dil,\" \",tokens_str.lower())\n",
    "        tokens = [i for i in tokens_str.lower() if i not in dil_filter]\n",
    "        tokens = ''.join(tokens)\n",
    "        tokens = word_tokenize(tokens)\n",
    "        tokens = list(filter(None, tokens))\n",
    "#         tokens = [x.lower() for x in tokens]\n",
    "    elif tokenizers == 'spacy':\n",
    "#         tokens = re.sub(dil,\" \",tokens_str.lower())\n",
    "        tokens = [i for i in tokens_str.lower() if i not in dil_filter]\n",
    "        tokens = ''.join(tokens)\n",
    "        tokens = sp_tokenizer(tokens)\n",
    "        token_li = []\n",
    "        for token in tokens:\n",
    "            token_li.append(token.text)\n",
    "        tokens = list(filter(None, token_li))\n",
    "#         tokens = [x.lower() for x in token_li]\n",
    "    tokens_li = []\n",
    "    for token in tokens:\n",
    "        tokens_li.append(''.join([i for i in token if not i.isdigit()]))\n",
    "    tokens_str = ' '.join(tokens_li)\n",
    "    tokens_str = re.sub(dil,\" \",tokens_str.lower())\n",
    "    tokens_li = tokens_str.split()\n",
    "    tokens_li = list(filter(None, tokens_li))\n",
    "    ori_tokens_num = len(tokens_li)\n",
    "    tokens_li = [i for i in tokens_li if i in glove_words]\n",
    "    try:\n",
    "        rate = len(tokens_li)/ori_tokens_num\n",
    "    except ZeroDivisionError:\n",
    "        rate = np.nan\n",
    "    return tokens_li , rate#ori_tokens_num/len(tokens_li)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:53<00:00, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595051529116455 0.959558811893717 0.9605178211130393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>original_tokenize_num</th>\n",
       "      <th>text_keras</th>\n",
       "      <th>rate_keras</th>\n",
       "      <th>text_nltk</th>\n",
       "      <th>rate_nltk</th>\n",
       "      <th>text_apacy</th>\n",
       "      <th>rate_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19305</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>My votes (FWIW): Team MVP: Pat Verbeek. He fan...</td>\n",
       "      <td>453</td>\n",
       "      <td>[my, votes, fwiw, team, mvp, pat, he, fans, go...</td>\n",
       "      <td>0.950207</td>\n",
       "      <td>[my, votes, fwiw, team, mvp, pat, he, fans, go...</td>\n",
       "      <td>0.950207</td>\n",
       "      <td>[my, votes, fwiw, team, mvp, pat, he, fans, go...</td>\n",
       "      <td>0.950207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19313</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Kovalev is too talented a player to play for R...</td>\n",
       "      <td>160</td>\n",
       "      <td>[kovalev, talented, player, play, roger, niels...</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>[kovalev, talented, player, play, roger, needs...</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>[kovalev, talented, player, play, roger, needs...</td>\n",
       "      <td>0.938272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19122</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>New Jersey 1 0 2--3 Pittsburgh 2 3 1--6 First ...</td>\n",
       "      <td>919</td>\n",
       "      <td>[new, jersey, pittsburgh, first, period, pitts...</td>\n",
       "      <td>0.867299</td>\n",
       "      <td>[new, jersey, pittsburgh, first, period, pitts...</td>\n",
       "      <td>0.867299</td>\n",
       "      <td>[new, jersey, pittsburgh, first, period, pitts...</td>\n",
       "      <td>0.867299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19312</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>First of all, the Penguins WILL win the cup ag...</td>\n",
       "      <td>66</td>\n",
       "      <td>[first, all, penguins, will, win, cup, again, ...</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>[first, all, penguins, will, win, cup, again, ...</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>[first, all, penguins, will, win, cup, again, ...</td>\n",
       "      <td>0.981481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>19115</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>You can't. But good luck trying.</td>\n",
       "      <td>6</td>\n",
       "      <td>[you, can't, but, good, luck, trying]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[you, ca, n't, but, good, luck, trying]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[you, ca, n't, but, good, luck, trying]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>20797</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>: When the object of their belief is said to b...</td>\n",
       "      <td>269</td>\n",
       "      <td>[when, object, belief, said, perfect, make, be...</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>[when, object, belief, said, perfect, make, be...</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>[when, object, belief, said, perfect, make, be...</td>\n",
       "      <td>0.978102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>20537</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Koff! You mean that as long as I put you to sl...</td>\n",
       "      <td>23</td>\n",
       "      <td>[koff, you, mean, long, i, sleep, first, i, ki...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[koff, you, mean, long, i, sleep, first, i, ki...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[koff, you, mean, long, i, sleep, first, i, ki...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>20757</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>: Mr Connor's assertion that \"more complex\" ==...</td>\n",
       "      <td>135</td>\n",
       "      <td>[mr, assertion, more, complex, later, paleonto...</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>[mr, connor, 's, assertion, more, complex, lat...</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>[mr, connor, 's, assertion, more, complex, lat...</td>\n",
       "      <td>0.985915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>20677</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>(excess stuff deleted...) I know of a similar ...</td>\n",
       "      <td>139</td>\n",
       "      <td>[excess, stuff, deleted, i, know, similar, inc...</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>[excess, stuff, deleted, i, know, similar, inc...</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>[excess, stuff, deleted, i, know, similar, inc...</td>\n",
       "      <td>0.987952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>20719</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>The \"System\" refered to a \"moral system\". You ...</td>\n",
       "      <td>40</td>\n",
       "      <td>[the, system, refered, moral, system, you, sho...</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>[the, system, refered, moral, system, you, hav...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[the, system, refered, moral, system, you, sho...</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id          category  \\\n",
       "0      19305  rec.sport.hockey   \n",
       "1      19313  rec.sport.hockey   \n",
       "2      19122  rec.sport.hockey   \n",
       "3      19312  rec.sport.hockey   \n",
       "4      19115  rec.sport.hockey   \n",
       "...      ...               ...   \n",
       "11309  20797       alt.atheism   \n",
       "11310  20537       alt.atheism   \n",
       "11311  20757       alt.atheism   \n",
       "11312  20677       alt.atheism   \n",
       "11313  20719       alt.atheism   \n",
       "\n",
       "                                                    text  \\\n",
       "0      My votes (FWIW): Team MVP: Pat Verbeek. He fan...   \n",
       "1      Kovalev is too talented a player to play for R...   \n",
       "2      New Jersey 1 0 2--3 Pittsburgh 2 3 1--6 First ...   \n",
       "3      First of all, the Penguins WILL win the cup ag...   \n",
       "4                       You can't. But good luck trying.   \n",
       "...                                                  ...   \n",
       "11309  : When the object of their belief is said to b...   \n",
       "11310  Koff! You mean that as long as I put you to sl...   \n",
       "11311  : Mr Connor's assertion that \"more complex\" ==...   \n",
       "11312  (excess stuff deleted...) I know of a similar ...   \n",
       "11313  The \"System\" refered to a \"moral system\". You ...   \n",
       "\n",
       "      original_tokenize_num  \\\n",
       "0                       453   \n",
       "1                       160   \n",
       "2                       919   \n",
       "3                        66   \n",
       "4                         6   \n",
       "...                     ...   \n",
       "11309                   269   \n",
       "11310                    23   \n",
       "11311                   135   \n",
       "11312                   139   \n",
       "11313                    40   \n",
       "\n",
       "                                              text_keras  rate_keras  \\\n",
       "0      [my, votes, fwiw, team, mvp, pat, he, fans, go...    0.950207   \n",
       "1      [kovalev, talented, player, play, roger, niels...    0.951220   \n",
       "2      [new, jersey, pittsburgh, first, period, pitts...    0.867299   \n",
       "3      [first, all, penguins, will, win, cup, again, ...    0.981481   \n",
       "4                  [you, can't, but, good, luck, trying]    1.000000   \n",
       "...                                                  ...         ...   \n",
       "11309  [when, object, belief, said, perfect, make, be...    0.930233   \n",
       "11310  [koff, you, mean, long, i, sleep, first, i, ki...    1.000000   \n",
       "11311  [mr, assertion, more, complex, later, paleonto...    0.971014   \n",
       "11312  [excess, stuff, deleted, i, know, similar, inc...    0.987805   \n",
       "11313  [the, system, refered, moral, system, you, sho...    0.950000   \n",
       "\n",
       "                                               text_nltk  rate_nltk  \\\n",
       "0      [my, votes, fwiw, team, mvp, pat, he, fans, go...   0.950207   \n",
       "1      [kovalev, talented, player, play, roger, needs...   0.938272   \n",
       "2      [new, jersey, pittsburgh, first, period, pitts...   0.867299   \n",
       "3      [first, all, penguins, will, win, cup, again, ...   0.981481   \n",
       "4                [you, ca, n't, but, good, luck, trying]   1.000000   \n",
       "...                                                  ...        ...   \n",
       "11309  [when, object, belief, said, perfect, make, be...   0.978102   \n",
       "11310  [koff, you, mean, long, i, sleep, first, i, ki...   1.000000   \n",
       "11311  [mr, connor, 's, assertion, more, complex, lat...   0.985915   \n",
       "11312  [excess, stuff, deleted, i, know, similar, inc...   0.987952   \n",
       "11313  [the, system, refered, moral, system, you, hav...   1.000000   \n",
       "\n",
       "                                              text_apacy  rate_spacy  \n",
       "0      [my, votes, fwiw, team, mvp, pat, he, fans, go...    0.950207  \n",
       "1      [kovalev, talented, player, play, roger, needs...    0.938272  \n",
       "2      [new, jersey, pittsburgh, first, period, pitts...    0.867299  \n",
       "3      [first, all, penguins, will, win, cup, again, ...    0.981481  \n",
       "4                [you, ca, n't, but, good, luck, trying]    1.000000  \n",
       "...                                                  ...         ...  \n",
       "11309  [when, object, belief, said, perfect, make, be...    0.978102  \n",
       "11310  [koff, you, mean, long, i, sleep, first, i, ki...    1.000000  \n",
       "11311  [mr, connor, 's, assertion, more, complex, lat...    0.985915  \n",
       "11312  [excess, stuff, deleted, i, know, similar, inc...    0.987952  \n",
       "11313  [the, system, refered, moral, system, you, sho...    0.950000  \n",
       "\n",
       "[11314 rows x 10 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df = pd.DataFrame(columns=['id','category','text','original_tokenize_num','preprocess_text','preprocss_tokenize_num'])\n",
    "train_df = pd.DataFrame(columns=['id','category','text','original_tokenize_num',\n",
    "                                 'text_keras','rate_keras','text_nltk','rate_nltk','text_apacy','rate_spacy'])\n",
    "train_dir = './data/20news-bydate-v3/20news-bydate-train/'\n",
    "cat_dir = next(os.walk(train_dir))[1]\n",
    "count = 0\n",
    "for cat in tqdm(cat_dir):\n",
    "    in_dir = train_dir + cat + '/'\n",
    "    news_list = next(os.walk(in_dir))[2]\n",
    "    news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "    news_list = list(filter(lambda f: f.endswith(\".txt\"), news_list))\n",
    "    for news_path in news_list:\n",
    "        id_num = news_path.split('/')[-1].split('.')[0]\n",
    "        with open(news_path,'r',encoding='latin1') as f:\n",
    "            news_text = f.read()\n",
    "            news_text = re.sub('\\n',' ',news_text)\n",
    "            news_text_tok_ori = T.text_to_word_sequence(news_text,filters='',lower=False)\n",
    "            news_text_ori = \" \".join(news_text_tok_ori)\n",
    "            news_text_tok_num_ori = len(news_text_tok_ori)\n",
    "            news_text_tok_keras,rate_keras = text_preprcess(news_text,tokenizers='keras')\n",
    "            news_text_tok_nltk,rate_nltk = text_preprcess(news_text,tokenizers='nltk')\n",
    "            news_text_tok_spacy,rate_spacy = text_preprcess(news_text,tokenizers='spacy')\n",
    "        train_df.loc[count] = [id_num,cat,news_text_ori,news_text_tok_num_ori,\n",
    "                               news_text_tok_keras,rate_keras,news_text_tok_nltk,rate_nltk,news_text_tok_spacy,rate_spacy]\n",
    "        count+=1\n",
    "#         break\n",
    "    \n",
    "# train_df.loc[0] = [0,0,0]\n",
    "print(train_df.rate_keras.mean() , train_df.rate_nltk.mean(),train_df.rate_spacy.mean()  )\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* keras tokenizer: 95.95% tokens in glove\n",
    "* nltk tokenizer: 95.96% tokens in glove\n",
    "* spacy tokenizer: 96.05% tokens in glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 42.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>original_tokenize_num</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>preprocss_tokenize_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19305</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>My votes (FWIW): Team MVP: Pat Verbeek. He fan...</td>\n",
       "      <td>453</td>\n",
       "      <td>my votes fwiw team mvp pat he fans goal mouth ...</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21002</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>I have posted disp135.zip to alt.binaries.pict...</td>\n",
       "      <td>1447</td>\n",
       "      <td>i posted you distribute program freely noncomm...</td>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>21586</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>Why don't you call the City and ask? Oak Park ...</td>\n",
       "      <td>327</td>\n",
       "      <td>why city ask oak park illegal handgun ban well...</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>23146</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>-&gt; &gt;Now let me get this straight. After a nice...</td>\n",
       "      <td>301</td>\n",
       "      <td>now let straight after nice long rant how peop...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>23229</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>NEWS YOU MAY HAVE MISSED, APR 19, 1993 Not bec...</td>\n",
       "      <td>615</td>\n",
       "      <td>news you may have missed apr not busy us media...</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>19586</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>Is this the one that had the {wrench|pliers} f...</td>\n",
       "      <td>12</td>\n",
       "      <td>is inside recovery</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>22675</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>Hmm, followup on my own posting... Well, who c...</td>\n",
       "      <td>973</td>\n",
       "      <td>hmm followup posting well cares first let try ...</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>27525</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>: I'm looking to buy a 17\" monitor soon, and i...</td>\n",
       "      <td>111</td>\n",
       "      <td>i 'm looking buy monitor soon i ca n't decide ...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>26073</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>Methinks you recall wrong. Mitchell hit close ...</td>\n",
       "      <td>84</td>\n",
       "      <td>methinks recall wrong mitchell hit close atlan...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>28486</td>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>Fortunately, wire-wrapping is a better wiring ...</td>\n",
       "      <td>125</td>\n",
       "      <td>fortunately wirewrapping better wiring techniq...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>25314</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>Forsale: SONY MHC-3600 HI-FI Bookshelf stereo ...</td>\n",
       "      <td>120</td>\n",
       "      <td>forsale sony mhc hifi bookshelf stereo months ...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>17681</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>Paraphrase of initial post: \\tCan I fight a sp...</td>\n",
       "      <td>49</td>\n",
       "      <td>paraphrase initial post can i fight speeding t...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>25854</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>Please, please don't make Barney to a modern m...</td>\n",
       "      <td>33</td>\n",
       "      <td>please make barney modern mythical figure i de...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>23931</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>A relative of mine has recently been diagnosed...</td>\n",
       "      <td>43</td>\n",
       "      <td>a relative recently diagnosed stage papillary ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>26597</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>18684</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>I apologize if this post isn't entirely approp...</td>\n",
       "      <td>52</td>\n",
       "      <td>i apologize post entirely appropriate newsgrou...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>20307</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>I have heard of two packages for the PC that s...</td>\n",
       "      <td>52</td>\n",
       "      <td>i heard packages pc support xwin the linux fre...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>24375</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>What exactly does the windows bitmap format lo...</td>\n",
       "      <td>43</td>\n",
       "      <td>what exactly windows bitmap format look like i...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>28065</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>I'm wondering if anybody else out there is a c...</td>\n",
       "      <td>166</td>\n",
       "      <td>i 'm wondering anybody clutchless shifter i 'v...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>20556</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Just what do gay people do that straight peopl...</td>\n",
       "      <td>128</td>\n",
       "      <td>just gay people straight people do n't absolut...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                  category  \\\n",
       "0   19305          rec.sport.hockey   \n",
       "1   21002             comp.graphics   \n",
       "2   21586        talk.politics.guns   \n",
       "3   23146        talk.politics.misc   \n",
       "4   23229     talk.politics.mideast   \n",
       "5   19586                 sci.space   \n",
       "6   22675                 sci.crypt   \n",
       "7   27525  comp.sys.ibm.pc.hardware   \n",
       "8   26073        rec.sport.baseball   \n",
       "9   28486           sci.electronics   \n",
       "10  25314              misc.forsale   \n",
       "11  17681           rec.motorcycles   \n",
       "12  25854        talk.religion.misc   \n",
       "13  23931                   sci.med   \n",
       "14  26597     comp.sys.mac.hardware   \n",
       "15  18684    soc.religion.christian   \n",
       "16  20307            comp.windows.x   \n",
       "17  24375   comp.os.ms-windows.misc   \n",
       "18  28065                 rec.autos   \n",
       "19  20556               alt.atheism   \n",
       "\n",
       "                                                 text original_tokenize_num  \\\n",
       "0   My votes (FWIW): Team MVP: Pat Verbeek. He fan...                   453   \n",
       "1   I have posted disp135.zip to alt.binaries.pict...                  1447   \n",
       "2   Why don't you call the City and ask? Oak Park ...                   327   \n",
       "3   -> >Now let me get this straight. After a nice...                   301   \n",
       "4   NEWS YOU MAY HAVE MISSED, APR 19, 1993 Not bec...                   615   \n",
       "5   Is this the one that had the {wrench|pliers} f...                    12   \n",
       "6   Hmm, followup on my own posting... Well, who c...                   973   \n",
       "7   : I'm looking to buy a 17\" monitor soon, and i...                   111   \n",
       "8   Methinks you recall wrong. Mitchell hit close ...                    84   \n",
       "9   Fortunately, wire-wrapping is a better wiring ...                   125   \n",
       "10  Forsale: SONY MHC-3600 HI-FI Bookshelf stereo ...                   120   \n",
       "11  Paraphrase of initial post: \\tCan I fight a sp...                    49   \n",
       "12  Please, please don't make Barney to a modern m...                    33   \n",
       "13  A relative of mine has recently been diagnosed...                    43   \n",
       "14                                                                        0   \n",
       "15  I apologize if this post isn't entirely approp...                    52   \n",
       "16  I have heard of two packages for the PC that s...                    52   \n",
       "17  What exactly does the windows bitmap format lo...                    43   \n",
       "18  I'm wondering if anybody else out there is a c...                   166   \n",
       "19  Just what do gay people do that straight peopl...                   128   \n",
       "\n",
       "                                      preprocess_text preprocss_tokenize_num  \n",
       "0   my votes fwiw team mvp pat he fans goal mouth ...                    229  \n",
       "1   i posted you distribute program freely noncomm...                    912  \n",
       "2   why city ask oak park illegal handgun ban well...                    180  \n",
       "3   now let straight after nice long rant how peop...                    150  \n",
       "4   news you may have missed apr not busy us media...                    368  \n",
       "5                                  is inside recovery                      3  \n",
       "6   hmm followup posting well cares first let try ...                    481  \n",
       "7   i 'm looking buy monitor soon i ca n't decide ...                     56  \n",
       "8   methinks recall wrong mitchell hit close atlan...                     39  \n",
       "9   fortunately wirewrapping better wiring techniq...                     71  \n",
       "10  forsale sony mhc hifi bookshelf stereo months ...                     80  \n",
       "11  paraphrase initial post can i fight speeding t...                     35  \n",
       "12  please make barney modern mythical figure i de...                     17  \n",
       "13  a relative recently diagnosed stage papillary ...                     23  \n",
       "14                                                                         0  \n",
       "15  i apologize post entirely appropriate newsgrou...                     26  \n",
       "16  i heard packages pc support xwin the linux fre...                     23  \n",
       "17  what exactly windows bitmap format look like i...                     24  \n",
       "18  i 'm wondering anybody clutchless shifter i 'v...                     96  \n",
       "19  just gay people straight people do n't absolut...                     76  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(columns=['id','category','text','original_tokenize_num','preprocess_text','preprocss_tokenize_num'])\n",
    "train_dir = './data/20news-bydate-v3/20news-bydate-train/'\n",
    "cat_dir = next(os.walk(train_dir))[1]\n",
    "count = 0\n",
    "for cat in tqdm(cat_dir):\n",
    "    in_dir = train_dir + cat + '/'\n",
    "    news_list = next(os.walk(in_dir))[2]\n",
    "    news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "    news_list = list(filter(lambda f: f.endswith(\".txt\"), news_list))\n",
    "    for news_path in news_list:\n",
    "        id_num = news_path.split('/')[-1].split('.')[0]\n",
    "        with open(news_path,'r',encoding='latin1') as f:\n",
    "            news_text = f.read()\n",
    "            news_text = re.sub('\\n',' ',news_text)\n",
    "            news_text_tok_ori = T.text_to_word_sequence(news_text,filters='',lower=False)\n",
    "            news_text_ori = \" \".join(news_text_tok_ori)\n",
    "            news_text_tok_num_ori = len(news_text_tok_ori)\n",
    "#             news_text_tok_pre,rate_keras = text_preprcess(news_text,tokenizers='keras')\n",
    "#             news_text_tok_pre,rate_nltk = text_preprcess(news_text,tokenizers='nltk')\n",
    "            news_text_tok_pre,rate_spacy = text_preprcess(news_text,tokenizers='spacy')\n",
    "            news_text_pre = \" \".join(news_text_tok_pre)\n",
    "        train_df.loc[count] = [id_num,cat,news_text_ori,news_text_tok_num_ori,news_text_pre,len(news_text_tok_pre)]\n",
    "        count+=1\n",
    "        break\n",
    "    \n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have posted disp135.zip to alt.binaries.pictures.utilities ****** You may distribute this program freely for non-commercial use if no fee is gained. ****** There is no warranty. The author is not responsible for any damage caused by this program. Important changes since version 1,30: Fix bugs in file management system (file displaying). Improve file management system (more user-friendly). Fix bug in XPM version 3 reading. Fix bugs in TARGA reading/writng. Fix bug in GEM/IMG reading. Add support for PCX and GEM/IMG writing. Auto-skip macbinary header. (1) Introduction: This program can let you READ, WRITE and DISPLAY images with different formats. It also let you do some special effects(ROTATION, DITHERING ....) on image. Its main purpose is to let you convert image among different formts. Include simple file management system. Support \\'slide show\\'. There is NO LIMIT on image size. Currently this program supports 8, 15, 16, 24 bits display. If you want to use HiColor or TrueColor, you must have VESA driver. If you want to modify video driver, please read section (8). (2) Hardware Requirement: PC 386 or better. MSDOS 3,3 or higher. min amount of ram is 4M bytes(Maybe less memory will also work). (I recommend min 8M bytes for better performance). Hard disk for swapping(virtual memory). The following description is borrowed from DJGPP. Supported Wares: * Up to 128M of extended memory (expanded under VCPI) * Up to 128M of disk space used for swapping * SuperVGA 256-color mode up to 1024x768 * 80387 * XMS & VDISK memory allocation strategies * VCPI programs, such as QEMM, DESQview, and 386MAX Unsupported: * DPMI * Microsoft Windows Features: 80387 emulator, 32-bit unix-ish environment, flat memory model, SVGA graphics. (3) Installation: Video drivers, emu387 and go32.exe are borrowed from DJGPP. (If you use Western Digital VGA chips, read readme.wd) (This GO32.EXE is a modified version for vesa and is COMPLETELY compatible with original version) + *** But some people report that this go32.exe is not compatible with + other DJGPP programs in their system. If you encounter this problem, + DON\\'T put go32.exe within search path. *** Please read runme.bat for how to run this program. If you choose xxxxx.grn as video driver, add \\'nc 256\\' to environment GO32. For example, go32=driver x:/xxxxx/xxxxx.grn nc 256 If you don\\'t have 80x87, add \\'emu x:/xxxxx/emu387\\' to environment GO32. For example, go32=driver x:/xxxxx/xxxxx.grd emu x:/xxxxx/emu387 **** Notes: 1. I only test tr8900.grn, et4000.grn and vesa.grn. Other drivers are not tested. 2. I have modified et4000.grn to support 8, 15, 16, 24 bits display. You don\\'t need to use vesa driver. If et4000.grn doesn\\'t work, please try vesa.grn. 3. For those who want to use HiColor or TrueColor display, please use vesa.grn(except et4000 users). You can find vesa BIOS driver from : wuarchive.wustl.edu: /mirrors/msdos/graphics godzilla.cgl.rmit.oz.au: /kjb/MGL (4) Command Line Switch: + Usage : display [-d|--display initial_display_type] + [-s|--sort sort_method] + [-h|-?] Display type: 8(SVGA,default), 15, 16(HiColor), 24(TrueColor) + Sort method: \\'name\\', \\'ext\\' (5) Function Key: F2 : Change disk drive + CTRL-A -- CTRL-Z : change disk drive. F3 : Change filename mask (See match.doc) F4 : Change parameters F5 : Some effects on picture, eg. flip, rotate .... F7 : Make Directory t : Tag file + : Tag group files (See match.doc) T : Tag all files u : Untag file - : Untag group files (See match.doc) U : Untag all files Ins : Change display type (8,15,16,24) in \\'read\\' & \\'screen\\' menu. F6,m,M : Move file(s) F8,d,D : Delete file(s) r,R : Rename file c,C : Copy File(s) z,Z : Display first 10 bytes in Ascii, Hex and Dec modes. + f,F : Display disk free space. Page Up/Down : Move one page TAB : Change processing target. Arrow keys, Home, End, Page Up, Page Down: Scroll image. Home: Left Most. End: Right Most. Page Up: Top Most. Page Down: Bottom Most. in \\'screen\\' & \\'effect\\' menu : Left,Right arrow: Change display type(8, 15, 16, 24 bits) s,S : Slide Show. ESCAPE to terminate. ALT-X : Quit program without prompting. + ALT-A : Reread directory. Escape : Abort function and return. (6) Support Format: Read: GIF(.gif), Japan MAG(.mag), Japan PIC(.pic), Sun Raster(.ras), Jpeg(.jpg), XBM(.xbm), Utah RLE(.rle), PBM(.pbm), PGM(.pgm), PPM(.ppm), PM(.pm), PCX(.pcx), Japan MKI(.mki), Tiff(.tif), Targa(.tga), XPM(.xpm), Mac Paint(.mac), GEM/IMG(.img), IFF/ILBM(.lbm), Window BMP(.bmp), QRT ray tracing(.qrt), Mac PICT(.pct), VIS(.vis), PDS(.pds), VIKING(.vik), VICAR(.vic), FITS(.fit), Usenix FACE(.fac). the extensions in () are standard extensions. Write: GIF, Sun Raster, Jpeg, XBM, PBM, PGM, PPM, PM, Tiff, Targa, XPM, Mac Paint, Ascii, Laser Jet, IFF/ILBM, Window BMP, + Mac PICT, VIS, FITS, FACE, PCX, GEM/IMG. All Read/Write support full color(8 bits), grey scale, b/w dither, and 24 bits image, if allowed for that format. (7) Detail: Initialization: Set default display type to highest display type. Find allowable screen resolution(for .grn video driver only). 1. When you run this program, you will enter \\'read\\' menu. Whthin this menu you can press any function key except F5. If you move or copy files, you will enter \\'write\\' menu. the \\'write\\' menu is much like \\'read\\' menu, but only allow you to change directory. + The header line in \\'read\\' menu includes \"(d:xx,f:xx,t:xx)\". + d : display type. f: number of files. t: number of tagged files. pressing SPACE in \\'read\\' menu will let you select which format to use for reading current file. pressing RETURN in \\'read\\' menu will let you reading current file. This program will automatically determine which format this file is. The procedure is: First, check magic number. If fail, check standard extension. Still fail, report error. pressing s or S in \\'read\\' menu will do \\'Slide Show\\'. If delay time is 0, program will wait until you hit a key (except ESCAPE). If any error occurs, program will make a beep. ESCAPE to terminate. pressing Ins in \\'read\\' menu will change display type. pressing ALT-X in \\'read\\' menu will quit program without prompting. 2. Once image file is successfully read, you will enter \\'screen\\' menu. Within this menu F5 is turn on. You can do special effect on image. pressing RETURN: show image. in graphic mode, press RETURN, SPACE or ESCAPE to return to text mode. pressing TAB: change processing target. This program allows you to do special effects on 8-bit or 24-bit image. pressing Left,Right arrow: change display type. 8, 15, 16, 24 bits. pressing SPACE: save current image to file. B/W Dither: save as black/white image(1 bit). Grey Scale: save as grey image(8 bits). Full Color: save as color image(8 bits). True Color: save as 24-bit image. This program will ask you some questions if you want to write image to file. Some questions are format-dependent. Finally This program will prompt you a filename. If you want to save file under another directory other than current directory, please press SPACE. after pressing SPACE, you will enter \\'write2\\' menu. You can change directory to what you want. Then, pressing SPACE: this program will prompt you \\'original\\' filename. pressing RETURN: this program will prompt you \\'selected\\' filename (filename under bar). 3. This program supports 8, 15, 16, 24 bits display. 4. This Program is MEMORY GREEDY. If you don\\'t have enough memory, the performance is poor. 5. If you want to save 8 bits image : try GIF then TIFF(LZW) then TARGA then Sun Raster then BMP then ... If you want to save 24 bits image (lossless): try TIFF(LZW) or TARGA or ILBM or Sun Raster (No one is better for true 24bits image) 6. I recommend Jpeg for storing 24 bits images, even 8 bits images. 7. Not all subroutines are fully tested 8. This document is not well written. If you have any PROBLEM, SUGGESTION, COMMENT about this program, Please send to u7711501@bicmos.ee.nctu.edu.tw (140,113,11,13). I need your suggestion to improve this program. (There is NO anonymous ftp on this site) (8) Tech. information: Program (user interface and some subroutines) written by Jih-Shin Ho. Some subroutines are borrowed from XV(2,21) and PBMPLUS(dec 91). Tiff(V3,2) and Jpeg(V4) reading/writing are through public domain libraries. Compiled with DJGPP. You can get whole DJGPP package from SIMTEL20 or mirror sites. For example, wuarchive.wustl.edu: /mirrors/msdos/djgpp (9) For Thoese who want to modify video driver: 1. get GRX source code from SIMTEL20 or mirror sites. 2. For HiColor and TrueColor: 15 bits : # of colors is set to 32768. 16 bits : # of colors is set to 0xc010. 24 bits : # of colors is set to 0xc018. Acknowledgment: I would like to thank the authors of XV and PBMPLUS for their permission to let me use their subroutines. Also I will thank the authors who write Tiff and Jpeg libraries. Thank DJ. Without DJGPP I can\\'t do any thing on PC.'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[1,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(all_length):\n",
    "    '''\n",
    "    input: length list of elements e.g.[1,1,1,3,5,9,4,2,1,3,54,78,5...]\n",
    "    output1: mean、std、mode、min、q1、median(q2)、q3、max、iqr、outlier、far out\n",
    "    output2: statistics graph、10%~90% form\n",
    "    '''\n",
    "    stat_dict = {}\n",
    "    stat_dict['mean'] = np.mean(all_length)\n",
    "    stat_dict['std'] = np.std(all_length)\n",
    "    stat_dict['mode'] = np.argmax(np.bincount(all_length))\n",
    "    stat_dict['min'] = np.min(all_length)\n",
    "    stat_dict['q1'] = np.quantile(all_length,0.25)\n",
    "    stat_dict['median'] = np.quantile(all_length,0.5)\n",
    "    stat_dict['q3'] = np.quantile(all_length,0.75)\n",
    "    stat_dict['max'] = np.max(all_length)\n",
    "    stat_dict['iqr'] = stat_dict['q3'] - stat_dict['q1']\n",
    "    stat_dict['outlier'] = stat_dict['q3'] + 1.5*stat_dict['iqr']\n",
    "    stat_dict['far_out'] = stat_dict['q3'] + 3*stat_dict['iqr']\n",
    "    for i in [10,20,30,40,50,60,70,80,90,100]:\n",
    "        stat_dict[str(i)+'%'] = np.percentile(all_length,i)\n",
    "    return pd.DataFrame.from_dict(stat_dict,orient='index',columns=['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>144.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>214.881008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mode</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q1</td>\n",
       "      <td>23.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>median</td>\n",
       "      <td>63.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q3</td>\n",
       "      <td>157.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>912.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iqr</td>\n",
       "      <td>133.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>outlier</td>\n",
       "      <td>358.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>far_out</td>\n",
       "      <td>558.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10%</td>\n",
       "      <td>15.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20%</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30%</td>\n",
       "      <td>25.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40%</td>\n",
       "      <td>37.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>63.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60%</td>\n",
       "      <td>77.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70%</td>\n",
       "      <td>112.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80%</td>\n",
       "      <td>189.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90%</td>\n",
       "      <td>379.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100%</td>\n",
       "      <td>912.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "mean     144.450000\n",
       "std      214.881008\n",
       "mode      23.000000\n",
       "min        0.000000\n",
       "q1        23.750000\n",
       "median    63.500000\n",
       "q3       157.500000\n",
       "max      912.000000\n",
       "iqr      133.750000\n",
       "outlier  358.125000\n",
       "far_out  558.750000\n",
       "10%       15.600000\n",
       "20%       23.000000\n",
       "30%       25.400000\n",
       "40%       37.400000\n",
       "50%       63.500000\n",
       "60%       77.600000\n",
       "70%      112.200000\n",
       "80%      189.800000\n",
       "90%      379.300000\n",
       "100%     912.000000"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_num_li = train_df.preprocss_tokenize_num.tolist()\n",
    "basic_statistics(tokens_num_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just',\n",
       " 'what',\n",
       " 'do',\n",
       " 'gay',\n",
       " 'people',\n",
       " 'do',\n",
       " 'that',\n",
       " 'straight',\n",
       " 'people',\n",
       " \"don't?\",\n",
       " 'absolutely',\n",
       " 'nothing.',\n",
       " \"i'm\",\n",
       " 'a',\n",
       " 'very',\n",
       " 'straight(as',\n",
       " 'an',\n",
       " 'arrow),',\n",
       " '17-year',\n",
       " 'old',\n",
       " 'male',\n",
       " 'that',\n",
       " 'is',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bsa.',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'care',\n",
       " 'what',\n",
       " 'gay',\n",
       " 'people',\n",
       " 'do',\n",
       " 'among',\n",
       " 'each',\n",
       " 'other,',\n",
       " 'as',\n",
       " 'long',\n",
       " 'as',\n",
       " 'they',\n",
       " \"don't\",\n",
       " 'make',\n",
       " 'passes',\n",
       " 'at',\n",
       " 'me',\n",
       " 'or',\n",
       " 'anything.',\n",
       " 'at',\n",
       " 'my',\n",
       " 'summer',\n",
       " 'camp',\n",
       " 'where',\n",
       " 'i',\n",
       " 'work,',\n",
       " 'my',\n",
       " 'boss',\n",
       " 'is',\n",
       " 'gay.',\n",
       " 'not',\n",
       " 'in',\n",
       " 'a',\n",
       " \"'pansy'\",\n",
       " 'way',\n",
       " 'of',\n",
       " 'gay',\n",
       " '(i',\n",
       " 'know',\n",
       " 'a',\n",
       " 'few),',\n",
       " 'but',\n",
       " 'just',\n",
       " \"'one\",\n",
       " 'of',\n",
       " 'the',\n",
       " \"guys'.\",\n",
       " 'he',\n",
       " \"doesn't\",\n",
       " 'push',\n",
       " 'anything',\n",
       " 'on',\n",
       " 'me,',\n",
       " 'and',\n",
       " 'we',\n",
       " 'give',\n",
       " 'him',\n",
       " 'the',\n",
       " 'same',\n",
       " 'respect',\n",
       " 'back,',\n",
       " 'due',\n",
       " 'to',\n",
       " 'his',\n",
       " 'position.',\n",
       " 'if',\n",
       " 'anything,',\n",
       " 'the',\n",
       " 'bsa',\n",
       " 'has',\n",
       " 'taught',\n",
       " 'me,',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know,',\n",
       " 'tolerance',\n",
       " 'or',\n",
       " 'something.',\n",
       " 'before',\n",
       " 'i',\n",
       " 'met',\n",
       " 'this',\n",
       " 'guy,',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'all',\n",
       " 'gays',\n",
       " 'were',\n",
       " \"'faries'.\",\n",
       " 'so,',\n",
       " 'the',\n",
       " 'bsa',\n",
       " 'has',\n",
       " 'taught',\n",
       " 'me',\n",
       " 'to',\n",
       " 'be',\n",
       " 'an',\n",
       " 'antibigot.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/20news-bydate-v3/20news-bydate-train/alt.atheism/20556.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20748.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20810.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20574.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20692.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20705.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20519.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20565.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20835.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20649.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20684.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20872.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20739.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20547.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20578.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20704.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20932.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20528.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20769.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20869.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20549.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20776.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20936.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20980.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20780.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20515.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20575.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20766.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20750.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20562.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20700.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20594.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20972.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20801.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20910.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20756.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20883.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20539.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20687.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20852.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20945.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20734.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20865.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20694.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20897.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20833.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20966.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20808.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20816.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20793.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20893.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20926.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20975.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20914.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20952.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20913.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20879.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20925.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20666.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20958.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20760.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20881.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20640.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20834.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20600.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20727.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20842.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20768.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20829.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20871.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20741.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20886.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20927.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20652.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20679.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20586.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20527.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20976.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20613.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20794.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20559.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20595.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20959.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20744.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20541.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20717.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20589.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20725.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20526.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20598.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20802.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20674.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20901.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20667.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20538.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20577.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20693.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20853.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20938.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20545.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20823.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20859.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20803.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20921.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20861.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20832.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20622.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20680.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20696.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20988.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20747.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20805.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20627.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20858.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20977.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20686.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20800.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20664.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20792.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20698.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20795.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20770.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20688.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20683.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20807.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20531.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20764.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20553.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20954.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20827.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20767.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20916.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20896.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20782.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20970.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20939.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20955.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20894.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20646.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20951.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20819.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20946.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20979.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20905.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20847.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20864.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20570.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20611.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20870.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20779.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20635.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20900.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20665.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20580.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20670.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20840.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20787.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20962.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20701.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20542.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20956.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20737.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20612.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20743.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20522.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20918.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20618.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20814.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20763.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20851.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20728.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20662.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20745.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20517.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20609.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20676.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20839.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20848.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20891.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20825.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20950.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20758.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20911.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20751.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20902.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20775.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20548.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20535.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20623.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20604.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20762.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20566.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20690.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20711.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20552.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20655.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20581.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20984.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20681.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20607.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20788.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20532.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20774.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20544.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20678.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20812.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20981.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20944.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20536.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20620.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20647.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20785.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20804.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20878.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20843.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20659.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20935.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20672.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20551.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20629.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20703.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20986.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20720.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20621.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20749.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20917.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20746.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20930.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20707.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20880.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20657.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20540.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20671.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20656.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20636.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20815.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20772.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20862.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20963.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20850.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20663.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20571.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20799.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20860.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20929.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20601.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20875.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20854.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20765.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20989.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20637.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20985.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20948.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20730.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20715.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20919.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20863.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20983.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20721.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20754.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20633.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20709.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20759.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20806.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20550.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20777.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20937.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20895.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20661.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20658.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20568.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20898.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20723.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20753.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20731.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20824.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20738.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20923.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20564.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20884.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20726.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20903.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20610.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20855.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20874.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20922.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20521.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20625.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20596.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20836.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20887.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20933.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20818.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20890.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20991.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20628.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20543.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20994.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20591.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20642.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20826.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20588.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20654.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20987.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20846.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20668.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20561.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20587.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20602.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20899.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20811.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20648.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20634.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20892.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20572.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20554.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20724.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20638.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20789.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20632.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20906.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20590.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20877.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20742.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20710.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20631.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20817.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20888.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20830.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20771.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20733.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20518.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20904.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20828.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20982.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20974.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20555.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20920.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20856.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20644.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20606.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20608.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20845.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20524.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20821.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20791.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20953.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20689.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20708.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20626.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20643.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20691.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20786.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20943.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20530.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20809.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20957.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20908.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20978.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20928.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20716.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20961.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20605.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20857.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20889.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20660.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20712.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20567.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20624.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20844.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20702.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20778.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20569.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20560.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20582.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20729.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20682.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20735.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20614.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20573.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20558.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20973.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20713.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20740.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20675.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20736.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20523.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20617.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20706.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20576.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20525.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20968.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20866.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20603.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20534.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20516.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20924.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20971.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20761.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20599.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20650.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20820.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20563.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20990.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20597.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20585.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20755.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20868.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20992.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20949.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20876.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20993.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20615.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20630.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20645.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20533.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20867.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20685.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20639.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20907.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20882.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20813.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20695.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20641.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20838.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20722.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20699.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20967.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20520.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20781.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20885.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20837.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20960.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20796.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20841.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20969.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20546.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20557.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20942.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20784.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20912.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20790.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20940.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20529.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20909.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20941.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20583.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20752.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20965.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20593.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20915.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20616.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20592.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20653.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20697.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20714.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20579.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20831.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20783.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20773.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20873.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20651.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20931.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20947.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20718.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20798.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20669.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20619.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20732.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20584.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20673.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20964.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20822.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20934.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20849.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20797.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20537.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20757.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20677.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20719.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_list = next(os.walk(in_dir))[2]\n",
    "news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/leoqaz12/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/leoqaz12/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "| dictionary: 50264 types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (decoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerSentenceEncoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\n",
    "roberta.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"now let straight after nice long rant how people need personal responsibility economic social lives sudden 's radicals such me i guess responsible poor people 's lifestyles tell think poor people dumb think themselves there reasons disintegration family and support systems general nation 's poor somehow i think murphy janis the sane person 's list you want generation 's vaunted cultural revolution lasting change worse try socalled relevant values education hey like good idea time how know needed real education mean took granted the 's generation spoiled irresponsible the depression create mothers fathers determined kids want going overboard creating nation brats consider contrast famous events july apollo woodstock which group large numbers people feed reverted cultural level primitives defecation public etc and group assembled took care itself dispersed damage deaths large numbers drug problems was n't woodstock called biggest parking lot history they rejected society went nature parent 's cars\""
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 tensor([    0,  8310,   905,  1359,    71,  2579,   251, 25693,   141,    82,\n",
      "          240,  1081,  2640,   776,   592,  1074,  7207,   128,    29, 35842,\n",
      "          215,   162,   939,  4443,  2149,  2129,    82,   128,    29, 28182,\n",
      "         1137,   206,  2129,    82, 16881,   206,  1235,    89,  2188, 32654,\n",
      "         8475,   284,     8,   323,  1743,   937,  1226,   128,    29,  2129,\n",
      "         7421,   939,   206, 22802, 16628, 10408,   354,     5, 37091,   621,\n",
      "          128,    29,   889,    47,   236,  2706,   128,    29,   748, 19264,\n",
      "         4106,  7977,  9735,   464,  3007,   860, 17380,  9315,  4249,  3266,\n",
      "         1265, 17232,   101,   205,  1114,    86,   141,   216,   956,   588,\n",
      "         1265,  1266,   362,  4159,     5,   128,    29,  2706, 29136, 21573,\n",
      "            5,  6943,  1045,  8826, 17850,  3030,  1159,   236,   164, 35912,\n",
      "         2351,  1226,  5378,  2923,  1701,  5709,  3395,  1061,  1236, 21347,\n",
      "         6256, 38520,  5627,  9607,    61,   333,   739,  1530,    82,  3993,\n",
      "        41411,  4106,   672,  9156, 35143,  3816,  3204,  1258,   285,  4753,\n",
      "            8,   333, 14525,   362,   575,  1495, 32807,  1880,  3257,   739,\n",
      "         1530,  1262,  1272,    21,   295,    75,  5627,  9607,   373,   934,\n",
      "         2932,   319,   750,    51,  3946,  2313,   439,  2574,  4095,   128,\n",
      "           29,  1677,     2])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-f46ab41149aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features_aligned_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_ori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/roberta/hub_interface.py\u001b[0m in \u001b[0;36mextract_features_aligned_to_words\u001b[0;34m(self, sentence, return_all_hiddens)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_toks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0maligned_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignment_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_features_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malignment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# wrap in spaCy Doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/roberta/alignment_utils.py\u001b[0m in \u001b[0;36malign_features_to_words\u001b[0;34m(roberta, features, alignment)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;31m#assert torch.all(torch.abs(output.sum(dim=0) - features.sum(dim=0)) < 1e-4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokens_ori = train_df.loc[3,'preprocess_text']#'how\\'s are you'#\n",
    "tokens = roberta.encode(tokens_ori)\n",
    "# assert tokens.tolist() == [0, 31414, 232, 328, 2]\n",
    "# assert roberta.decode(tokens) == 'Hello world!'\n",
    "print(len(tokens) , tokens)\n",
    "roberta.decode(tokens)\n",
    "doc = roberta.extract_features_aligned_to_words(tokens_ori)\n",
    "print(len(doc))\n",
    "for tok in doc:\n",
    "    print('{:10}{} (...)'.format(str(tok), tok.vector[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, ['I', 'said', ',', '\"', 'hello', 'RoBERTa', '.', '\"'])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = 'I said, \"hello RoBERTa.\"'\n",
    "tokens = sp_tokenizer(tokens)\n",
    "token_li = []\n",
    "for token in tokens:\n",
    "    token_li.append(token.text)\n",
    "len(token_li) , token_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tokens exceeds maximum length: 592 > 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-7a2d631178ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast_layer_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# == torch.Size([1, 5, 1024])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/roberta/hub_interface.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, tokens, return_all_hiddens)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             raise ValueError('tokens exceeds maximum length: {} > {}'.format(\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             ))\n\u001b[1;32m     84\u001b[0m         features, extra = self.model(\n",
      "\u001b[0;31mValueError\u001b[0m: tokens exceeds maximum length: 592 > 512"
     ]
    }
   ],
   "source": [
    "last_layer_features = roberta.extract_features(tokens)\n",
    "print(last_layer_features.size())# == torch.Size([1, 5, 1024])\n",
    "\n",
    "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)\n",
    "assert len(all_layers) == 25\n",
    "assert torch.all(all_layers[-1] == last_layer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 19, 1024)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer_features.detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 898823/898823 [00:06<00:00, 134563.64B/s]\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 545019.70B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0925 15:40:29.729804 139833327294272 tokenization_utils.py:665] Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "590"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(train_df.loc[0,'text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
