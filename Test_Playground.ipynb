{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, csv\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import xlsxwriter\n",
    "import random\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import unicodedata as udata\n",
    "# import pause, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from distutils.dir_util import copy_tree\n",
    "import sklearn\n",
    "from sklearn.metrics import *\n",
    "import itertools as it\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import functools\n",
    "import spacy\n",
    "\n",
    "import tensorflow.keras.preprocessing.text as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_1 = (stopwords.words('english')) #Stopword\n",
    "with open('data/preprocess/stop_words.txt') as f:\n",
    "    stop_words_2 = f.read().splitlines() #stop_list1\n",
    "stop_words_3 = pickle.load(open('data/preprocess/stop_list2.pkl','rb'))\n",
    "stop_words_all = set(stop_words_1 + stop_words_2 + stop_words_3)\n",
    "len(stop_words_all) # stopwords#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk = ['A','B']\n",
    "[x.lower() for x in kk]\n",
    "'A5555'.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A', 'a'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['A','a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', \"'s\", '(', 'beautiful', ')', '4toys', 'does', \"n't\"]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenizer = spacy.load(\"en\")\n",
    "kk = sp_tokenizer(\"It's (beautiful) 4toys doesn't\")\n",
    "token_li=[]\n",
    "for token in kk:\n",
    "    token_li.append(token.text)\n",
    "token_li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196009"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenizer = spacy.load(\"en\")\n",
    "dil= r\"[!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~]+\\ *\" \n",
    "dil_filter = '!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_\\`{|}~\\t\\n'\n",
    "glove = pd.read_table('./data/preprocess/glove.840B.300d.txt', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "glove_words = set(glove.index.tolist())\n",
    "len(glove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprcess(text,tokenizers='nltk'):\n",
    "    '''\n",
    "    Input: string\n",
    "    Return: preprocessed list\n",
    "    '''\n",
    "    tokens = [i for i in text.split() if i not in stop_words_all]\n",
    "    tokens_str = ' '.join(tokens)\n",
    "    if tokenizers == 'keras':\n",
    "        tokens = T.text_to_word_sequence(tokens_str,filters=dil_filter) # lower casse & tokenize\n",
    "        tokens = list(filter(None, tokens))\n",
    "    elif tokenizers == 'nltk':\n",
    "#         tokens = re.sub(dil,\" \",tokens_str.lower())\n",
    "        tokens = [i for i in tokens_str.lower() if i not in dil_filter]\n",
    "        tokens = ''.join(tokens)\n",
    "        tokens = word_tokenize(tokens)\n",
    "        tokens = list(filter(None, tokens))\n",
    "#         tokens = [x.lower() for x in tokens]\n",
    "    elif tokenizers == 'spacy':\n",
    "#         tokens = re.sub(dil,\" \",tokens_str.lower())\n",
    "        tokens = [i for i in tokens_str.lower() if i not in dil_filter]\n",
    "        tokens = ''.join(tokens)\n",
    "        tokens = sp_tokenizer(tokens)\n",
    "        token_li = []\n",
    "        for token in tokens:\n",
    "            token_li.append(token.text)\n",
    "        tokens = list(filter(None, token_li))\n",
    "#         tokens = [x.lower() for x in token_li]\n",
    "    tokens_li = []\n",
    "    for token in tokens:\n",
    "        tokens_li.append(''.join([i for i in token if not i.isdigit()]))\n",
    "    tokens_str = ' '.join(tokens_li)\n",
    "    tokens_str = re.sub(dil,\" \",tokens_str.lower())\n",
    "    tokens_li = tokens_str.split()\n",
    "    tokens_li = list(filter(None, tokens_li))\n",
    "    ori_tokens_num = len(tokens_li)\n",
    "    tokens_li = [i for i in tokens_li if i in glove_words]\n",
    "    try:\n",
    "        rate = len(tokens_li)/ori_tokens_num\n",
    "    except ZeroDivisionError:\n",
    "        rate = np.nan\n",
    "    return tokens_li , rate#ori_tokens_num/len(tokens_li)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:53<00:00, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595051529116455 0.959558811893717 0.9605178211130393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>original_tokenize_num</th>\n",
       "      <th>text_keras</th>\n",
       "      <th>rate_keras</th>\n",
       "      <th>text_nltk</th>\n",
       "      <th>rate_nltk</th>\n",
       "      <th>text_apacy</th>\n",
       "      <th>rate_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19305</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>My votes (FWIW): Team MVP: Pat Verbeek. He fan...</td>\n",
       "      <td>453</td>\n",
       "      <td>[my, votes, fwiw, team, mvp, pat, he, fans, go...</td>\n",
       "      <td>0.950207</td>\n",
       "      <td>[my, votes, fwiw, team, mvp, pat, he, fans, go...</td>\n",
       "      <td>0.950207</td>\n",
       "      <td>[my, votes, fwiw, team, mvp, pat, he, fans, go...</td>\n",
       "      <td>0.950207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19313</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Kovalev is too talented a player to play for R...</td>\n",
       "      <td>160</td>\n",
       "      <td>[kovalev, talented, player, play, roger, niels...</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>[kovalev, talented, player, play, roger, needs...</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>[kovalev, talented, player, play, roger, needs...</td>\n",
       "      <td>0.938272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19122</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>New Jersey 1 0 2--3 Pittsburgh 2 3 1--6 First ...</td>\n",
       "      <td>919</td>\n",
       "      <td>[new, jersey, pittsburgh, first, period, pitts...</td>\n",
       "      <td>0.867299</td>\n",
       "      <td>[new, jersey, pittsburgh, first, period, pitts...</td>\n",
       "      <td>0.867299</td>\n",
       "      <td>[new, jersey, pittsburgh, first, period, pitts...</td>\n",
       "      <td>0.867299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19312</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>First of all, the Penguins WILL win the cup ag...</td>\n",
       "      <td>66</td>\n",
       "      <td>[first, all, penguins, will, win, cup, again, ...</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>[first, all, penguins, will, win, cup, again, ...</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>[first, all, penguins, will, win, cup, again, ...</td>\n",
       "      <td>0.981481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>19115</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>You can't. But good luck trying.</td>\n",
       "      <td>6</td>\n",
       "      <td>[you, can't, but, good, luck, trying]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[you, ca, n't, but, good, luck, trying]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[you, ca, n't, but, good, luck, trying]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>20797</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>: When the object of their belief is said to b...</td>\n",
       "      <td>269</td>\n",
       "      <td>[when, object, belief, said, perfect, make, be...</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>[when, object, belief, said, perfect, make, be...</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>[when, object, belief, said, perfect, make, be...</td>\n",
       "      <td>0.978102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>20537</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Koff! You mean that as long as I put you to sl...</td>\n",
       "      <td>23</td>\n",
       "      <td>[koff, you, mean, long, i, sleep, first, i, ki...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[koff, you, mean, long, i, sleep, first, i, ki...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[koff, you, mean, long, i, sleep, first, i, ki...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>20757</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>: Mr Connor's assertion that \"more complex\" ==...</td>\n",
       "      <td>135</td>\n",
       "      <td>[mr, assertion, more, complex, later, paleonto...</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>[mr, connor, 's, assertion, more, complex, lat...</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>[mr, connor, 's, assertion, more, complex, lat...</td>\n",
       "      <td>0.985915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>20677</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>(excess stuff deleted...) I know of a similar ...</td>\n",
       "      <td>139</td>\n",
       "      <td>[excess, stuff, deleted, i, know, similar, inc...</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>[excess, stuff, deleted, i, know, similar, inc...</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>[excess, stuff, deleted, i, know, similar, inc...</td>\n",
       "      <td>0.987952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>20719</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>The \"System\" refered to a \"moral system\". You ...</td>\n",
       "      <td>40</td>\n",
       "      <td>[the, system, refered, moral, system, you, sho...</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>[the, system, refered, moral, system, you, hav...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[the, system, refered, moral, system, you, sho...</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id          category  \\\n",
       "0      19305  rec.sport.hockey   \n",
       "1      19313  rec.sport.hockey   \n",
       "2      19122  rec.sport.hockey   \n",
       "3      19312  rec.sport.hockey   \n",
       "4      19115  rec.sport.hockey   \n",
       "...      ...               ...   \n",
       "11309  20797       alt.atheism   \n",
       "11310  20537       alt.atheism   \n",
       "11311  20757       alt.atheism   \n",
       "11312  20677       alt.atheism   \n",
       "11313  20719       alt.atheism   \n",
       "\n",
       "                                                    text  \\\n",
       "0      My votes (FWIW): Team MVP: Pat Verbeek. He fan...   \n",
       "1      Kovalev is too talented a player to play for R...   \n",
       "2      New Jersey 1 0 2--3 Pittsburgh 2 3 1--6 First ...   \n",
       "3      First of all, the Penguins WILL win the cup ag...   \n",
       "4                       You can't. But good luck trying.   \n",
       "...                                                  ...   \n",
       "11309  : When the object of their belief is said to b...   \n",
       "11310  Koff! You mean that as long as I put you to sl...   \n",
       "11311  : Mr Connor's assertion that \"more complex\" ==...   \n",
       "11312  (excess stuff deleted...) I know of a similar ...   \n",
       "11313  The \"System\" refered to a \"moral system\". You ...   \n",
       "\n",
       "      original_tokenize_num  \\\n",
       "0                       453   \n",
       "1                       160   \n",
       "2                       919   \n",
       "3                        66   \n",
       "4                         6   \n",
       "...                     ...   \n",
       "11309                   269   \n",
       "11310                    23   \n",
       "11311                   135   \n",
       "11312                   139   \n",
       "11313                    40   \n",
       "\n",
       "                                              text_keras  rate_keras  \\\n",
       "0      [my, votes, fwiw, team, mvp, pat, he, fans, go...    0.950207   \n",
       "1      [kovalev, talented, player, play, roger, niels...    0.951220   \n",
       "2      [new, jersey, pittsburgh, first, period, pitts...    0.867299   \n",
       "3      [first, all, penguins, will, win, cup, again, ...    0.981481   \n",
       "4                  [you, can't, but, good, luck, trying]    1.000000   \n",
       "...                                                  ...         ...   \n",
       "11309  [when, object, belief, said, perfect, make, be...    0.930233   \n",
       "11310  [koff, you, mean, long, i, sleep, first, i, ki...    1.000000   \n",
       "11311  [mr, assertion, more, complex, later, paleonto...    0.971014   \n",
       "11312  [excess, stuff, deleted, i, know, similar, inc...    0.987805   \n",
       "11313  [the, system, refered, moral, system, you, sho...    0.950000   \n",
       "\n",
       "                                               text_nltk  rate_nltk  \\\n",
       "0      [my, votes, fwiw, team, mvp, pat, he, fans, go...   0.950207   \n",
       "1      [kovalev, talented, player, play, roger, needs...   0.938272   \n",
       "2      [new, jersey, pittsburgh, first, period, pitts...   0.867299   \n",
       "3      [first, all, penguins, will, win, cup, again, ...   0.981481   \n",
       "4                [you, ca, n't, but, good, luck, trying]   1.000000   \n",
       "...                                                  ...        ...   \n",
       "11309  [when, object, belief, said, perfect, make, be...   0.978102   \n",
       "11310  [koff, you, mean, long, i, sleep, first, i, ki...   1.000000   \n",
       "11311  [mr, connor, 's, assertion, more, complex, lat...   0.985915   \n",
       "11312  [excess, stuff, deleted, i, know, similar, inc...   0.987952   \n",
       "11313  [the, system, refered, moral, system, you, hav...   1.000000   \n",
       "\n",
       "                                              text_apacy  rate_spacy  \n",
       "0      [my, votes, fwiw, team, mvp, pat, he, fans, go...    0.950207  \n",
       "1      [kovalev, talented, player, play, roger, needs...    0.938272  \n",
       "2      [new, jersey, pittsburgh, first, period, pitts...    0.867299  \n",
       "3      [first, all, penguins, will, win, cup, again, ...    0.981481  \n",
       "4                [you, ca, n't, but, good, luck, trying]    1.000000  \n",
       "...                                                  ...         ...  \n",
       "11309  [when, object, belief, said, perfect, make, be...    0.978102  \n",
       "11310  [koff, you, mean, long, i, sleep, first, i, ki...    1.000000  \n",
       "11311  [mr, connor, 's, assertion, more, complex, lat...    0.985915  \n",
       "11312  [excess, stuff, deleted, i, know, similar, inc...    0.987952  \n",
       "11313  [the, system, refered, moral, system, you, sho...    0.950000  \n",
       "\n",
       "[11314 rows x 10 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df = pd.DataFrame(columns=['id','category','text','original_tokenize_num','preprocess_text','preprocss_tokenize_num'])\n",
    "train_df = pd.DataFrame(columns=['id','category','text','original_tokenize_num',\n",
    "                                 'text_keras','rate_keras','text_nltk','rate_nltk','text_apacy','rate_spacy'])\n",
    "train_dir = './data/20news-bydate-v3/20news-bydate-train/'\n",
    "cat_dir = next(os.walk(train_dir))[1]\n",
    "count = 0\n",
    "for cat in tqdm(cat_dir):\n",
    "    in_dir = train_dir + cat + '/'\n",
    "    news_list = next(os.walk(in_dir))[2]\n",
    "    news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "    news_list = list(filter(lambda f: f.endswith(\".txt\"), news_list))\n",
    "    for news_path in news_list:\n",
    "        id_num = news_path.split('/')[-1].split('.')[0]\n",
    "        with open(news_path,'r',encoding='latin1') as f:\n",
    "            news_text = f.read()\n",
    "            news_text = re.sub('\\n',' ',news_text)\n",
    "            news_text_tok_ori = T.text_to_word_sequence(news_text,filters='',lower=False)\n",
    "            news_text_ori = \" \".join(news_text_tok_ori)\n",
    "            news_text_tok_num_ori = len(news_text_tok_ori)\n",
    "            news_text_tok_keras,rate_keras = text_preprcess(news_text,tokenizers='keras')\n",
    "            news_text_tok_nltk,rate_nltk = text_preprcess(news_text,tokenizers='nltk')\n",
    "            news_text_tok_spacy,rate_spacy = text_preprcess(news_text,tokenizers='spacy')\n",
    "        train_df.loc[count] = [id_num,cat,news_text_ori,news_text_tok_num_ori,\n",
    "                               news_text_tok_keras,rate_keras,news_text_tok_nltk,rate_nltk,news_text_tok_spacy,rate_spacy]\n",
    "        count+=1\n",
    "#         break\n",
    "    \n",
    "# train_df.loc[0] = [0,0,0]\n",
    "print(train_df.rate_keras.mean() , train_df.rate_nltk.mean(),train_df.rate_spacy.mean()  )\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* keras tokenizer: 95.95% tokens in glove\n",
    "* nltk tokenizer: 95.96% tokens in glove\n",
    "* spacy tokenizer: 96.05% tokens in glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 42.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>original_tokenize_num</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>preprocss_tokenize_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19305</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>My votes (FWIW): Team MVP: Pat Verbeek. He fan...</td>\n",
       "      <td>453</td>\n",
       "      <td>my votes fwiw team mvp pat he fans goal mouth ...</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21002</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>I have posted disp135.zip to alt.binaries.pict...</td>\n",
       "      <td>1447</td>\n",
       "      <td>i posted you distribute program freely noncomm...</td>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>21586</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>Why don't you call the City and ask? Oak Park ...</td>\n",
       "      <td>327</td>\n",
       "      <td>why city ask oak park illegal handgun ban well...</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>23146</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>-&gt; &gt;Now let me get this straight. After a nice...</td>\n",
       "      <td>301</td>\n",
       "      <td>now let straight after nice long rant how peop...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>23229</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>NEWS YOU MAY HAVE MISSED, APR 19, 1993 Not bec...</td>\n",
       "      <td>615</td>\n",
       "      <td>news you may have missed apr not busy us media...</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>19586</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>Is this the one that had the {wrench|pliers} f...</td>\n",
       "      <td>12</td>\n",
       "      <td>is inside recovery</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>22675</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>Hmm, followup on my own posting... Well, who c...</td>\n",
       "      <td>973</td>\n",
       "      <td>hmm followup posting well cares first let try ...</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>27525</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>: I'm looking to buy a 17\" monitor soon, and i...</td>\n",
       "      <td>111</td>\n",
       "      <td>i 'm looking buy monitor soon i ca n't decide ...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>26073</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>Methinks you recall wrong. Mitchell hit close ...</td>\n",
       "      <td>84</td>\n",
       "      <td>methinks recall wrong mitchell hit close atlan...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>28486</td>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>Fortunately, wire-wrapping is a better wiring ...</td>\n",
       "      <td>125</td>\n",
       "      <td>fortunately wirewrapping better wiring techniq...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>25314</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>Forsale: SONY MHC-3600 HI-FI Bookshelf stereo ...</td>\n",
       "      <td>120</td>\n",
       "      <td>forsale sony mhc hifi bookshelf stereo months ...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>17681</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>Paraphrase of initial post: \\tCan I fight a sp...</td>\n",
       "      <td>49</td>\n",
       "      <td>paraphrase initial post can i fight speeding t...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>25854</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>Please, please don't make Barney to a modern m...</td>\n",
       "      <td>33</td>\n",
       "      <td>please make barney modern mythical figure i de...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>23931</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>A relative of mine has recently been diagnosed...</td>\n",
       "      <td>43</td>\n",
       "      <td>a relative recently diagnosed stage papillary ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>26597</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>18684</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>I apologize if this post isn't entirely approp...</td>\n",
       "      <td>52</td>\n",
       "      <td>i apologize post entirely appropriate newsgrou...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>20307</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>I have heard of two packages for the PC that s...</td>\n",
       "      <td>52</td>\n",
       "      <td>i heard packages pc support xwin the linux fre...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>24375</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>What exactly does the windows bitmap format lo...</td>\n",
       "      <td>43</td>\n",
       "      <td>what exactly windows bitmap format look like i...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>28065</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>I'm wondering if anybody else out there is a c...</td>\n",
       "      <td>166</td>\n",
       "      <td>i 'm wondering anybody clutchless shifter i 'v...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>20556</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Just what do gay people do that straight peopl...</td>\n",
       "      <td>128</td>\n",
       "      <td>just gay people straight people do n't absolut...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                  category  \\\n",
       "0   19305          rec.sport.hockey   \n",
       "1   21002             comp.graphics   \n",
       "2   21586        talk.politics.guns   \n",
       "3   23146        talk.politics.misc   \n",
       "4   23229     talk.politics.mideast   \n",
       "5   19586                 sci.space   \n",
       "6   22675                 sci.crypt   \n",
       "7   27525  comp.sys.ibm.pc.hardware   \n",
       "8   26073        rec.sport.baseball   \n",
       "9   28486           sci.electronics   \n",
       "10  25314              misc.forsale   \n",
       "11  17681           rec.motorcycles   \n",
       "12  25854        talk.religion.misc   \n",
       "13  23931                   sci.med   \n",
       "14  26597     comp.sys.mac.hardware   \n",
       "15  18684    soc.religion.christian   \n",
       "16  20307            comp.windows.x   \n",
       "17  24375   comp.os.ms-windows.misc   \n",
       "18  28065                 rec.autos   \n",
       "19  20556               alt.atheism   \n",
       "\n",
       "                                                 text original_tokenize_num  \\\n",
       "0   My votes (FWIW): Team MVP: Pat Verbeek. He fan...                   453   \n",
       "1   I have posted disp135.zip to alt.binaries.pict...                  1447   \n",
       "2   Why don't you call the City and ask? Oak Park ...                   327   \n",
       "3   -> >Now let me get this straight. After a nice...                   301   \n",
       "4   NEWS YOU MAY HAVE MISSED, APR 19, 1993 Not bec...                   615   \n",
       "5   Is this the one that had the {wrench|pliers} f...                    12   \n",
       "6   Hmm, followup on my own posting... Well, who c...                   973   \n",
       "7   : I'm looking to buy a 17\" monitor soon, and i...                   111   \n",
       "8   Methinks you recall wrong. Mitchell hit close ...                    84   \n",
       "9   Fortunately, wire-wrapping is a better wiring ...                   125   \n",
       "10  Forsale: SONY MHC-3600 HI-FI Bookshelf stereo ...                   120   \n",
       "11  Paraphrase of initial post: \\tCan I fight a sp...                    49   \n",
       "12  Please, please don't make Barney to a modern m...                    33   \n",
       "13  A relative of mine has recently been diagnosed...                    43   \n",
       "14                                                                        0   \n",
       "15  I apologize if this post isn't entirely approp...                    52   \n",
       "16  I have heard of two packages for the PC that s...                    52   \n",
       "17  What exactly does the windows bitmap format lo...                    43   \n",
       "18  I'm wondering if anybody else out there is a c...                   166   \n",
       "19  Just what do gay people do that straight peopl...                   128   \n",
       "\n",
       "                                      preprocess_text preprocss_tokenize_num  \n",
       "0   my votes fwiw team mvp pat he fans goal mouth ...                    229  \n",
       "1   i posted you distribute program freely noncomm...                    912  \n",
       "2   why city ask oak park illegal handgun ban well...                    180  \n",
       "3   now let straight after nice long rant how peop...                    150  \n",
       "4   news you may have missed apr not busy us media...                    368  \n",
       "5                                  is inside recovery                      3  \n",
       "6   hmm followup posting well cares first let try ...                    481  \n",
       "7   i 'm looking buy monitor soon i ca n't decide ...                     56  \n",
       "8   methinks recall wrong mitchell hit close atlan...                     39  \n",
       "9   fortunately wirewrapping better wiring techniq...                     71  \n",
       "10  forsale sony mhc hifi bookshelf stereo months ...                     80  \n",
       "11  paraphrase initial post can i fight speeding t...                     35  \n",
       "12  please make barney modern mythical figure i de...                     17  \n",
       "13  a relative recently diagnosed stage papillary ...                     23  \n",
       "14                                                                         0  \n",
       "15  i apologize post entirely appropriate newsgrou...                     26  \n",
       "16  i heard packages pc support xwin the linux fre...                     23  \n",
       "17  what exactly windows bitmap format look like i...                     24  \n",
       "18  i 'm wondering anybody clutchless shifter i 'v...                     96  \n",
       "19  just gay people straight people do n't absolut...                     76  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(columns=['id','category','text','original_tokenize_num','preprocess_text','preprocss_tokenize_num'])\n",
    "train_dir = './data/20news-bydate-v3/20news-bydate-train/'\n",
    "cat_dir = next(os.walk(train_dir))[1]\n",
    "count = 0\n",
    "for cat in tqdm(cat_dir):\n",
    "    in_dir = train_dir + cat + '/'\n",
    "    news_list = next(os.walk(in_dir))[2]\n",
    "    news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "    news_list = list(filter(lambda f: f.endswith(\".txt\"), news_list))\n",
    "    for news_path in news_list:\n",
    "        id_num = news_path.split('/')[-1].split('.')[0]\n",
    "        with open(news_path,'r',encoding='latin1') as f:\n",
    "            news_text = f.read()\n",
    "            news_text = re.sub('\\n',' ',news_text)\n",
    "            news_text_tok_ori = T.text_to_word_sequence(news_text,filters='',lower=False)\n",
    "            news_text_ori = \" \".join(news_text_tok_ori)\n",
    "            news_text_tok_num_ori = len(news_text_tok_ori)\n",
    "#             news_text_tok_pre,rate_keras = text_preprcess(news_text,tokenizers='keras')\n",
    "#             news_text_tok_pre,rate_nltk = text_preprcess(news_text,tokenizers='nltk')\n",
    "            news_text_tok_pre,rate_spacy = text_preprcess(news_text,tokenizers='spacy')\n",
    "            news_text_pre = \" \".join(news_text_tok_pre)\n",
    "        train_df.loc[count] = [id_num,cat,news_text_ori,news_text_tok_num_ori,news_text_pre,len(news_text_tok_pre)]\n",
    "        count+=1\n",
    "        break\n",
    "    \n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have posted disp135.zip to alt.binaries.pictures.utilities ****** You may distribute this program freely for non-commercial use if no fee is gained. ****** There is no warranty. The author is not responsible for any damage caused by this program. Important changes since version 1,30: Fix bugs in file management system (file displaying). Improve file management system (more user-friendly). Fix bug in XPM version 3 reading. Fix bugs in TARGA reading/writng. Fix bug in GEM/IMG reading. Add support for PCX and GEM/IMG writing. Auto-skip macbinary header. (1) Introduction: This program can let you READ, WRITE and DISPLAY images with different formats. It also let you do some special effects(ROTATION, DITHERING ....) on image. Its main purpose is to let you convert image among different formts. Include simple file management system. Support \\'slide show\\'. There is NO LIMIT on image size. Currently this program supports 8, 15, 16, 24 bits display. If you want to use HiColor or TrueColor, you must have VESA driver. If you want to modify video driver, please read section (8). (2) Hardware Requirement: PC 386 or better. MSDOS 3,3 or higher. min amount of ram is 4M bytes(Maybe less memory will also work). (I recommend min 8M bytes for better performance). Hard disk for swapping(virtual memory). The following description is borrowed from DJGPP. Supported Wares: * Up to 128M of extended memory (expanded under VCPI) * Up to 128M of disk space used for swapping * SuperVGA 256-color mode up to 1024x768 * 80387 * XMS & VDISK memory allocation strategies * VCPI programs, such as QEMM, DESQview, and 386MAX Unsupported: * DPMI * Microsoft Windows Features: 80387 emulator, 32-bit unix-ish environment, flat memory model, SVGA graphics. (3) Installation: Video drivers, emu387 and go32.exe are borrowed from DJGPP. (If you use Western Digital VGA chips, read readme.wd) (This GO32.EXE is a modified version for vesa and is COMPLETELY compatible with original version) + *** But some people report that this go32.exe is not compatible with + other DJGPP programs in their system. If you encounter this problem, + DON\\'T put go32.exe within search path. *** Please read runme.bat for how to run this program. If you choose xxxxx.grn as video driver, add \\'nc 256\\' to environment GO32. For example, go32=driver x:/xxxxx/xxxxx.grn nc 256 If you don\\'t have 80x87, add \\'emu x:/xxxxx/emu387\\' to environment GO32. For example, go32=driver x:/xxxxx/xxxxx.grd emu x:/xxxxx/emu387 **** Notes: 1. I only test tr8900.grn, et4000.grn and vesa.grn. Other drivers are not tested. 2. I have modified et4000.grn to support 8, 15, 16, 24 bits display. You don\\'t need to use vesa driver. If et4000.grn doesn\\'t work, please try vesa.grn. 3. For those who want to use HiColor or TrueColor display, please use vesa.grn(except et4000 users). You can find vesa BIOS driver from : wuarchive.wustl.edu: /mirrors/msdos/graphics godzilla.cgl.rmit.oz.au: /kjb/MGL (4) Command Line Switch: + Usage : display [-d|--display initial_display_type] + [-s|--sort sort_method] + [-h|-?] Display type: 8(SVGA,default), 15, 16(HiColor), 24(TrueColor) + Sort method: \\'name\\', \\'ext\\' (5) Function Key: F2 : Change disk drive + CTRL-A -- CTRL-Z : change disk drive. F3 : Change filename mask (See match.doc) F4 : Change parameters F5 : Some effects on picture, eg. flip, rotate .... F7 : Make Directory t : Tag file + : Tag group files (See match.doc) T : Tag all files u : Untag file - : Untag group files (See match.doc) U : Untag all files Ins : Change display type (8,15,16,24) in \\'read\\' & \\'screen\\' menu. F6,m,M : Move file(s) F8,d,D : Delete file(s) r,R : Rename file c,C : Copy File(s) z,Z : Display first 10 bytes in Ascii, Hex and Dec modes. + f,F : Display disk free space. Page Up/Down : Move one page TAB : Change processing target. Arrow keys, Home, End, Page Up, Page Down: Scroll image. Home: Left Most. End: Right Most. Page Up: Top Most. Page Down: Bottom Most. in \\'screen\\' & \\'effect\\' menu : Left,Right arrow: Change display type(8, 15, 16, 24 bits) s,S : Slide Show. ESCAPE to terminate. ALT-X : Quit program without prompting. + ALT-A : Reread directory. Escape : Abort function and return. (6) Support Format: Read: GIF(.gif), Japan MAG(.mag), Japan PIC(.pic), Sun Raster(.ras), Jpeg(.jpg), XBM(.xbm), Utah RLE(.rle), PBM(.pbm), PGM(.pgm), PPM(.ppm), PM(.pm), PCX(.pcx), Japan MKI(.mki), Tiff(.tif), Targa(.tga), XPM(.xpm), Mac Paint(.mac), GEM/IMG(.img), IFF/ILBM(.lbm), Window BMP(.bmp), QRT ray tracing(.qrt), Mac PICT(.pct), VIS(.vis), PDS(.pds), VIKING(.vik), VICAR(.vic), FITS(.fit), Usenix FACE(.fac). the extensions in () are standard extensions. Write: GIF, Sun Raster, Jpeg, XBM, PBM, PGM, PPM, PM, Tiff, Targa, XPM, Mac Paint, Ascii, Laser Jet, IFF/ILBM, Window BMP, + Mac PICT, VIS, FITS, FACE, PCX, GEM/IMG. All Read/Write support full color(8 bits), grey scale, b/w dither, and 24 bits image, if allowed for that format. (7) Detail: Initialization: Set default display type to highest display type. Find allowable screen resolution(for .grn video driver only). 1. When you run this program, you will enter \\'read\\' menu. Whthin this menu you can press any function key except F5. If you move or copy files, you will enter \\'write\\' menu. the \\'write\\' menu is much like \\'read\\' menu, but only allow you to change directory. + The header line in \\'read\\' menu includes \"(d:xx,f:xx,t:xx)\". + d : display type. f: number of files. t: number of tagged files. pressing SPACE in \\'read\\' menu will let you select which format to use for reading current file. pressing RETURN in \\'read\\' menu will let you reading current file. This program will automatically determine which format this file is. The procedure is: First, check magic number. If fail, check standard extension. Still fail, report error. pressing s or S in \\'read\\' menu will do \\'Slide Show\\'. If delay time is 0, program will wait until you hit a key (except ESCAPE). If any error occurs, program will make a beep. ESCAPE to terminate. pressing Ins in \\'read\\' menu will change display type. pressing ALT-X in \\'read\\' menu will quit program without prompting. 2. Once image file is successfully read, you will enter \\'screen\\' menu. Within this menu F5 is turn on. You can do special effect on image. pressing RETURN: show image. in graphic mode, press RETURN, SPACE or ESCAPE to return to text mode. pressing TAB: change processing target. This program allows you to do special effects on 8-bit or 24-bit image. pressing Left,Right arrow: change display type. 8, 15, 16, 24 bits. pressing SPACE: save current image to file. B/W Dither: save as black/white image(1 bit). Grey Scale: save as grey image(8 bits). Full Color: save as color image(8 bits). True Color: save as 24-bit image. This program will ask you some questions if you want to write image to file. Some questions are format-dependent. Finally This program will prompt you a filename. If you want to save file under another directory other than current directory, please press SPACE. after pressing SPACE, you will enter \\'write2\\' menu. You can change directory to what you want. Then, pressing SPACE: this program will prompt you \\'original\\' filename. pressing RETURN: this program will prompt you \\'selected\\' filename (filename under bar). 3. This program supports 8, 15, 16, 24 bits display. 4. This Program is MEMORY GREEDY. If you don\\'t have enough memory, the performance is poor. 5. If you want to save 8 bits image : try GIF then TIFF(LZW) then TARGA then Sun Raster then BMP then ... If you want to save 24 bits image (lossless): try TIFF(LZW) or TARGA or ILBM or Sun Raster (No one is better for true 24bits image) 6. I recommend Jpeg for storing 24 bits images, even 8 bits images. 7. Not all subroutines are fully tested 8. This document is not well written. If you have any PROBLEM, SUGGESTION, COMMENT about this program, Please send to u7711501@bicmos.ee.nctu.edu.tw (140,113,11,13). I need your suggestion to improve this program. (There is NO anonymous ftp on this site) (8) Tech. information: Program (user interface and some subroutines) written by Jih-Shin Ho. Some subroutines are borrowed from XV(2,21) and PBMPLUS(dec 91). Tiff(V3,2) and Jpeg(V4) reading/writing are through public domain libraries. Compiled with DJGPP. You can get whole DJGPP package from SIMTEL20 or mirror sites. For example, wuarchive.wustl.edu: /mirrors/msdos/djgpp (9) For Thoese who want to modify video driver: 1. get GRX source code from SIMTEL20 or mirror sites. 2. For HiColor and TrueColor: 15 bits : # of colors is set to 32768. 16 bits : # of colors is set to 0xc010. 24 bits : # of colors is set to 0xc018. Acknowledgment: I would like to thank the authors of XV and PBMPLUS for their permission to let me use their subroutines. Also I will thank the authors who write Tiff and Jpeg libraries. Thank DJ. Without DJGPP I can\\'t do any thing on PC.'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[1,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(all_length):\n",
    "    '''\n",
    "    input: length list of elements e.g.[1,1,1,3,5,9,4,2,1,3,54,78,5...]\n",
    "    output1: mean、std、mode、min、q1、median(q2)、q3、max、iqr、outlier、far out\n",
    "    output2: statistics graph、10%~90% form\n",
    "    '''\n",
    "    stat_dict = {}\n",
    "    stat_dict['mean'] = np.mean(all_length)\n",
    "    stat_dict['std'] = np.std(all_length)\n",
    "    stat_dict['mode'] = np.argmax(np.bincount(all_length))\n",
    "    stat_dict['min'] = np.min(all_length)\n",
    "    stat_dict['q1'] = np.quantile(all_length,0.25)\n",
    "    stat_dict['median'] = np.quantile(all_length,0.5)\n",
    "    stat_dict['q3'] = np.quantile(all_length,0.75)\n",
    "    stat_dict['max'] = np.max(all_length)\n",
    "    stat_dict['iqr'] = stat_dict['q3'] - stat_dict['q1']\n",
    "    stat_dict['outlier'] = stat_dict['q3'] + 1.5*stat_dict['iqr']\n",
    "    stat_dict['far_out'] = stat_dict['q3'] + 3*stat_dict['iqr']\n",
    "    for i in [10,20,30,40,50,60,70,80,90,100]:\n",
    "        stat_dict[str(i)+'%'] = np.percentile(all_length,i)\n",
    "    return pd.DataFrame.from_dict(stat_dict,orient='index',columns=['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>144.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>214.881008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mode</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q1</td>\n",
       "      <td>23.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>median</td>\n",
       "      <td>63.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q3</td>\n",
       "      <td>157.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>912.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iqr</td>\n",
       "      <td>133.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>outlier</td>\n",
       "      <td>358.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>far_out</td>\n",
       "      <td>558.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10%</td>\n",
       "      <td>15.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20%</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30%</td>\n",
       "      <td>25.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40%</td>\n",
       "      <td>37.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>63.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60%</td>\n",
       "      <td>77.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70%</td>\n",
       "      <td>112.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80%</td>\n",
       "      <td>189.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90%</td>\n",
       "      <td>379.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100%</td>\n",
       "      <td>912.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "mean     144.450000\n",
       "std      214.881008\n",
       "mode      23.000000\n",
       "min        0.000000\n",
       "q1        23.750000\n",
       "median    63.500000\n",
       "q3       157.500000\n",
       "max      912.000000\n",
       "iqr      133.750000\n",
       "outlier  358.125000\n",
       "far_out  558.750000\n",
       "10%       15.600000\n",
       "20%       23.000000\n",
       "30%       25.400000\n",
       "40%       37.400000\n",
       "50%       63.500000\n",
       "60%       77.600000\n",
       "70%      112.200000\n",
       "80%      189.800000\n",
       "90%      379.300000\n",
       "100%     912.000000"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_num_li = train_df.preprocss_tokenize_num.tolist()\n",
    "basic_statistics(tokens_num_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just',\n",
       " 'what',\n",
       " 'do',\n",
       " 'gay',\n",
       " 'people',\n",
       " 'do',\n",
       " 'that',\n",
       " 'straight',\n",
       " 'people',\n",
       " \"don't?\",\n",
       " 'absolutely',\n",
       " 'nothing.',\n",
       " \"i'm\",\n",
       " 'a',\n",
       " 'very',\n",
       " 'straight(as',\n",
       " 'an',\n",
       " 'arrow),',\n",
       " '17-year',\n",
       " 'old',\n",
       " 'male',\n",
       " 'that',\n",
       " 'is',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bsa.',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'care',\n",
       " 'what',\n",
       " 'gay',\n",
       " 'people',\n",
       " 'do',\n",
       " 'among',\n",
       " 'each',\n",
       " 'other,',\n",
       " 'as',\n",
       " 'long',\n",
       " 'as',\n",
       " 'they',\n",
       " \"don't\",\n",
       " 'make',\n",
       " 'passes',\n",
       " 'at',\n",
       " 'me',\n",
       " 'or',\n",
       " 'anything.',\n",
       " 'at',\n",
       " 'my',\n",
       " 'summer',\n",
       " 'camp',\n",
       " 'where',\n",
       " 'i',\n",
       " 'work,',\n",
       " 'my',\n",
       " 'boss',\n",
       " 'is',\n",
       " 'gay.',\n",
       " 'not',\n",
       " 'in',\n",
       " 'a',\n",
       " \"'pansy'\",\n",
       " 'way',\n",
       " 'of',\n",
       " 'gay',\n",
       " '(i',\n",
       " 'know',\n",
       " 'a',\n",
       " 'few),',\n",
       " 'but',\n",
       " 'just',\n",
       " \"'one\",\n",
       " 'of',\n",
       " 'the',\n",
       " \"guys'.\",\n",
       " 'he',\n",
       " \"doesn't\",\n",
       " 'push',\n",
       " 'anything',\n",
       " 'on',\n",
       " 'me,',\n",
       " 'and',\n",
       " 'we',\n",
       " 'give',\n",
       " 'him',\n",
       " 'the',\n",
       " 'same',\n",
       " 'respect',\n",
       " 'back,',\n",
       " 'due',\n",
       " 'to',\n",
       " 'his',\n",
       " 'position.',\n",
       " 'if',\n",
       " 'anything,',\n",
       " 'the',\n",
       " 'bsa',\n",
       " 'has',\n",
       " 'taught',\n",
       " 'me,',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know,',\n",
       " 'tolerance',\n",
       " 'or',\n",
       " 'something.',\n",
       " 'before',\n",
       " 'i',\n",
       " 'met',\n",
       " 'this',\n",
       " 'guy,',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'all',\n",
       " 'gays',\n",
       " 'were',\n",
       " \"'faries'.\",\n",
       " 'so,',\n",
       " 'the',\n",
       " 'bsa',\n",
       " 'has',\n",
       " 'taught',\n",
       " 'me',\n",
       " 'to',\n",
       " 'be',\n",
       " 'an',\n",
       " 'antibigot.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/20news-bydate-v3/20news-bydate-train/alt.atheism/20556.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20748.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20810.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20574.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20692.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20705.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20519.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20565.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20835.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20649.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20684.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20872.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20739.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20547.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20578.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20704.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20932.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20528.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20769.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20869.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20549.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20776.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20936.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20980.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20780.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20515.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20575.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20766.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20750.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20562.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20700.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20594.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20972.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20801.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20910.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20756.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20883.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20539.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20687.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20852.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20945.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20734.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20865.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20694.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20897.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20833.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20966.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20808.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20816.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20793.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20893.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20926.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20975.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20914.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20952.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20913.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20879.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20925.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20666.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20958.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20760.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20881.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20640.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20834.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20600.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20727.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20842.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20768.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20829.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20871.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20741.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20886.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20927.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20652.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20679.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20586.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20527.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20976.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20613.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20794.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20559.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20595.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20959.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20744.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20541.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20717.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20589.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20725.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20526.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20598.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20802.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20674.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20901.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20667.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20538.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20577.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20693.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20853.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20938.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20545.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20823.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20859.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20803.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20921.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20861.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20832.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20622.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20680.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20696.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20988.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20747.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20805.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20627.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20858.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20977.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20686.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20800.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20664.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20792.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20698.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20795.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20770.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20688.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20683.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20807.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20531.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20764.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20553.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20954.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20827.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20767.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20916.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20896.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20782.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20970.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20939.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20955.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20894.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20646.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20951.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20819.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20946.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20979.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20905.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20847.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20864.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20570.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20611.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20870.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20779.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20635.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20900.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20665.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20580.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20670.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20840.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20787.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20962.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20701.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20542.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20956.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20737.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20612.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20743.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20522.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20918.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20618.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20814.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20763.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20851.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20728.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20662.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20745.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20517.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20609.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20676.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20839.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20848.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20891.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20825.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20950.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20758.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20911.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20751.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20902.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20775.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20548.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20535.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20623.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20604.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20762.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20566.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20690.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20711.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20552.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20655.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20581.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20984.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20681.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20607.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20788.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20532.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20774.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20544.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20678.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20812.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20981.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20944.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20536.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20620.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20647.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20785.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20804.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20878.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20843.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20659.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20935.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20672.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20551.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20629.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20703.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20986.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20720.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20621.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20749.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20917.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20746.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20930.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20707.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20880.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20657.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20540.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20671.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20656.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20636.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20815.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20772.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20862.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20963.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20850.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20663.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20571.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20799.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20860.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20929.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20601.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20875.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20854.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20765.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20989.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20637.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20985.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20948.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20730.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20715.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20919.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20863.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20983.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20721.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20754.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20633.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20709.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20759.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20806.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20550.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20777.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20937.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20895.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20661.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20658.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20568.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20898.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20723.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20753.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20731.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20824.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20738.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20923.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20564.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20884.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20726.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20903.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20610.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20855.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20874.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20922.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20521.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20625.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20596.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20836.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20887.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20933.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20818.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20890.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20991.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20628.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20543.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20994.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20591.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20642.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20826.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20588.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20654.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20987.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20846.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20668.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20561.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20587.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20602.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20899.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20811.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20648.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20634.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20892.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20572.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20554.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20724.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20638.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20789.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20632.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20906.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20590.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20877.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20742.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20710.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20631.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20817.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20888.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20830.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20771.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20733.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20518.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20904.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20828.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20982.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20974.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20555.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20920.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20856.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20644.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20606.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20608.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20845.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20524.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20821.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20791.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20953.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20689.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20708.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20626.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20643.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20691.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20786.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20943.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20530.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20809.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20957.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20908.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20978.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20928.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20716.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20961.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20605.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20857.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20889.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20660.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20712.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20567.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20624.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20844.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20702.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20778.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20569.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20560.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20582.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20729.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20682.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20735.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20614.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20573.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20558.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20973.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20713.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20740.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20675.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20736.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20523.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20617.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20706.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20576.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20525.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20968.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20866.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20603.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20534.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20516.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20924.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20971.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20761.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20599.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20650.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20820.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20563.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20990.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20597.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20585.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20755.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20868.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20992.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20949.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20876.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20993.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20615.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20630.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20645.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20533.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20867.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20685.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20639.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20907.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20882.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20813.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20695.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20641.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20838.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20722.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20699.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20967.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20520.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20781.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20885.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20837.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20960.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20796.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20841.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20969.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20546.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20557.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20942.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20784.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20912.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20790.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20940.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20529.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20909.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20941.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20583.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20752.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20965.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20593.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20915.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20616.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20592.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20653.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20697.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20714.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20579.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20831.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20783.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20773.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20873.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20651.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20931.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20947.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20718.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20798.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20669.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20619.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20732.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20584.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20673.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20964.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20822.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20934.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20849.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20797.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20537.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20757.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20677.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20719.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_list = next(os.walk(in_dir))[2]\n",
    "news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/leoqaz12/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/leoqaz12/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "| dictionary: 50264 types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (decoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerSentenceEncoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\n",
    "roberta.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"now let straight after nice long rant how people need personal responsibility economic social lives sudden 's radicals such me i guess responsible poor people 's lifestyles tell think poor people dumb think themselves there reasons disintegration family and support systems general nation 's poor somehow i think murphy janis the sane person 's list you want generation 's vaunted cultural revolution lasting change worse try socalled relevant values education hey like good idea time how know needed real education mean took granted the 's generation spoiled irresponsible the depression create mothers fathers determined kids want going overboard creating nation brats consider contrast famous events july apollo woodstock which group large numbers people feed reverted cultural level primitives defecation public etc and group assembled took care itself dispersed damage deaths large numbers drug problems was n't woodstock called biggest parking lot history they rejected society went nature parent 's cars\""
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 tensor([    0,  8310,   905,  1359,    71,  2579,   251, 25693,   141,    82,\n",
      "          240,  1081,  2640,   776,   592,  1074,  7207,   128,    29, 35842,\n",
      "          215,   162,   939,  4443,  2149,  2129,    82,   128,    29, 28182,\n",
      "         1137,   206,  2129,    82, 16881,   206,  1235,    89,  2188, 32654,\n",
      "         8475,   284,     8,   323,  1743,   937,  1226,   128,    29,  2129,\n",
      "         7421,   939,   206, 22802, 16628, 10408,   354,     5, 37091,   621,\n",
      "          128,    29,   889,    47,   236,  2706,   128,    29,   748, 19264,\n",
      "         4106,  7977,  9735,   464,  3007,   860, 17380,  9315,  4249,  3266,\n",
      "         1265, 17232,   101,   205,  1114,    86,   141,   216,   956,   588,\n",
      "         1265,  1266,   362,  4159,     5,   128,    29,  2706, 29136, 21573,\n",
      "            5,  6943,  1045,  8826, 17850,  3030,  1159,   236,   164, 35912,\n",
      "         2351,  1226,  5378,  2923,  1701,  5709,  3395,  1061,  1236, 21347,\n",
      "         6256, 38520,  5627,  9607,    61,   333,   739,  1530,    82,  3993,\n",
      "        41411,  4106,   672,  9156, 35143,  3816,  3204,  1258,   285,  4753,\n",
      "            8,   333, 14525,   362,   575,  1495, 32807,  1880,  3257,   739,\n",
      "         1530,  1262,  1272,    21,   295,    75,  5627,  9607,   373,   934,\n",
      "         2932,   319,   750,    51,  3946,  2313,   439,  2574,  4095,   128,\n",
      "           29,  1677,     2])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-f46ab41149aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features_aligned_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_ori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/roberta/hub_interface.py\u001b[0m in \u001b[0;36mextract_features_aligned_to_words\u001b[0;34m(self, sentence, return_all_hiddens)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_toks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0maligned_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignment_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_features_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malignment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# wrap in spaCy Doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/roberta/alignment_utils.py\u001b[0m in \u001b[0;36malign_features_to_words\u001b[0;34m(roberta, features, alignment)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;31m#assert torch.all(torch.abs(output.sum(dim=0) - features.sum(dim=0)) < 1e-4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokens_ori = train_df.loc[3,'preprocess_text']#'how\\'s are you'#\n",
    "tokens = roberta.encode(tokens_ori)\n",
    "# assert tokens.tolist() == [0, 31414, 232, 328, 2]\n",
    "# assert roberta.decode(tokens) == 'Hello world!'\n",
    "print(len(tokens) , tokens)\n",
    "roberta.decode(tokens)\n",
    "doc = roberta.extract_features_aligned_to_words(tokens_ori)\n",
    "print(len(doc))\n",
    "for tok in doc:\n",
    "    print('{:10}{} (...)'.format(str(tok), tok.vector[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, ['I', 'said', ',', '\"', 'hello', 'RoBERTa', '.', '\"'])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = 'I said, \"hello RoBERTa.\"'\n",
    "tokens = sp_tokenizer(tokens)\n",
    "token_li = []\n",
    "for token in tokens:\n",
    "    token_li.append(token.text)\n",
    "len(token_li) , token_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tokens exceeds maximum length: 592 > 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-7a2d631178ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast_layer_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# == torch.Size([1, 5, 1024])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/roberta/hub_interface.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, tokens, return_all_hiddens)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             raise ValueError('tokens exceeds maximum length: {} > {}'.format(\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             ))\n\u001b[1;32m     84\u001b[0m         features, extra = self.model(\n",
      "\u001b[0;31mValueError\u001b[0m: tokens exceeds maximum length: 592 > 512"
     ]
    }
   ],
   "source": [
    "last_layer_features = roberta.extract_features(tokens)\n",
    "print(last_layer_features.size())# == torch.Size([1, 5, 1024])\n",
    "\n",
    "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)\n",
    "assert len(all_layers) == 25\n",
    "assert torch.all(all_layers[-1] == last_layer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 19, 1024)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer_features.detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 898823/898823 [00:06<00:00, 134563.64B/s]\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 545019.70B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0925 15:40:29.729804 139833327294272 tokenization_utils.py:665] Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "590"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(train_df.loc[0,'text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpjht4irru\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpjht4irru', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From <ipython-input-2-960d39c65024>:12: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:112: BaseResourceVariable.count_up_to (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpjht4irru/model.ckpt.\n",
      "WARNING:tensorflow:Training with estimator made no steps. Perhaps input is empty or misspecified.\n",
      "INFO:tensorflow:Loss for final step: None.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-25T03:59:44Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-1\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Inference Time : 0.10528s\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-25-03:59:44\n",
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 6611816.0, score = 6611816.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1: /tmp/tmpjht4irru/model.ckpt-1\n",
      "score: 6611816.0\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-1\n",
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:loss = 6611816.0, step = 2\n",
      "INFO:tensorflow:Saving checkpoints for 3 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6611816.0.\n",
      "delta: [[ 6.0338150e+01 -3.3985901e+01]\n",
      " [-3.8263580e+01 -3.6269409e+01]\n",
      " [-1.8039972e+02  1.7930125e+02]\n",
      " [-9.9875616e+02 -6.9757360e-01]\n",
      " [ 9.8684814e+01 -4.1847046e+01]]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-25T03:59:44Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-3\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Inference Time : 0.10220s\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-25-03:59:44\n",
      "INFO:tensorflow:Saving dict for global step 3: global_step = 3, loss = 4676776.5, score = 4676776.5\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3: /tmp/tmpjht4irru/model.ckpt-3\n",
      "score: 4676776.5\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-3\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:loss = 4676776.5, step = 3\n",
      "INFO:tensorflow:Saving checkpoints for 5 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4676776.5.\n",
      "delta: [[ 31.123703  19.39618 ]\n",
      " [-69.548004  61.5542  ]\n",
      " [-28.467346  45.750595]\n",
      " [ 97.54656   94.282936]\n",
      " [ 29.411499 -27.243713]]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-25T03:59:45Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-5\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Inference Time : 0.09759s\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-25-03:59:45\n",
      "INFO:tensorflow:Saving dict for global step 5: global_step = 5, loss = 4030337.5, score = 4030337.5\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5: /tmp/tmpjht4irru/model.ckpt-5\n",
      "score: 4030337.5\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-5\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:loss = 4030337.5, step = 5\n",
      "INFO:tensorflow:Saving checkpoints for 7 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4030337.5.\n",
      "delta: [[ 40.989746  42.059906]\n",
      " [-43.61667   21.547302]\n",
      " [  0.         0.      ]\n",
      " [ 66.66636   31.210045]\n",
      " [-11.713013 -17.82843 ]]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-25T03:59:45Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-7\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Inference Time : 0.10114s\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-25-03:59:45\n",
      "INFO:tensorflow:Saving dict for global step 7: global_step = 7, loss = 3737479.5, score = 3737479.5\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7: /tmp/tmpjht4irru/model.ckpt-7\n",
      "score: 3737479.5\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 7 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:loss = 3737479.5, step = 8\n",
      "INFO:tensorflow:Saving checkpoints for 9 into /tmp/tmpjht4irru/model.ckpt.\n",
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Loss for final step: 3737479.5.\n",
      "delta: [[ 46.576813   19.57956  ]\n",
      " [-38.874878   12.866638 ]\n",
      " [  0.          0.       ]\n",
      " [  0.0838623  33.73372  ]\n",
      " [ -7.6881714  -7.3582764]]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-25T03:59:46Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-9\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Inference Time : 0.09830s\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-25-03:59:46\n",
      "INFO:tensorflow:Saving dict for global step 9: global_step = 9, loss = 3560007.5, score = 3560007.5\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9: /tmp/tmpjht4irru/model.ckpt-9\n",
      "score: 3560007.5\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-9\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 9 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:loss = 3560007.5, step = 10\n",
      "INFO:tensorflow:Saving checkpoints for 11 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3560007.5.\n",
      "delta: [[ 23.455994   23.525055 ]\n",
      " [-22.899094   28.559814 ]\n",
      " [  0.          0.       ]\n",
      " [ -4.6200867  30.763382 ]\n",
      " [  0.          0.       ]]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-25T03:59:46Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-11\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Inference Time : 0.21466s\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-25-03:59:47\n",
      "INFO:tensorflow:Saving dict for global step 11: global_step = 11, loss = 3492203.5, score = 3492203.5\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 11: /tmp/tmpjht4irru/model.ckpt-11\n",
      "score: 3492203.5\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-11\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 11 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:loss = 3492203.5, step = 12\n",
      "INFO:tensorflow:Saving checkpoints for 13 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3492203.5.\n",
      "delta: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-25T03:59:47Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-13\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Inference Time : 0.10124s\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-25-03:59:47\n",
      "INFO:tensorflow:Saving dict for global step 13: global_step = 13, loss = 3492203.5, score = 3492203.5\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 13: /tmp/tmpjht4irru/model.ckpt-13\n",
      "score: 3492203.5\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-13\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 13 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:loss = 3492203.5, step = 13\n",
      "INFO:tensorflow:Saving checkpoints for 15 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3492203.5.\n",
      "delta: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-25T03:59:47Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-15\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Inference Time : 0.10034s\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-25-03:59:48\n",
      "INFO:tensorflow:Saving dict for global step 15: global_step = 15, loss = 3492203.5, score = 3492203.5\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 15: /tmp/tmpjht4irru/model.ckpt-15\n",
      "score: 3492203.5\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-15\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 15 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:loss = 3492203.5, step = 16\n",
      "INFO:tensorflow:Saving checkpoints for 17 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3492203.5.\n",
      "delta: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-25T03:59:48Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-17\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Inference Time : 0.10055s\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-25-03:59:48\n",
      "INFO:tensorflow:Saving dict for global step 17: global_step = 17, loss = 3492203.5, score = 3492203.5\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 17: /tmp/tmpjht4irru/model.ckpt-17\n",
      "score: 3492203.5\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-17\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 17 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:loss = 3492203.5, step = 17\n",
      "INFO:tensorflow:Saving checkpoints for 19 into /tmp/tmpjht4irru/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3492203.5.\n",
      "delta: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-25T03:59:49Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-19\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Inference Time : 0.09827s\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-25-03:59:49\n",
      "INFO:tensorflow:Saving dict for global step 19: global_step = 19, loss = 3492203.5, score = 3492203.5\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 19: /tmp/tmpjht4irru/model.ckpt-19\n",
      "score: 3492203.5\n",
      "cluster centers: [[396.27625 390.187  ]\n",
      " [222.7929  789.34845]\n",
      " [789.8891  225.74942]\n",
      " [159.6767  189.99008]\n",
      " [727.57385 797.9692 ]]\n",
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjht4irru/model.ckpt-19\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "point: [270.59833189 470.8875245 ] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [604.65509214 980.61246973] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [303.3302505  187.76119297] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [823.64433259 915.32152118] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [499.78209448 394.08140644] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [956.74897765 924.47877021] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [281.42019842 813.10646329] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [212.45600583 468.06540358] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [376.68626048 381.27034813] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [650.83228923 711.24874014] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [121.56735988 727.91169434] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [618.87875364 892.2466157 ] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [454.25404696 272.04698039] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [758.84945319 800.77138061] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [961.67941322 810.69990547] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [647.96954182 225.94370582] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [260.52158911 116.11636004] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [550.32341246 482.84577569] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [792.04474509 485.00935244] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [739.193435   855.35435772] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [102.77926169  84.86783565] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [582.41915816 890.15903065] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [441.70833924 181.34805333] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [329.28741426 477.11300549] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [914.05409174 228.18809276] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [993.35007032 649.38700595] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [125.79261136 188.49642553] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [888.86630362 152.88541806] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [455.21347907 424.70887616] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [895.05982687 416.32788503] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [579.04811642 996.58975991] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [742.89694775 632.50325784] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [409.77374347 143.04995505] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [962.19579734 546.89640355] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [309.15881237 881.8214313 ] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [191.51017093 193.93816164] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [718.22937912 451.2624622 ] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [815.91641809 963.32898388] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [  4.97734786 524.3604739 ] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [ 87.2654246  811.52013545] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [533.48483977 937.19349981] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [348.57125362 560.62188189] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [ 97.48452562 942.02768005] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [193.79184654 319.61221683] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [724.6357477  372.02921785] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [156.47870234  88.15824704] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [ 75.14288718 170.66587159] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [443.03799464 179.42632289] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [651.92329418 784.29041414] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [923.02844304 416.07803215] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [915.3130655  862.28454064] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [9.98756192e+02 6.97573619e-01] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [595.69068015 639.73401656] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [519.50080354 590.10131981] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [261.02513727 463.77522111] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [407.40674298 962.59321137] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [ 69.52017537 874.53625338] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [472.06157424 327.60435214] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [358.11167012 421.66553685] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [695.03467297 995.5620014 ] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [526.27069668 278.80060445] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [195.76818347 484.35450288] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [55.78548378 33.43988124] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [ 29.1935936  555.77591292] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [729.35827402 525.58652847] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [836.73697642  82.75103103] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [806.87297011 271.77193228] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [599.11162874  19.91379742] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [269.6734099  112.58330836] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [220.92174265 855.43177337] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [554.57182508 189.45441648] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [439.44961521 535.26738825] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [795.87468609 206.7127803 ] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [221.03432244 702.69035297] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [ 65.50069336 460.29190962] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [243.34803048 937.68512165] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [391.06734788 281.81690742] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [550.74633826 628.72959067] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [180.33168858 856.21939448] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [503.01073954  93.97778188] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [114.77845953 270.62426272] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [446.24584314 706.19722261] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [820.81684927 145.43259637] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [285.93893264 581.50994581] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [623.92022394  50.19484413] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [247.65206765 330.93956191] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [140.93452372 161.43724573] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [ 17.88768357 621.33607122] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [887.56715191 123.08710633] is in cluster 2 centered at [789.8891  225.74942]\n",
      "point: [326.9847042  560.44540029] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [774.42084176 650.67436284] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [ 75.62017603 356.63186776] is in cluster 3 centered at [159.6767  189.99008]\n",
      "point: [429.14729727 851.61610998] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [145.13824819 894.03122377] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [421.87540859 985.51096232] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [435.99512637 701.08988924] is in cluster 1 centered at [222.7929  789.34845]\n",
      "point: [478.03693602 560.27421065] is in cluster 0 centered at [396.27625 390.187  ]\n",
      "point: [596.63472498 961.08529551] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [628.86168839 596.52270215] is in cluster 4 centered at [727.57385 797.9692 ]\n",
      "point: [263.5107573 105.226543 ] is in cluster 3 centered at [159.6767  189.99008]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "num_points = 100\n",
    "dimensions = 2\n",
    "points = np.random.uniform(0, 1000, [num_points, dimensions])\n",
    "\n",
    "def input_fn():\n",
    "  return tf.compat.v1.train.limit_epochs(\n",
    "      tf.convert_to_tensor(points, dtype=tf.float32), num_epochs=1)\n",
    "\n",
    "num_clusters = 5\n",
    "kmeans = tf.compat.v1.estimator.experimental.KMeans(\n",
    "    num_clusters=num_clusters, use_mini_batch=False)\n",
    "\n",
    "# train\n",
    "num_iterations = 10\n",
    "previous_centers = None\n",
    "for _ in range(num_iterations):\n",
    "  kmeans.train(input_fn)\n",
    "  cluster_centers = kmeans.cluster_centers()\n",
    "  if previous_centers is not None:\n",
    "    print ('delta:', cluster_centers - previous_centers)\n",
    "  previous_centers = cluster_centers\n",
    "  print ('score:', kmeans.score(input_fn))\n",
    "print ('cluster centers:', cluster_centers)\n",
    "\n",
    "# map the input points to their clusters\n",
    "cluster_indices = list(kmeans.predict_cluster_index(input_fn))\n",
    "for i, point in enumerate(points):\n",
    "  cluster_index = cluster_indices[i]\n",
    "  center = cluster_centers[cluster_index]\n",
    "  print ('point:', point, 'is in cluster', cluster_index, 'centered at', center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Using cached tensorflow_datasets-2.0.0-py3-none-any.whl (3.1 MB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-2.4.1-py3-none-any.whl (475 kB)\n",
      "Requirement already up-to-date: tensorflow-gpu in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-datasets) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-datasets) (3.9.2)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 115 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from tensorflow-datasets) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: dill in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from tensorflow-datasets) (0.3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: wrapt in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-datasets) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=18.1.0 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from tensorflow-datasets) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-datasets) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-datasets) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-datasets) (1.12.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.21.1-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from transformers) (0.0.34)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/leoqaz12/.local/lib/python3.7/site-packages (from transformers) (2019.6.8)\n",
      "Collecting tokenizers==0.0.11\n",
      "  Downloading tokenizers-0.0.11-cp37-cp37m-manylinux1_x86_64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 242 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: sentencepiece in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from transformers) (0.1.83)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /home/leoqaz12/.local/lib/python3.7/site-packages (from transformers) (1.9.183)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<2.2.0,>=2.1.0 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-gpu) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.26.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-gpu) (0.1.7)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-gpu) (0.33.6)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-gpu) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-gpu) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/leoqaz12/.local/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow-datasets) (41.2.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /home/leoqaz12/.local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Collecting googleapis-common-protos\n",
      "  Downloading googleapis-common-protos-1.51.0.tar.gz (35 kB)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/leoqaz12/.local/lib/python3.7/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.183 in /home/leoqaz12/.local/lib/python3.7/site-packages (from boto3->transformers) (1.12.183)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /home/leoqaz12/.local/lib/python3.7/site-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /home/leoqaz12/.local/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /home/leoqaz12/.local/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.183->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.183->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /home/leoqaz12/anaconda3/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
      "Building wheels for collected packages: future, promise, googleapis-common-protos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491056 sha256=921d5ffcbbe68e8013ca1750153e69fbde20057e507a1c290c6d989cdd6c0b47\n",
      "  Stored in directory: /home/leoqaz12/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=4a93579570ecfee6e0fc131377a1d1acf1f685c3e5ab710d55a6ebb51daefa88\n",
      "  Stored in directory: /home/leoqaz12/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "  Building wheel for googleapis-common-protos (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for googleapis-common-protos: filename=googleapis_common_protos-1.51.0-py3-none-any.whl size=77593 sha256=cb2d90cf63fc4dc65b9119f3406489136c8e5fe8f80ba94599d79856b901150c\n",
      "  Stored in directory: /home/leoqaz12/.cache/pip/wheels/4c/a1/71/5e427276ceeff277fd76878d1b19fbf4587a2845015d86864b\n",
      "Successfully built future promise googleapis-common-protos\n",
      "Installing collected packages: future, promise, googleapis-common-protos, tensorflow-metadata, tensorflow-datasets, tokenizers, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 2.3.0\n",
      "    Uninstalling transformers-2.3.0:\n",
      "      Successfully uninstalled transformers-2.3.0\n",
      "Successfully installed future-0.18.2 googleapis-common-protos-1.51.0 promise-2.3 tensorflow-datasets-2.0.0 tensorflow-metadata-0.21.1 tokenizers-0.0.11 transformers-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow-datasets transformers tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig, glue_convert_examples_to_features, glue_processors\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = BATCH_SIZE * 2\n",
    "EPOCHS = 3\n",
    "\n",
    "TASK = \"sts-b\"\n",
    "\n",
    "if TASK == \"sst-2\":\n",
    "    TFDS_TASK = \"sst2\"\n",
    "elif TASK == \"sts-b\":\n",
    "    TFDS_TASK = \"stsb\"\n",
    "else:\n",
    "    TFDS_TASK = TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_labels = len(glue_processors[TASK]().get_labels())\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load pre-computed datasetinfo (eg: splits) from bucket.\n",
      "INFO:absl:Loading info from GCS for glue/stsb/1.0.0\n",
      "INFO:absl:Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Generating dataset glue (/home/leoqaz12/tensorflow_datasets/glue/stsb/1.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset glue (784.05 KiB) to /home/leoqaz12/tensorflow_datasets/glue/stsb/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09fadca750e344cdbf5c43bf33d0e372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ab7475311c4130a08ae85f5a838b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7de9b132a94038b0d1cbddbda9aab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5 into /home/leoqaz12/tensorflow_datasets/downloads/fire.goog.com_v0_b_mtl-sent-repr.apps.co6bt4yD9AxpUHFx_A8-JendYtpqj4z0a2BRSkXkk3quA.zipalt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5.tmp.d2551b953d7e40bdb9c5e504ec6e148f...\n",
      "/home/leoqaz12/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "INFO:absl:Generating split train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to /home/leoqaz12/tensorflow_datasets/glue/stsb/1.0.0.incompleteJJHSAK/glue-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e9fccaab7d49b29ba0c3719aa2e3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5749.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing /home/leoqaz12/tensorflow_datasets/glue/stsb/1.0.0.incompleteJJHSAK/glue-train.tfrecord. Shard lengths: [5749]\n",
      "INFO:absl:Generating split validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to /home/leoqaz12/tensorflow_datasets/glue/stsb/1.0.0.incompleteJJHSAK/glue-validation.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90f7a82a54e4479bd9b97839f9593b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing /home/leoqaz12/tensorflow_datasets/glue/stsb/1.0.0.incompleteJJHSAK/glue-validation.tfrecord. Shard lengths: [1500]\n",
      "INFO:absl:Generating split test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to /home/leoqaz12/tensorflow_datasets/glue/stsb/1.0.0.incompleteJJHSAK/glue-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3006b08c315746d0920b4f36f2db3b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing /home/leoqaz12/tensorflow_datasets/glue/stsb/1.0.0.incompleteJJHSAK/glue-test.tfrecord. Shard lengths: [1379]\n",
      "INFO:absl:Skipping computing stats for mode ComputeStatsMode.AUTO.\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /home/leoqaz12/tensorflow_datasets/glue/stsb/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset glue downloaded and prepared to /home/leoqaz12/tensorflow_datasets/glue/stsb/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\r"
     ]
    }
   ],
   "source": [
    "data, info = tensorflow_datasets.load(f'glue/{TFDS_TASK}',with_info=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, 128, TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = info.splits['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
      "array([ 101,  138, 1685, 1873, 1110, 2807, 1113, 3364,  112,  188, 4443,\n",
      "        119,  102,  138, 1376, 1873, 1110, 2807, 1113, 3364,  112,  188,\n",
      "       4443,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=4>)\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataset:\n",
    "    print(data)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
