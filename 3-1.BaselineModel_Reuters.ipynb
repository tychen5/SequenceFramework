{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.text import *\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import gc\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(all_length):\n",
    "    '''\n",
    "    input: length list of elements e.g.[1,1,1,3,5,9,4,2,1,3,54,78,5...]\n",
    "    output1: mean、std、mode、min、q1、median(q2)、q3、max、iqr、outlier、far out\n",
    "    output2: statistics graph、10%~90% form\n",
    "    '''\n",
    "    stat_dict = {}\n",
    "    stat_dict['mean'] = np.mean(all_length)\n",
    "    stat_dict['std'] = np.std(all_length)\n",
    "    stat_dict['mode'] = np.argmax(np.bincount(all_length))\n",
    "    stat_dict['min'] = np.min(all_length)\n",
    "    stat_dict['q1'] = np.quantile(all_length,0.25)\n",
    "    stat_dict['median'] = np.quantile(all_length,0.5)\n",
    "    stat_dict['q3'] = np.quantile(all_length,0.75)\n",
    "    stat_dict['max'] = np.max(all_length)\n",
    "    stat_dict['iqr'] = stat_dict['q3'] - stat_dict['q1']\n",
    "    stat_dict['outlier'] = stat_dict['q3'] + 1.5*stat_dict['iqr']\n",
    "    stat_dict['far_out'] = stat_dict['q3'] + 3*stat_dict['iqr']\n",
    "    for i in [10,20,30,40,50,60,70,80,90,100]:\n",
    "        stat_dict[str(i)+'%'] = np.percentile(all_length,i)\n",
    "    return pd.DataFrame.from_dict(stat_dict,orient='index',columns=['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 8352#8352 #Top most frequent words to consider. Any less frequent word will appear as oov_char value in the sequence data.\n",
    "max_length = 360#360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words#: 30979\n",
      "8260 train sequences\n",
      "2066 test sequences\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "print('all_words#:',len(word_index))\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,maxlen=max_length,\n",
    "                                                         test_split=0.2,seed=830913)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>145.964197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>145.878476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mode</th>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q1</th>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q3</th>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2376.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iqr</th>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outlier</th>\n",
       "      <td>360.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far_out</th>\n",
       "      <td>540.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30%</th>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40%</th>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60%</th>\n",
       "      <td>112.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70%</th>\n",
       "      <td>154.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>206.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>315.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>2376.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              length\n",
       "mean      145.964197\n",
       "std       145.878476\n",
       "mode       17.000000\n",
       "min         2.000000\n",
       "q1         60.000000\n",
       "median     95.000000\n",
       "q3        180.000000\n",
       "max      2376.000000\n",
       "iqr       120.000000\n",
       "outlier   360.000000\n",
       "far_out   540.000000\n",
       "10%        35.000000\n",
       "20%        53.000000\n",
       "30%        67.000000\n",
       "40%        81.000000\n",
       "50%        95.000000\n",
       "60%       112.000000\n",
       "70%       154.000000\n",
       "80%       206.000000\n",
       "90%       315.000000\n",
       "100%     2376.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_len = [len(x) for x in x_train]\n",
    "# test_len = [len(x) for x in x_test]\n",
    "# all_len = train_len\n",
    "# all_len.extend(test_len)\n",
    "# basic_statistics(all_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(all_len)\n",
    "# df.to_excel('./results/length_dist.xlsx', header=False, index=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 11228,\n",
       "         53: 4213,\n",
       "         352: 647,\n",
       "         26: 8451,\n",
       "         14: 15015,\n",
       "         279: 801,\n",
       "         39: 5818,\n",
       "         72: 3091,\n",
       "         4497: 26,\n",
       "         18: 11039,\n",
       "         83: 2597,\n",
       "         5291: 21,\n",
       "         88: 2381,\n",
       "         5397: 20,\n",
       "         11: 20141,\n",
       "         3412: 37,\n",
       "         19: 10755,\n",
       "         151: 1363,\n",
       "         230: 962,\n",
       "         831: 253,\n",
       "         15: 13329,\n",
       "         165: 1232,\n",
       "         318: 707,\n",
       "         3780: 33,\n",
       "         124: 1676,\n",
       "         1527: 117,\n",
       "         1424: 128,\n",
       "         35: 6588,\n",
       "         5302: 20,\n",
       "         12: 16668,\n",
       "         17: 11191,\n",
       "         486: 459,\n",
       "         341: 663,\n",
       "         142: 1466,\n",
       "         255: 870,\n",
       "         219: 997,\n",
       "         429: 528,\n",
       "         68: 3363,\n",
       "         146: 1402,\n",
       "         252: 882,\n",
       "         191: 1098,\n",
       "         15448: 3,\n",
       "         3631: 35,\n",
       "         2283: 65,\n",
       "         71: 3120,\n",
       "         10: 29581,\n",
       "         342: 660,\n",
       "         49: 4565,\n",
       "         1977: 80,\n",
       "         324: 695,\n",
       "         27: 8311,\n",
       "         9222: 8,\n",
       "         672: 330,\n",
       "         450: 506,\n",
       "         5: 42393,\n",
       "         547: 406,\n",
       "         40: 5593,\n",
       "         471: 476,\n",
       "         4810: 24,\n",
       "         149: 1371,\n",
       "         26639: 1,\n",
       "         794: 266,\n",
       "         734: 300,\n",
       "         8976: 8,\n",
       "         8975: 8,\n",
       "         4007: 31,\n",
       "         9: 29956,\n",
       "         25113: 1,\n",
       "         247: 904,\n",
       "         8: 29978,\n",
       "         1299: 144,\n",
       "         381: 618,\n",
       "         34: 7010,\n",
       "         385: 604,\n",
       "         13: 15224,\n",
       "         109: 1810,\n",
       "         167: 1224,\n",
       "         4: 82723,\n",
       "         60: 3654,\n",
       "         130: 1617,\n",
       "         1461: 123,\n",
       "         3366: 38,\n",
       "         1896: 86,\n",
       "         204: 1046,\n",
       "         33: 7037,\n",
       "         3512: 36,\n",
       "         888: 234,\n",
       "         25: 8579,\n",
       "         423: 535,\n",
       "         256: 866,\n",
       "         7: 33157,\n",
       "         1075: 185,\n",
       "         329: 685,\n",
       "         769: 281,\n",
       "         302: 742,\n",
       "         113: 1749,\n",
       "         446: 508,\n",
       "         107: 1841,\n",
       "         226: 979,\n",
       "         240: 923,\n",
       "         817: 258,\n",
       "         101: 1923,\n",
       "         36: 6436,\n",
       "         189: 1107,\n",
       "         6501: 15,\n",
       "         57: 3756,\n",
       "         6: 40350,\n",
       "         481: 464,\n",
       "         239: 927,\n",
       "         118: 1704,\n",
       "         246: 905,\n",
       "         424: 535,\n",
       "         386: 601,\n",
       "         415: 545,\n",
       "         1677: 103,\n",
       "         24: 9022,\n",
       "         291: 767,\n",
       "         662: 337,\n",
       "         85: 2474,\n",
       "         2243: 67,\n",
       "         880: 236,\n",
       "         224: 988,\n",
       "         1077: 184,\n",
       "         54: 3858,\n",
       "         29: 7797,\n",
       "         530: 419,\n",
       "         30: 7627,\n",
       "         1500: 119,\n",
       "         4175: 29,\n",
       "         69: 3203,\n",
       "         12040: 5,\n",
       "         84: 2506,\n",
       "         442: 509,\n",
       "         31: 7288,\n",
       "         373: 628,\n",
       "         159: 1298,\n",
       "         7370: 12,\n",
       "         41: 5553,\n",
       "         351: 648,\n",
       "         343: 656,\n",
       "         6880: 14,\n",
       "         21: 10377,\n",
       "         840: 249,\n",
       "         184: 1129,\n",
       "         5463: 20,\n",
       "         526: 420,\n",
       "         87: 2405,\n",
       "         108: 1812,\n",
       "         140: 1471,\n",
       "         168: 1207,\n",
       "         231: 961,\n",
       "         286: 779,\n",
       "         1351: 138,\n",
       "         397: 574,\n",
       "         105: 1866,\n",
       "         3497: 36,\n",
       "         120: 1694,\n",
       "         1456: 124,\n",
       "         161: 1272,\n",
       "         64: 3453,\n",
       "         1909: 84,\n",
       "         762: 283,\n",
       "         550: 403,\n",
       "         325: 694,\n",
       "         1766: 95,\n",
       "         613: 365,\n",
       "         548: 406,\n",
       "         3205: 41,\n",
       "         16: 12395,\n",
       "         5700: 18,\n",
       "         15663: 3,\n",
       "         51: 4256,\n",
       "         562: 396,\n",
       "         299: 753,\n",
       "         45: 5081,\n",
       "         306: 733,\n",
       "         194: 1086,\n",
       "         572: 388,\n",
       "         1222: 156,\n",
       "         3586: 35,\n",
       "         7718: 11,\n",
       "         22: 9345,\n",
       "         3297: 39,\n",
       "         3380: 38,\n",
       "         66: 3380,\n",
       "         2794: 50,\n",
       "         1163: 167,\n",
       "         178: 1160,\n",
       "         74: 3060,\n",
       "         865: 242,\n",
       "         46: 4836,\n",
       "         353: 645,\n",
       "         134: 1564,\n",
       "         70: 3184,\n",
       "         4596: 25,\n",
       "         86: 2434,\n",
       "         47: 4688,\n",
       "         4315: 27,\n",
       "         597: 377,\n",
       "         688: 323,\n",
       "         5242: 21,\n",
       "         32: 7100,\n",
       "         4215: 28,\n",
       "         63: 3492,\n",
       "         180: 1143,\n",
       "         183: 1133,\n",
       "         61: 3652,\n",
       "         2979: 46,\n",
       "         59: 3689,\n",
       "         4999: 22,\n",
       "         123: 1680,\n",
       "         235: 946,\n",
       "         131: 1607,\n",
       "         891: 233,\n",
       "         4088: 30,\n",
       "         98: 2063,\n",
       "         1025: 196,\n",
       "         633: 351,\n",
       "         2543: 58,\n",
       "         150: 1366,\n",
       "         710: 313,\n",
       "         220: 994,\n",
       "         48: 4574,\n",
       "         864: 243,\n",
       "         276: 809,\n",
       "         360: 641,\n",
       "         211: 1026,\n",
       "         1004: 203,\n",
       "         128: 1619,\n",
       "         10485: 6,\n",
       "         5769: 18,\n",
       "         651: 343,\n",
       "         574: 387,\n",
       "         400: 571,\n",
       "         3673: 34,\n",
       "         6189: 16,\n",
       "         186: 1121,\n",
       "         3879: 32,\n",
       "         968: 212,\n",
       "         90: 2331,\n",
       "         1081: 183,\n",
       "         5604: 19,\n",
       "         4167: 29,\n",
       "         14223: 3,\n",
       "         265: 850,\n",
       "         861: 244,\n",
       "         483: 461,\n",
       "         2858: 48,\n",
       "         3044: 44,\n",
       "         2576: 56,\n",
       "         551: 401,\n",
       "         50: 4383,\n",
       "         5702: 18,\n",
       "         1594: 110,\n",
       "         2161: 71,\n",
       "         111: 1753,\n",
       "         304: 735,\n",
       "         44: 5082,\n",
       "         2128: 72,\n",
       "         3632: 35,\n",
       "         62: 3646,\n",
       "         4200: 29,\n",
       "         2535: 58,\n",
       "         160: 1298,\n",
       "         294: 764,\n",
       "         76: 3019,\n",
       "         67: 3374,\n",
       "         1872: 87,\n",
       "         915: 227,\n",
       "         89: 2371,\n",
       "         135: 1519,\n",
       "         312: 715,\n",
       "         117: 1705,\n",
       "         225: 979,\n",
       "         206: 1042,\n",
       "         152: 1363,\n",
       "         372: 629,\n",
       "         680: 325,\n",
       "         37: 6266,\n",
       "         38: 6157,\n",
       "         387: 600,\n",
       "         516: 431,\n",
       "         500: 447,\n",
       "         729: 301,\n",
       "         838: 250,\n",
       "         52: 4244,\n",
       "         846: 247,\n",
       "         458: 487,\n",
       "         757: 286,\n",
       "         1605: 109,\n",
       "         3963: 31,\n",
       "         317: 708,\n",
       "         55: 3838,\n",
       "         528: 420,\n",
       "         2457: 60,\n",
       "         16383: 2,\n",
       "         260: 859,\n",
       "         2735: 51,\n",
       "         681: 325,\n",
       "         1195: 159,\n",
       "         4779: 24,\n",
       "         1204: 158,\n",
       "         28: 8056,\n",
       "         196: 1072,\n",
       "         1735: 97,\n",
       "         523: 424,\n",
       "         145: 1407,\n",
       "         2068: 75,\n",
       "         420: 538,\n",
       "         73: 3061,\n",
       "         418: 541,\n",
       "         525: 421,\n",
       "         8255: 10,\n",
       "         102: 1921,\n",
       "         289: 772,\n",
       "         1474: 121,\n",
       "         93: 2246,\n",
       "         1926: 83,\n",
       "         273: 819,\n",
       "         542: 410,\n",
       "         202: 1049,\n",
       "         876: 237,\n",
       "         331: 684,\n",
       "         208: 1034,\n",
       "         147: 1400,\n",
       "         126: 1644,\n",
       "         28507: 1,\n",
       "         661: 339,\n",
       "         16629: 2,\n",
       "         768: 281,\n",
       "         3186: 42,\n",
       "         77: 3011,\n",
       "         238: 931,\n",
       "         43: 5369,\n",
       "         133: 1566,\n",
       "         91: 2277,\n",
       "         410: 550,\n",
       "         132: 1586,\n",
       "         663: 337,\n",
       "         233: 950,\n",
       "         5900: 18,\n",
       "         6404: 16,\n",
       "         4855: 23,\n",
       "         625: 355,\n",
       "         42: 5379,\n",
       "         438: 516,\n",
       "         80: 2749,\n",
       "         1901: 85,\n",
       "         158: 1318,\n",
       "         20: 10746,\n",
       "         355: 645,\n",
       "         6529: 15,\n",
       "         56: 3810,\n",
       "         17636: 2,\n",
       "         938: 220,\n",
       "         316: 709,\n",
       "         100: 1956,\n",
       "         261: 856,\n",
       "         439: 516,\n",
       "         7675: 11,\n",
       "         1286: 145,\n",
       "         29406: 1,\n",
       "         5073: 22,\n",
       "         4755: 24,\n",
       "         2827: 49,\n",
       "         19901: 2,\n",
       "         12035: 5,\n",
       "         9454: 7,\n",
       "         1452: 124,\n",
       "         986: 207,\n",
       "         148: 1374,\n",
       "         732: 301,\n",
       "         310: 724,\n",
       "         281: 793,\n",
       "         200: 1050,\n",
       "         11347: 5,\n",
       "         19454: 2,\n",
       "         1649: 105,\n",
       "         21032: 1,\n",
       "         903: 230,\n",
       "         963: 214,\n",
       "         675: 326,\n",
       "         21760: 1,\n",
       "         4254: 28,\n",
       "         18761: 2,\n",
       "         187: 1121,\n",
       "         188: 1114,\n",
       "         4163: 29,\n",
       "         3114: 43,\n",
       "         546: 406,\n",
       "         3611: 35,\n",
       "         5630: 19,\n",
       "         502: 445,\n",
       "         11066: 5,\n",
       "         701: 316,\n",
       "         765: 281,\n",
       "         1354: 136,\n",
       "         251: 882,\n",
       "         335: 670,\n",
       "         79: 2883,\n",
       "         4117: 29,\n",
       "         22496: 1,\n",
       "         6845: 14,\n",
       "         894: 232,\n",
       "         2302: 65,\n",
       "         205: 1045,\n",
       "         2430: 61,\n",
       "         362: 640,\n",
       "         524: 424,\n",
       "         78: 2905,\n",
       "         3063: 44,\n",
       "         863: 243,\n",
       "         582: 382,\n",
       "         832: 252,\n",
       "         323: 699,\n",
       "         195: 1081,\n",
       "         18932: 2,\n",
       "         22172: 1,\n",
       "         8947: 8,\n",
       "         1178: 162,\n",
       "         144: 1430,\n",
       "         671: 332,\n",
       "         396: 577,\n",
       "         6379: 16,\n",
       "         175: 1176,\n",
       "         1041: 193,\n",
       "         1379: 134,\n",
       "         735: 300,\n",
       "         788: 271,\n",
       "         2763: 51,\n",
       "         270: 826,\n",
       "         23: 9113,\n",
       "         515: 432,\n",
       "         472: 474,\n",
       "         272: 823,\n",
       "         4242: 28,\n",
       "         284: 783,\n",
       "         750: 291,\n",
       "         6115: 17,\n",
       "         5201: 21,\n",
       "         462: 484,\n",
       "         482: 464,\n",
       "         780: 275,\n",
       "         122: 1690,\n",
       "         1047: 191,\n",
       "         138: 1500,\n",
       "         2885: 48,\n",
       "         2528: 58,\n",
       "         4455: 26,\n",
       "         1069: 186,\n",
       "         207: 1038,\n",
       "         2938: 47,\n",
       "         8418: 9,\n",
       "         726: 302,\n",
       "         3711: 34,\n",
       "         2276: 66,\n",
       "         1402: 131,\n",
       "         1088: 181,\n",
       "         203: 1047,\n",
       "         5473: 20,\n",
       "         258: 861,\n",
       "         2819: 49,\n",
       "         3688: 34,\n",
       "         162: 1267,\n",
       "         1643: 105,\n",
       "         5553: 19,\n",
       "         4328: 27,\n",
       "         5453: 20,\n",
       "         921: 225,\n",
       "         139: 1491,\n",
       "         245: 906,\n",
       "         1271: 147,\n",
       "         1555: 114,\n",
       "         156: 1323,\n",
       "         2933: 47,\n",
       "         7392: 12,\n",
       "         749: 291,\n",
       "         6077: 17,\n",
       "         4313: 27,\n",
       "         875: 238,\n",
       "         314: 714,\n",
       "         2080: 74,\n",
       "         5237: 21,\n",
       "         2132: 72,\n",
       "         5529: 19,\n",
       "         1415: 129,\n",
       "         2178: 70,\n",
       "         296: 760,\n",
       "         5405: 20,\n",
       "         3806: 32,\n",
       "         1044: 192,\n",
       "         3952: 31,\n",
       "         363: 640,\n",
       "         842: 248,\n",
       "         852: 246,\n",
       "         4601: 25,\n",
       "         127: 1629,\n",
       "         591: 379,\n",
       "         262: 856,\n",
       "         5030: 22,\n",
       "         13989: 3,\n",
       "         5182: 21,\n",
       "         7018: 13,\n",
       "         3126: 43,\n",
       "         5569: 19,\n",
       "         5963: 17,\n",
       "         11427: 5,\n",
       "         1480: 120,\n",
       "         141: 1468,\n",
       "         2425: 61,\n",
       "         1038: 194,\n",
       "         851: 246,\n",
       "         311: 718,\n",
       "         512: 435,\n",
       "         519: 430,\n",
       "         2385: 62,\n",
       "         497: 450,\n",
       "         580: 383,\n",
       "         6682: 15,\n",
       "         332: 678,\n",
       "         1161: 167,\n",
       "         798: 265,\n",
       "         121: 1693,\n",
       "         1358: 136,\n",
       "         232: 956,\n",
       "         6360: 16,\n",
       "         829: 253,\n",
       "         6082: 17,\n",
       "         356: 643,\n",
       "         179: 1148,\n",
       "         14601: 3,\n",
       "         1139: 173,\n",
       "         2131: 72,\n",
       "         7590: 12,\n",
       "         1094: 180,\n",
       "         1458: 124,\n",
       "         1843: 89,\n",
       "         6084: 17,\n",
       "         7151: 13,\n",
       "         13785: 4,\n",
       "         7931: 11,\n",
       "         645: 346,\n",
       "         2510: 59,\n",
       "         1841: 89,\n",
       "         371: 630,\n",
       "         8891: 8,\n",
       "         297: 759,\n",
       "         900: 231,\n",
       "         3130: 42,\n",
       "         1064: 187,\n",
       "         988: 207,\n",
       "         679: 326,\n",
       "         278: 809,\n",
       "         65: 3440,\n",
       "         2753: 51,\n",
       "         401: 569,\n",
       "         5722: 18,\n",
       "         4821: 24,\n",
       "         201: 1050,\n",
       "         12192: 4,\n",
       "         1384: 132,\n",
       "         2732: 52,\n",
       "         699: 316,\n",
       "         4662: 25,\n",
       "         2307: 65,\n",
       "         9116: 8,\n",
       "         17694: 2,\n",
       "         652: 343,\n",
       "         6989: 13,\n",
       "         1109: 178,\n",
       "         155: 1328,\n",
       "         4957: 22,\n",
       "         1714: 99,\n",
       "         1895: 86,\n",
       "         1545: 115,\n",
       "         1485: 120,\n",
       "         778: 277,\n",
       "         354: 645,\n",
       "         1200: 158,\n",
       "         215: 1017,\n",
       "         181: 1140,\n",
       "         907: 229,\n",
       "         106: 1859,\n",
       "         1669: 104,\n",
       "         464: 482,\n",
       "         359: 643,\n",
       "         222: 990,\n",
       "         480: 464,\n",
       "         4216: 28,\n",
       "         3451: 37,\n",
       "         895: 232,\n",
       "         567: 392,\n",
       "         985: 207,\n",
       "         1434: 127,\n",
       "         18534: 2,\n",
       "         15146: 3,\n",
       "         16226: 3,\n",
       "         824: 256,\n",
       "         669: 333,\n",
       "         670: 333,\n",
       "         114: 1737,\n",
       "         1533: 116,\n",
       "         1365: 135,\n",
       "         1056: 188,\n",
       "         569: 391,\n",
       "         2594: 56,\n",
       "         250: 884,\n",
       "         1393: 131,\n",
       "         2291: 65,\n",
       "         12908: 4,\n",
       "         870: 240,\n",
       "         13684: 4,\n",
       "         19776: 2,\n",
       "         1022: 196,\n",
       "         5331: 20,\n",
       "         3080: 44,\n",
       "         422: 535,\n",
       "         337: 666,\n",
       "         1348: 138,\n",
       "         3207: 41,\n",
       "         15548: 3,\n",
       "         15395: 3,\n",
       "         8487: 9,\n",
       "         13131: 4,\n",
       "         2743: 51,\n",
       "         1679: 103,\n",
       "         1657: 105,\n",
       "         691: 321,\n",
       "         24421: 1,\n",
       "         249: 892,\n",
       "         5901: 18,\n",
       "         979: 208,\n",
       "         1304: 143,\n",
       "         4958: 22,\n",
       "         1449: 125,\n",
       "         4492: 26,\n",
       "         529: 419,\n",
       "         5356: 20,\n",
       "         1601: 109,\n",
       "         1446: 126,\n",
       "         1008: 201,\n",
       "         4040: 30,\n",
       "         104: 1883,\n",
       "         7775: 11,\n",
       "         257: 862,\n",
       "         217: 1001,\n",
       "         553: 400,\n",
       "         2781: 50,\n",
       "         5821: 18,\n",
       "         110: 1758,\n",
       "         8016: 10,\n",
       "         4185: 29,\n",
       "         777: 277,\n",
       "         4034: 30,\n",
       "         1215: 156,\n",
       "         193: 1093,\n",
       "         9162: 8,\n",
       "         58: 3710,\n",
       "         27814: 1,\n",
       "         1614: 108,\n",
       "         3326: 39,\n",
       "         1324: 141,\n",
       "         6046: 17,\n",
       "         709: 313,\n",
       "         16640: 2,\n",
       "         8808: 8,\n",
       "         4374: 27,\n",
       "         4932: 23,\n",
       "         4766: 24,\n",
       "         2005: 79,\n",
       "         9694: 7,\n",
       "         3533: 36,\n",
       "         3388: 38,\n",
       "         3343: 39,\n",
       "         27085: 1,\n",
       "         1833: 89,\n",
       "         321: 703,\n",
       "         9451: 7,\n",
       "         19750: 2,\n",
       "         1745: 96,\n",
       "         1965: 81,\n",
       "         2662: 54,\n",
       "         1262: 149,\n",
       "         2019: 78,\n",
       "         29713: 1,\n",
       "         3467: 37,\n",
       "         3860: 32,\n",
       "         599: 376,\n",
       "         182: 1137,\n",
       "         3138: 42,\n",
       "         1046: 191,\n",
       "         479: 466,\n",
       "         236: 945,\n",
       "         564: 395,\n",
       "         1997: 79,\n",
       "         1209: 157,\n",
       "         736: 299,\n",
       "         2148: 71,\n",
       "         9184: 8,\n",
       "         12187: 4,\n",
       "         308: 730,\n",
       "         1607: 109,\n",
       "         1082: 182,\n",
       "         328: 687,\n",
       "         1226: 155,\n",
       "         210: 1027,\n",
       "         961: 214,\n",
       "         4985: 22,\n",
       "         2023: 77,\n",
       "         1016: 199,\n",
       "         448: 506,\n",
       "         724: 305,\n",
       "         303: 736,\n",
       "         909: 229,\n",
       "         223: 989,\n",
       "         2040: 76,\n",
       "         3016: 45,\n",
       "         11303: 5,\n",
       "         287: 776,\n",
       "         8269: 10,\n",
       "         3227: 41,\n",
       "         198: 1058,\n",
       "         3235: 41,\n",
       "         1898: 85,\n",
       "         1024: 196,\n",
       "         1974: 80,\n",
       "         12497: 4,\n",
       "         11381: 5,\n",
       "         2563: 57,\n",
       "         478: 467,\n",
       "         9633: 7,\n",
       "         97: 2077,\n",
       "         305: 734,\n",
       "         1066: 186,\n",
       "         585: 380,\n",
       "         177: 1166,\n",
       "         6794: 14,\n",
       "         4759: 24,\n",
       "         253: 877,\n",
       "         228: 964,\n",
       "         1457: 124,\n",
       "         1927: 83,\n",
       "         112: 1753,\n",
       "         1349: 138,\n",
       "         616: 363,\n",
       "         1873: 87,\n",
       "         214: 1022,\n",
       "         212: 1025,\n",
       "         75: 3040,\n",
       "         1572: 112,\n",
       "         2505: 59,\n",
       "         190: 1100,\n",
       "         447: 507,\n",
       "         508: 437,\n",
       "         1171: 165,\n",
       "         358: 643,\n",
       "         1414: 129,\n",
       "         654: 343,\n",
       "         1218: 156,\n",
       "         967: 212,\n",
       "         457: 489,\n",
       "         1136: 173,\n",
       "         1622: 107,\n",
       "         1007: 202,\n",
       "         1237: 153,\n",
       "         3447: 37,\n",
       "         9263: 8,\n",
       "         657: 342,\n",
       "         820: 257,\n",
       "         1673: 103,\n",
       "         6851: 14,\n",
       "         1521: 117,\n",
       "         95: 2162,\n",
       "         23859: 1,\n",
       "         711: 312,\n",
       "         444: 508,\n",
       "         744: 294,\n",
       "         1515: 117,\n",
       "         2009: 78,\n",
       "         6847: 14,\n",
       "         3850: 32,\n",
       "         378: 621,\n",
       "         2825: 49,\n",
       "         958: 215,\n",
       "         1207: 157,\n",
       "         129: 1617,\n",
       "         485: 460,\n",
       "         154: 1331,\n",
       "         3030: 45,\n",
       "         992: 205,\n",
       "         1242: 152,\n",
       "         816: 258,\n",
       "         2985: 45,\n",
       "         571: 388,\n",
       "         520: 427,\n",
       "         725: 304,\n",
       "         115: 1737,\n",
       "         14289: 3,\n",
       "         10148: 6,\n",
       "         6768: 14,\n",
       "         11543: 5,\n",
       "         1524: 117,\n",
       "         7321: 12,\n",
       "         608: 369,\n",
       "         1176: 162,\n",
       "         3596: 35,\n",
       "         173: 1182,\n",
       "         563: 395,\n",
       "         488: 458,\n",
       "         4533: 26,\n",
       "         1413: 129,\n",
       "         1807: 91,\n",
       "         2666: 53,\n",
       "         408: 552,\n",
       "         164: 1243,\n",
       "         586: 380,\n",
       "         2335: 63,\n",
       "         1111: 178,\n",
       "         1666: 104,\n",
       "         1322: 141,\n",
       "         1290: 145,\n",
       "         4756: 24,\n",
       "         404: 563,\n",
       "         908: 229,\n",
       "         740: 295,\n",
       "         621: 359,\n",
       "         1316: 142,\n",
       "         15632: 3,\n",
       "         3815: 32,\n",
       "         650: 343,\n",
       "         1825: 90,\n",
       "         4234: 28,\n",
       "         673: 330,\n",
       "         3145: 42,\n",
       "         192: 1095,\n",
       "         510: 436,\n",
       "         604: 371,\n",
       "         2147: 71,\n",
       "         506: 440,\n",
       "         443: 508,\n",
       "         469: 476,\n",
       "         11887: 5,\n",
       "         2564: 57,\n",
       "         125: 1665,\n",
       "         454: 493,\n",
       "         19191: 2,\n",
       "         1152: 169,\n",
       "         96: 2097,\n",
       "         25255: 1,\n",
       "         6087: 17,\n",
       "         847: 247,\n",
       "         3530: 36,\n",
       "         3427: 37,\n",
       "         2852: 49,\n",
       "         1107: 178,\n",
       "         1742: 96,\n",
       "         81: 2664,\n",
       "         5207: 21,\n",
       "         2542: 58,\n",
       "         1450: 125,\n",
       "         116: 1706,\n",
       "         1498: 119,\n",
       "         2256: 66,\n",
       "         1467: 122,\n",
       "         1476: 121,\n",
       "         5163: 21,\n",
       "         1435: 127,\n",
       "         395: 578,\n",
       "         4568: 25,\n",
       "         4214: 28,\n",
       "         3031: 44,\n",
       "         6719: 14,\n",
       "         4341: 27,\n",
       "         1009: 200,\n",
       "         716: 307,\n",
       "         1578: 112,\n",
       "         4616: 25,\n",
       "         738: 297,\n",
       "         5060: 22,\n",
       "         3857: 32,\n",
       "         1423: 128,\n",
       "         507: 439,\n",
       "         384: 605,\n",
       "         1281: 146,\n",
       "         615: 363,\n",
       "         1540: 115,\n",
       "         1026: 196,\n",
       "         899: 231,\n",
       "         715: 307,\n",
       "         602: 373,\n",
       "         290: 769,\n",
       "         349: 653,\n",
       "         737: 297,\n",
       "         614: 365,\n",
       "         850: 246,\n",
       "         452: 496,\n",
       "         552: 401,\n",
       "         170: 1196,\n",
       "         5668: 19,\n",
       "         2582: 56,\n",
       "         761: 283,\n",
       "         266: 842,\n",
       "         3244: 40,\n",
       "         941: 219,\n",
       "         242: 920,\n",
       "         1225: 155,\n",
       "         477: 467,\n",
       "         2575: 56,\n",
       "         24857: 1,\n",
       "         12612: 4,\n",
       "         7005: 13,\n",
       "         2673: 53,\n",
       "         4081: 30,\n",
       "         4131: 29,\n",
       "         1360: 135,\n",
       "         884: 235,\n",
       "         8520: 9,\n",
       "         13719: 4,\n",
       "         12262: 4,\n",
       "         2888: 48,\n",
       "         339: 665,\n",
       "         609: 368,\n",
       "         812: 259,\n",
       "         13737: 4,\n",
       "         419: 539,\n",
       "         434: 520,\n",
       "         277: 809,\n",
       "         295: 761,\n",
       "         92: 2248,\n",
       "         7315: 12,\n",
       "         3748: 33,\n",
       "         280: 798,\n",
       "         209: 1033,\n",
       "         2142: 72,\n",
       "         12833: 4,\n",
       "         390: 589,\n",
       "         473: 474,\n",
       "         248: 901,\n",
       "         1362: 135,\n",
       "         20092: 2,\n",
       "         747: 291,\n",
       "         218: 998,\n",
       "         5589: 19,\n",
       "         1475: 121,\n",
       "         2669: 53,\n",
       "         476: 469,\n",
       "         800: 264,\n",
       "         3522: 36,\n",
       "         6930: 14,\n",
       "         4296: 28,\n",
       "         3367: 38,\n",
       "         487: 459,\n",
       "         594: 378,\n",
       "         6929: 14,\n",
       "         14191: 3,\n",
       "         810: 260,\n",
       "         7949: 11,\n",
       "         1061: 187,\n",
       "         1158: 168,\n",
       "         3505: 36,\n",
       "         2918: 47,\n",
       "         3026: 45,\n",
       "         3218: 41,\n",
       "         559: 397,\n",
       "         2014: 78,\n",
       "         1129: 175,\n",
       "         7346: 12,\n",
       "         853: 245,\n",
       "         15507: 3,\n",
       "         581: 382,\n",
       "         103: 1911,\n",
       "         10487: 6,\n",
       "         1208: 157,\n",
       "         7764: 11,\n",
       "         719: 307,\n",
       "         13098: 4,\n",
       "         6696: 14,\n",
       "         166: 1226,\n",
       "         730: 301,\n",
       "         15939: 3,\n",
       "         6122: 17,\n",
       "         3272: 40,\n",
       "         94: 2181,\n",
       "         1159: 167,\n",
       "         793: 267,\n",
       "         4252: 28,\n",
       "         176: 1172,\n",
       "         1112: 177,\n",
       "         791: 267,\n",
       "         3520: 36,\n",
       "         2248: 66,\n",
       "         1327: 141,\n",
       "         8776: 9,\n",
       "         605: 370,\n",
       "         172: 1182,\n",
       "         1202: 158,\n",
       "         897: 232,\n",
       "         1516: 117,\n",
       "         1059: 188,\n",
       "         157: 1319,\n",
       "         ...})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_words = []\n",
    "# for x in x_train:\n",
    "#     train_words.extend(x)\n",
    "# test_words = []\n",
    "# for x in x_test:\n",
    "#     test_words.extend(x)\n",
    "# all_words = train_words\n",
    "# all_words.extend(test_words)\n",
    "# all_statistcs = Counter(all_words)\n",
    "# all_statistcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24452</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18567</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27222</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26864</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24794</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30980 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "1      11228\n",
       "53      4213\n",
       "352      647\n",
       "26      8451\n",
       "14     15015\n",
       "...      ...\n",
       "24452      1\n",
       "18567      2\n",
       "27222      1\n",
       "26864      1\n",
       "24794      1\n",
       "\n",
       "[30980 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.DataFrame.from_dict(dict(all_statistcs), orient = 'index')\n",
    "# df.to_excel('./results/words_dist2.xlsx', header=False, index=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8260, 360) (2066, 360)\n"
     ]
    }
   ],
   "source": [
    "trainX = tf.keras.preprocessing.sequence.pad_sequences(x_train,maxlen=max_length,padding='post',value=0)\n",
    "testX = tf.keras.preprocessing.sequence.pad_sequences(x_test,maxlen=max_length,padding='post',value=0)\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "# do = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph execution\n",
    "### Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_id = Input(shape=(max_length,), dtype='int32', name='int_ids') # 輸入的api funvtion name ID\n",
    "int_ids = Masking(mask_value=0)(int_id)\n",
    "sent_emb = Embedding(max_words, hidden_dim,input_length=max_length\n",
    "                    ,trainable=True,name='glove_emb')(int_ids) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = GRU(int(hidden_dim/2),return_sequences=True,return_state=False,name='common_extract'\n",
    "                      ,trainable=True)(sent_emb)\n",
    "rnn = BatchNormalization(name='bn')(rnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil = TimeDistributed(Dense(1,activation='sigmoid',\n",
    "                             name='filter_out'),name='TD2')(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul = Multiply()([fil,sent_emb])\n",
    "clf = LSTM(int(hidden_dim/2),dropout=do,recurrent_dropout=do,name='lstm')(mul)\n",
    "clf = BatchNormalization(name='bn3')(clf)\n",
    "clf = Dense(max(y_train)+1,activation='softmax',name='clf')(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "int_ids (InputLayer)            [(None, 358)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 358)          0           int_ids[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "glove_emb (Embedding)           (None, 358, 128)     1280000     masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "common_extract (GRU)            (None, 358, 64)      37248       glove_emb[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 358, 64)      256         common_extract[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TD2 (TimeDistributed)           (None, 358, 1)       65          bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 358, 128)     0           TD2[0][0]                        \n",
      "                                                                 glove_emb[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 64)           49408       multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn3 (BatchNormalization)        (None, 64)           256         lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "clf (Dense)                     (None, 46)           2990        bn3[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 1,370,223\n",
      "Trainable params: 1,369,967\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=int_id, outputs = clf)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-189e30c4af05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     return K.sum(layer.output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# kk = tf.keras.backend.ea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TD2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       self._track_trackable(\n\u001b[1;32m    294\u001b[0m           self.optimizer, name='optimizer', overwrite=True)\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weight_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \"\"\"\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_bool_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_bool_casting\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m       \u001b[0;31m# Default: V1-style Graph execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_in_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using a `tf.Tensor` as a Python `bool`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_disallow_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_in_graph_mode\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    521\u001b[0m     raise errors.OperatorNotAllowedInGraphError(\n\u001b[1;32m    522\u001b[0m         \u001b[0;34m\"{} is not allowed in Graph execution. Use Eager execution or decorate\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \" this function with @tf.function.\".format(task))\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_disallow_bool_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function."
     ]
    }
   ],
   "source": [
    "# loss\n",
    "import keras.backend as K\n",
    "def custom_objective(layer):\n",
    "    return K.sum(layer.output)\n",
    "#     return K.sum(layer.output)\n",
    "# kk = tf.keras.backend.ea\n",
    "model.compile(loss=custom_objective(model.get_layer(name='TD2')),optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole model\n",
    "do = 0\n",
    "init = tensorflow.keras.initializers.Ones()\n",
    "class base_model(Model):\n",
    "    def __init__(self):\n",
    "        super(base_model, self).__init__()\n",
    "        self.mask = Masking(mask_value=0)\n",
    "        self.emb = Embedding(max_words, hidden_dim,input_length=max_length\n",
    "                    ,trainable=True,name='glove_emb')\n",
    "        self.rnn1 = GRU(int(hidden_dim/2),return_sequences=True,return_state=False,name='common_extract'\n",
    "                      ,trainable=True)\n",
    "        self.bn1 = BatchNormalization(name='bn1')\n",
    "        self.fil = Dense(1,activation='hard_sigmoid',kernel_initializer=init,bias_initializer=init,name='filter_out')\n",
    "        #self.fil = TimeDistributed(Dense(1,activation='sigmoid', name='filter_out'),name='TD2')\n",
    "        self.mul = Multiply()\n",
    "        self.rnn2 = Bidirectional(GRU(int(hidden_dim/2),dropout=do,recurrent_dropout=do,name='lstm'))\n",
    "        self.rnn3 = LSTM(int(hidden_dim/2))\n",
    "        self.bn2 = BatchNormalization(name='bn2')\n",
    "        self.out = Dense(max(y_train)+1,activation='softmax',name='clf')\n",
    "    def transform(self,x):\n",
    "        return tf.math.round(x)\n",
    "    def call(self,x):\n",
    "        x = self.mask(x)\n",
    "        x1 = self.emb(x)\n",
    "        x = self.rnn1(x1)\n",
    "        x = self.bn1(x)\n",
    "        y = self.fil(x)\n",
    "        y1 = self.transform(y)\n",
    "        x2 = self.mul([y1,x1])\n",
    "        x = self.rnn2(x2) #x\n",
    "        x = self.bn2(x)\n",
    "        y2 = self.out(x)\n",
    "        return y,y1,y2\n",
    "        #return y,y1,y2,x2\n",
    "        \n",
    "model = base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial1 model\n",
    "init_w = tensorflow.keras.initializers.Constant(value=1.2) #portyion=0.6, w=0.9, b = 0.8-0.85 (0.83從0開始)\n",
    "init_b = tensorflow.keras.initializers.Constant(value=1.2) #w=1 ; b=0.499, portion=1\n",
    "def onezero(x):\n",
    "    portion = 0.6#0.6#0.6 #0.6~1\n",
    "    z = tf.where(x>=1.0, x - x + 1.0, x)\n",
    "    y = tf.where(z<=0.0, z - z + 0.0, portion*z)\n",
    "    return y\n",
    "\n",
    "class base_model_1(Model):\n",
    "    def __init__(self):\n",
    "        super(base_model_1, self).__init__()\n",
    "        self.mask = Masking(mask_value=0)\n",
    "        self.emb = Embedding(max_words, hidden_dim,input_length=max_length\n",
    "                    ,trainable=True,name='glove_emb')\n",
    "        #self.rnn1 = GRU(int(hidden_dim/4),return_sequences=True,return_state=False,name='common_extract'\n",
    "                      #,trainable=True)\n",
    "        self.att = Attention(name='selfatt')\n",
    "        #self.bn1 = BatchNormalization(name='bn1')\n",
    "        #self.fil = Dense(1,activation=onezero,name='filter_out')\n",
    "        self.fil = TimeDistributed(Dense(1,activation=onezero,kernel_initializer=init_w,bias_initializer=init_b, name='filter_out'),name='TD2') #relu/linear/step function\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.mask(x)\n",
    "        x1 = self.emb(x)\n",
    "        x = self.att([x1,x1])\n",
    "        #x = self.rnn1(x1)\n",
    "        #x = self.bn1(x)\n",
    "        y = self.fil(x)\n",
    "        return x1,y\n",
    "\n",
    "model1 = base_model_1()\n",
    "#phase2\n",
    "# model3 = load_model(saveP)\n",
    "\n",
    "model3 = base_model_1()\n",
    "model3.load_weights('./model/2019110501/model1')#,by_name=True)\n",
    "model1.emb.set_weights(model3.emb.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial2 model\n",
    "class base_model_2(Model):\n",
    "    def __init__(self):\n",
    "        super(base_model_2, self).__init__()\n",
    "        self.mul = Multiply()\n",
    "        self.rnn2 = Bidirectional(GRU(int(hidden_dim/2),dropout=do,recurrent_dropout=do,name='lstm'))\n",
    "        self.rnn3 = GRU(int(hidden_dim/2))\n",
    "        self.bn2 = BatchNormalization(name='bn2')\n",
    "        self.out = Dense(max(y_train)+1,activation='softmax',name='clf')\n",
    "\n",
    "    def call(self,x1,y1):\n",
    "        x2 = self.mul([y1,x1])\n",
    "        x = self.rnn3(x2) #x2 #y1=weight|binary\n",
    "        x = self.bn2(x)\n",
    "        y2 = self.out(x)\n",
    "        return y2\n",
    "    \n",
    "model2 = base_model_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5770753, shape=(32, 4), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# TEST\n",
    "x = tf.random.uniform((32, 6))\n",
    "out1,out2 = model1(x)\n",
    "out2 = tf.squeeze(out2,axis=-1)\n",
    "# out2 = out2.astype('float32')\n",
    "# out2 = tf.dtypes.cast(out2, tf.float8)\n",
    "# print(out2.shape)\n",
    "yy = tf.matmul(out2,kk)\n",
    "print(yy[0].shape)\n",
    "tf.where(yy[0]==0,1,0)\n",
    "\n",
    "gg = tf.range(-5,5)\n",
    "gg = tf.expand_dims(gg,axis=0)\n",
    "gg = tf.keras.backend.repeat_elements(gg,rep=32,axis=0)\n",
    "tf.where(gg==1,gg,0)\n",
    "\n",
    "kk = tf.Variable(np.array([[1.0,1.0,1.0,0.0,0.0,0.0],[0.0,1.0,1.0,1.0,0.0,0.0],[0.0,0.0,1.0,1.0,1.0,0.0],\n",
    "                           [0.0,0.0,0.0,1.0,1.0,1.0]]).T,dtype='float32')\n",
    "kk = tf.expand_dims(kk,axis=0)\n",
    "kk = tf.keras.backend.repeat_elements(kk,rep=1,axis=0)\n",
    "kk.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "記得要跑到一個好的data\n",
    "# batch_size = 128 #,reshuffle_each_iteration=True\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((trainX,y_train)).shuffle(trainX.shape[0]).batch(batch_size)\n",
    "# valid_ds = tf.data.Dataset.from_tensor_slices((testX,y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=10566360, shape=(1, 360, 359), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 1., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_num = 2 #連續幾個才叫做連續，要改modify_idx看有幾個\n",
    "\n",
    "arr_len = max_length - seq_num + 1\n",
    "seq_arr = []\n",
    "for i in range(arr_len):\n",
    "    ori_np = np.array([0]*max_length)\n",
    "    modify_idx = [i,i+1] #要跟著seq_num改\n",
    "    ori_np[modify_idx]=1\n",
    "    seq_arr.append(ori_np)\n",
    "seq_arr = np.array(seq_arr)\n",
    "seq_mask = tf.Variable(seq_arr.T,dtype='float32')\n",
    "seq_mask = tf.expand_dims(seq_mask,axis=0)\n",
    "seq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_object1(predictions): #filter loss\n",
    "    mask = tf.math.logical_not(tf.math.equal(predictions, 0))\n",
    "    loss_ = tf.reduce_mean(predictions)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "def one_percentage(predictions): #1 num\n",
    "    mask = tf.math.logical_not(tf.math.equal(predictions, 0))\n",
    "    loss_ = tf.reduce_mean(predictions)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "def seq_loss(predictions):\n",
    "    mask = tf.math.logical_not(tf.math.equal(predictions, 0))\n",
    "    predictions = tf.squeeze(predictions,axis=-1)\n",
    "    results = tf.matmul(predictions,seq_mask)\n",
    "    results = tf.where(results==seq_num,1.0,0.0)\n",
    "    loss_ = tf.reduce_mean(results)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "loss_object2 = tf.keras.losses.SparseCategoricalCrossentropy() #clf loss\n",
    "\n",
    "optimizer1 = tf.keras.optimizers.Nadam()\n",
    "optimizer2 = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss') #total_loss\n",
    "train_accloss = tf.keras.metrics.Mean(name='train_accloss')#loss_acc\n",
    "train_filloss = tf.keras.metrics.Mean(name='train_filloss') #loss_filter\n",
    "train_seqloss = tf.keras.metrics.Mean(name='train_seqloss') #loss_seq\n",
    "train_ones = tf.keras.metrics.Mean(name='train_ones') #ones_num\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy') #acc_rate\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss') #total_loss\n",
    "test_accloss = tf.keras.metrics.Mean(name='test_accloss')#loss_acc\n",
    "test_filloss = tf.keras.metrics.Mean(name='test_filloss') #loss_filter\n",
    "test_seqloss = tf.keras.metrics.Mean(name='test_seqloss')\n",
    "test_ones = tf.keras.metrics.Mean(name='test_ones') #ones_num\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate partial model\n",
    "alpha = 0.01 #pahse1: -0.1 / 0.0 ; phase2: 0.01~0.05~0.1 有1-就是希望1越多，沒1-就是希望0越多1越少\n",
    "beta = 1.0 #clf loss 越大越要求分好\n",
    "gamma = 0.0 #seqloss 越大越要求連續\n",
    "#-0.001 / 1.0 / 1.0\n",
    "\n",
    "@tf.function\n",
    "def train_step(x,yc):\n",
    "    with tf.GradientTape(persistent=False) as tape:\n",
    "        emb, pred_imp = model1(x)\n",
    "        #loss1 = alpha*loss_object1(pred_imp) #phase1\n",
    "        #pred_imp2 = tf.math.round(pred_imp)\n",
    "        #pred_imp3 = tf.clip_by_value(pred_imp,clip_value_max=1,clip_value_min=0)\n",
    "        pred_imp2 = tf.math.round(pred_imp)\n",
    "        loss1 = loss_object1(pred_imp) #有1-就是希望1越多，沒1-就是希望0越多1越少 #pahse2:alpha*loss_object1(pred_imp) ; phase1: alpha*(1-loss_object1(pred_imp))\n",
    "        pred_cat = model2(emb,pred_imp2) #pahse1: pred_imp; phase2; pred_imp2\n",
    "        loss2 = loss_object2(yc, pred_cat)\n",
    "        loss3 = 1-seq_loss(pred_imp2)\n",
    "        loss = alpha*loss1 + beta*loss2 + gamma*loss3\n",
    "    trainable_variable = model1.trainable_variables\n",
    "    trainable_variable.extend(model2.trainable_variables)\n",
    "    gradients = tape.gradient(loss,trainable_variable)\n",
    "    optimizer1.apply_gradients(zip(gradients,trainable_variable))\n",
    "    \n",
    "    train_loss(loss) #total_loss\n",
    "    train_filloss(loss1)\n",
    "    train_accloss(loss2)\n",
    "    train_seqloss(loss3) #loss_seq\n",
    "    train_accuracy(yc, pred_cat) #acc_rate\n",
    "    ones = one_percentage(pred_imp2) #pred_imp2\n",
    "    train_ones(ones) #ones_num\n",
    "    \n",
    "    \n",
    "@tf.function\n",
    "def test_step(x,yc):\n",
    "    emb, pred_imp = model1(x)\n",
    "    #loss1 = alpha*loss_object1(pred_imp) #phase1\n",
    "    #pred_imp2 = tf.math.round(pred_imp)\n",
    "    #pred_imp3 = tf.clip_by_value(pred_imp,clip_value_max=1,clip_value_min=0)\n",
    "    pred_imp2 = tf.math.round(pred_imp)\n",
    "    loss1 = loss_object1(pred_imp) #phase2\n",
    "    pred_cat = model2(emb,pred_imp2) #phase1: pred_imp ; phase2:pred_imp2\n",
    "    loss2 = loss_object2(yc, pred_cat)\n",
    "    loss3 = 1-seq_loss(pred_imp2)\n",
    "    #t_loss = loss1 + loss2\n",
    "    t_loss = alpha*loss1 + beta*loss2 + gamma*loss3\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_filloss(loss1)\n",
    "    test_accloss(loss2)\n",
    "    test_seqloss(loss3)\n",
    "    test_accuracy(yc, pred_cat)\n",
    "    t_ones = one_percentage(pred_imp2) #pred_imp2\n",
    "    test_ones(t_ones)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#AIO\n",
    "alpha = 0.1\n",
    "beta = 1\n",
    "gamma = 0\n",
    "@tf.function\n",
    "def train_step(x,yc):\n",
    "    with tf.GradientTape(persistent=True) as tape: #persistent=True\n",
    "        pred_imp,pred_round , pred_cat = model(x)\n",
    "#         pred_cat = model(x)\n",
    "#         loss = alpha*loss_object1(pred_imp) + loss_object2(yc,pred_cat)\n",
    "        loss1 = alpha*loss_object1(pred_imp)\n",
    "        loss2 = beta*loss_object2(yc,pred_cat)\n",
    "        loss = loss_object2(yc,pred_cat)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    grad1 = tape.gradient(loss1, model.trainable_variables)\n",
    "    grad2 = tape.gradient(loss2, model.trainable_variables)\n",
    "#     optimizer1.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    optimizer1.apply_gradients(zip(grad1, model.trainable_variables))\n",
    "    optimizer2.apply_gradients(zip(grad2, model.trainable_variables))\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         pred_imp , pred_cat = model(x)\n",
    "#         loss2 = loss_object2(yc,pred_cat)\n",
    "#         loss = alpha*loss_object1(pred_imp) + loss_object2(yc,pred_cat)\n",
    "#     grad2 = tape.gradient(loss2, model.trainable_variables)\n",
    "#     optimizer2.apply_gradients(zip(grad2, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(yc, pred_cat)\n",
    "    ones = one_percentage(pred_round)\n",
    "    train_ones(ones)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(x,yc):\n",
    "    pred_imp,pred_round, pred_cat = model(x)\n",
    "#     pred_cat = model(x)\n",
    "    t_loss = alpha*loss_object1(pred_imp) + loss_object2(yc,pred_cat) \n",
    "#     t_loss = loss_object2(yc,pred_cat)\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(yc, pred_cat)\n",
    "    t_ones = one_percentage(pred_round)\n",
    "    test_ones(t_ones)\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).emb.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).fil.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).fil.layer.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "Epoch 1, Total Loss: 2.6746320724487305, Clf Loss: 2.6686325073242188, Filter Loss: 0.5999999642372131, Seq Loss: 0.0, Accuracy Rate: 36.54%, Ones Portion: 1.0,             Test_Total_Loss: 2.3719048500061035, Test_Clf_Loss: 2.365905284881592, Test_Filter_Loss: 0.6000000238418579, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 37.17%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 2, Total Loss: 2.342060089111328, Clf Loss: 2.336060047149658, Filter Loss: 0.5999999642372131, Seq Loss: 0.0, Accuracy Rate: 37.72%, Ones Portion: 1.0,             Test_Total_Loss: 2.3184988498687744, Test_Clf_Loss: 2.3124988079071045, Test_Filter_Loss: 0.6000000238418579, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 37.17%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 3, Total Loss: 2.0723845958709717, Clf Loss: 2.0678486824035645, Filter Loss: 0.45359429717063904, Seq Loss: 0.41205495595932007, Accuracy Rate: 47.74%, Ones Portion: 0.5904194712638855,             Test_Total_Loss: 1.962981939315796, Test_Clf_Loss: 1.9627357721328735, Test_Filter_Loss: 0.0246084313839674, TEST_Seq_Loss: 0.999279797077179, Test_Accuracy_Rate: 53.63%, Test_Ones_Portion: 0.0010797155555337667\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 4, Total Loss: 1.9777015447616577, Clf Loss: 1.9775996208190918, Filter Loss: 0.010159243829548359, Seq Loss: 0.9994893074035645, Accuracy Rate: 53.06%, Ones Portion: 0.0008164799655787647,             Test_Total_Loss: 1.9264706373214722, Test_Clf_Loss: 1.9263911247253418, Test_Filter_Loss: 0.007974748499691486, TEST_Seq_Loss: 0.9994752407073975, Test_Accuracy_Rate: 54.50%, Test_Ones_Portion: 0.0008591284276917577\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 5, Total Loss: 1.9084751605987549, Clf Loss: 1.90841805934906, Filter Loss: 0.005699853878468275, Seq Loss: 0.9996069073677063, Accuracy Rate: 54.55%, Ones Portion: 0.0006732068140991032,             Test_Total_Loss: 1.9081346988677979, Test_Clf_Loss: 1.9080756902694702, Test_Filter_Loss: 0.005900170654058456, TEST_Seq_Loss: 0.999587893486023, Test_Accuracy_Rate: 54.50%, Test_Ones_Portion: 0.0007098938222043216\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 6, Total Loss: 1.9027345180511475, Clf Loss: 1.9026916027069092, Filter Loss: 0.004297356121242046, Seq Loss: 0.9996681809425354, Accuracy Rate: 54.48%, Ones Portion: 0.0005989803466945887,             Test_Total_Loss: 1.9115912914276123, Test_Clf_Loss: 1.911543846130371, Test_Filter_Loss: 0.004729165229946375, TEST_Seq_Loss: 0.9996300935745239, Test_Accuracy_Rate: 54.50%, Test_Ones_Portion: 0.0006570451078005135\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 7, Total Loss: 1.9031254053115845, Clf Loss: 1.9030910730361938, Filter Loss: 0.0034183613024652004, Seq Loss: 0.9997135996818542, Accuracy Rate: 54.54%, Ones Portion: 0.0005386673146858811,             Test_Total_Loss: 1.8998512029647827, Test_Clf_Loss: 1.8998128175735474, Test_Filter_Loss: 0.003793285693973303, TEST_Seq_Loss: 0.9996700286865234, Test_Accuracy_Rate: 54.79%, Test_Ones_Portion: 0.0006014726241119206\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 8, Total Loss: 1.8936843872070312, Clf Loss: 1.8936563730239868, Filter Loss: 0.002882933709770441, Seq Loss: 0.999739408493042, Accuracy Rate: 54.77%, Ones Portion: 0.0005020517273806036,             Test_Total_Loss: 1.8895115852355957, Test_Clf_Loss: 1.8894795179367065, Test_Filter_Loss: 0.003206336172297597, TEST_Seq_Loss: 0.9996997714042664, Test_Accuracy_Rate: 55.08%, Test_Ones_Portion: 0.0005602461751550436\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 9, Total Loss: 1.8846380710601807, Clf Loss: 1.8846136331558228, Filter Loss: 0.0024794801138341427, Seq Loss: 0.9997646808624268, Accuracy Rate: 55.10%, Ones Portion: 0.0004679522244259715,             Test_Total_Loss: 1.887339472770691, Test_Clf_Loss: 1.8873122930526733, Test_Filter_Loss: 0.0027249555569142103, TEST_Seq_Loss: 0.9997363090515137, Test_Accuracy_Rate: 55.28%, Test_Ones_Portion: 0.0005154196987859905\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 10, Total Loss: 1.8802573680877686, Clf Loss: 1.8802357912063599, Filter Loss: 0.0021695077884942293, Seq Loss: 0.9997832179069519, Accuracy Rate: 54.89%, Ones Portion: 0.00044468112173490226,             Test_Total_Loss: 1.887415885925293, Test_Clf_Loss: 1.8873916864395142, Test_Filter_Loss: 0.002412464702501893, TEST_Seq_Loss: 0.999737024307251, Test_Accuracy_Rate: 55.23%, Test_Ones_Portion: 0.0005149459466338158\n",
      "Epoch 11, Total Loss: 1.8814566135406494, Clf Loss: 1.8814376592636108, Filter Loss: 0.0019470022525638342, Seq Loss: 0.9997884631156921, Accuracy Rate: 54.98%, Ones Portion: 0.0004340004816185683,             Test_Total_Loss: 1.8873255252838135, Test_Clf_Loss: 1.887304425239563, Test_Filter_Loss: 0.0021274967584758997, TEST_Seq_Loss: 0.9997527003288269, Test_Accuracy_Rate: 55.08%, Test_Ones_Portion: 0.0004943362437188625\n",
      "Epoch 12, Total Loss: 1.8792750835418701, Clf Loss: 1.8792576789855957, Filter Loss: 0.0017164258752018213, Seq Loss: 0.9998070597648621, Accuracy Rate: 55.00%, Ones Portion: 0.0004102549864910543,             Test_Total_Loss: 1.8966270685195923, Test_Clf_Loss: 1.8966079950332642, Test_Filter_Loss: 0.001901106326840818, TEST_Seq_Loss: 0.9997689723968506, Test_Accuracy_Rate: 54.84%, Test_Ones_Portion: 0.00047120769158937037\n",
      "Epoch 13, Total Loss: 1.8849841356277466, Clf Loss: 1.8849684000015259, Filter Loss: 0.001573821296915412, Seq Loss: 0.9998151063919067, Accuracy Rate: 54.92%, Ones Portion: 0.00039773565367795527,             Test_Total_Loss: 1.9012596607208252, Test_Clf_Loss: 1.9012426137924194, Test_Filter_Loss: 0.0016873640706762671, TEST_Seq_Loss: 0.9997740387916565, Test_Accuracy_Rate: 54.74%, Test_Ones_Portion: 0.0004639730614144355\n",
      "Epoch 14, Total Loss: 1.883467674255371, Clf Loss: 1.883453369140625, Filter Loss: 0.0014120403211563826, Seq Loss: 0.9998258352279663, Accuracy Rate: 54.94%, Ones Portion: 0.00038122644764371216,             Test_Total_Loss: 1.8910436630249023, Test_Clf_Loss: 1.8910282850265503, Test_Filter_Loss: 0.0015726402634754777, TEST_Seq_Loss: 0.9997852444648743, Test_Accuracy_Rate: 54.94%, Test_Ones_Portion: 0.0004480869974941015\n",
      "Epoch 15, Total Loss: 1.8829184770584106, Clf Loss: 1.8829052448272705, Filter Loss: 0.0013579698279500008, Seq Loss: 0.9998223185539246, Accuracy Rate: 54.89%, Ones Portion: 0.0003877545241266489,             Test_Total_Loss: 1.8938674926757812, Test_Clf_Loss: 1.8938524723052979, Test_Filter_Loss: 0.001488467096351087, TEST_Seq_Loss: 0.9997908473014832, Test_Accuracy_Rate: 54.99%, Test_Ones_Portion: 0.0004411526897456497\n",
      "Epoch 16, Total Loss: 1.8806718587875366, Clf Loss: 1.8806593418121338, Filter Loss: 0.0012695301556959748, Seq Loss: 0.9998321533203125, Accuracy Rate: 54.96%, Ones Portion: 0.00037202544626779854,             Test_Total_Loss: 1.893615961074829, Test_Clf_Loss: 1.8936017751693726, Test_Filter_Loss: 0.0014072289923205972, TEST_Seq_Loss: 0.9998012781143188, Test_Accuracy_Rate: 54.89%, Test_Ones_Portion: 0.0004284769238438457\n",
      "Epoch 17, Total Loss: 1.883758544921875, Clf Loss: 1.8837469816207886, Filter Loss: 0.001166247297078371, Seq Loss: 0.99983811378479, Accuracy Rate: 54.90%, Ones Portion: 0.00036166925565339625,             Test_Total_Loss: 1.8911051750183105, Test_Clf_Loss: 1.8910919427871704, Test_Filter_Loss: 0.0013243734138086438, TEST_Seq_Loss: 0.9998072385787964, Test_Accuracy_Rate: 54.99%, Test_Ones_Portion: 0.00042077002581208944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Total Loss: 1.8808786869049072, Clf Loss: 1.880867600440979, Filter Loss: 0.0011228392831981182, Seq Loss: 0.9998456239700317, Accuracy Rate: 55.00%, Ones Portion: 0.00035443142405711114,             Test_Total_Loss: 1.8950473070144653, Test_Clf_Loss: 1.8950347900390625, Test_Filter_Loss: 0.0012437193654477596, TEST_Seq_Loss: 0.9998156428337097, Test_Accuracy_Rate: 54.99%, Test_Ones_Portion: 0.0004081874212715775\n",
      "Epoch 19, Total Loss: 1.8845950365066528, Clf Loss: 1.8845845460891724, Filter Loss: 0.0010422863997519016, Seq Loss: 0.9998493790626526, Accuracy Rate: 54.85%, Ones Portion: 0.00034853865508921444,             Test_Total_Loss: 1.891613245010376, Test_Clf_Loss: 1.8916015625, Test_Filter_Loss: 0.0011725371005013585, TEST_Seq_Loss: 0.999826967716217, Test_Accuracy_Rate: 54.99%, Test_Ones_Portion: 0.0003918083675671369\n",
      "Epoch 20, Total Loss: 1.882252812385559, Clf Loss: 1.8822427988052368, Filter Loss: 0.0009814886143431067, Seq Loss: 0.9998589158058167, Accuracy Rate: 54.93%, Ones Portion: 0.0003339757095091045,             Test_Total_Loss: 1.8943120241165161, Test_Clf_Loss: 1.8943010568618774, Test_Filter_Loss: 0.001094197272323072, TEST_Seq_Loss: 0.9998366236686707, Test_Accuracy_Rate: 54.84%, Test_Ones_Portion: 0.00037933397106826305\n",
      "Epoch 21, Total Loss: 1.8817471265792847, Clf Loss: 1.8817378282546997, Filter Loss: 0.0009207234834320843, Seq Loss: 0.9998663067817688, Accuracy Rate: 54.94%, Ones Portion: 0.00032233764068223536,             Test_Total_Loss: 1.8940932750701904, Test_Clf_Loss: 1.8940824270248413, Test_Filter_Loss: 0.0010500965872779489, TEST_Seq_Loss: 0.9998419284820557, Test_Accuracy_Rate: 54.99%, Test_Ones_Portion: 0.0003731015312951058\n",
      "Epoch 22, Total Loss: 1.8823542594909668, Clf Loss: 1.8823449611663818, Filter Loss: 0.0008931675110943615, Seq Loss: 0.9998660683631897, Accuracy Rate: 55.00%, Ones Portion: 0.00032324204221367836,             Test_Total_Loss: 1.8944073915481567, Test_Clf_Loss: 1.894397258758545, Test_Filter_Loss: 0.0010124754626303911, TEST_Seq_Loss: 0.9998423457145691, Test_Accuracy_Rate: 54.89%, Test_Ones_Portion: 0.0003735870704986155\n",
      "Epoch 23, Total Loss: 1.879046082496643, Clf Loss: 1.8790371417999268, Filter Loss: 0.000851168530061841, Seq Loss: 0.9998674988746643, Accuracy Rate: 55.04%, Ones Portion: 0.00032406096579506993,             Test_Total_Loss: 1.8986345529556274, Test_Clf_Loss: 1.8986244201660156, Test_Filter_Loss: 0.000986503902822733, TEST_Seq_Loss: 0.9998435974121094, Test_Accuracy_Rate: 54.79%, Test_Ones_Portion: 0.0003739846288226545\n",
      "Epoch 24, Total Loss: 1.8821290731430054, Clf Loss: 1.8821207284927368, Filter Loss: 0.0008355402969755232, Seq Loss: 0.9998645782470703, Accuracy Rate: 54.84%, Ones Portion: 0.0003308348823338747,             Test_Total_Loss: 1.8936680555343628, Test_Clf_Loss: 1.8936583995819092, Test_Filter_Loss: 0.000952312198933214, TEST_Seq_Loss: 0.9998440742492676, Test_Accuracy_Rate: 54.89%, Test_Ones_Portion: 0.00037260260432958603\n",
      "Epoch 25, Total Loss: 1.87771737575531, Clf Loss: 1.8777096271514893, Filter Loss: 0.0007879657205194235, Seq Loss: 0.9998698234558105, Accuracy Rate: 54.95%, Ones Portion: 0.0003215014876332134,             Test_Total_Loss: 1.8924535512924194, Test_Clf_Loss: 1.8924446105957031, Test_Filter_Loss: 0.0009160483605228364, TEST_Seq_Loss: 0.9998487830162048, Test_Accuracy_Rate: 54.79%, Test_Ones_Portion: 0.0003622456861194223\n",
      "Epoch 26, Total Loss: 1.8795024156570435, Clf Loss: 1.8794950246810913, Filter Loss: 0.0007661261479370296, Seq Loss: 0.9998722672462463, Accuracy Rate: 54.82%, Ones Portion: 0.00031908779055811465,             Test_Total_Loss: 1.8934173583984375, Test_Clf_Loss: 1.893408179283142, Test_Filter_Loss: 0.0008914269274100661, TEST_Seq_Loss: 0.9998507499694824, Test_Accuracy_Rate: 54.65%, Test_Ones_Portion: 0.0003625395183917135\n",
      "Epoch 27, Total Loss: 1.8828034400939941, Clf Loss: 1.8827961683273315, Filter Loss: 0.000740351970307529, Seq Loss: 0.9998751282691956, Accuracy Rate: 54.83%, Ones Portion: 0.00031803836463950574,             Test_Total_Loss: 1.894818902015686, Test_Clf_Loss: 1.8948101997375488, Test_Filter_Loss: 0.0008700676262378693, TEST_Seq_Loss: 0.9998536705970764, Test_Accuracy_Rate: 54.55%, Test_Ones_Portion: 0.00035878250491805375\n",
      "Epoch 28, Total Loss: 1.8854976892471313, Clf Loss: 1.8854913711547852, Filter Loss: 0.0006923014298081398, Seq Loss: 0.9998788833618164, Accuracy Rate: 54.76%, Ones Portion: 0.0003034612163901329,             Test_Total_Loss: 1.896802544593811, Test_Clf_Loss: 1.8967944383621216, Test_Filter_Loss: 0.0008087054593488574, TEST_Seq_Loss: 0.9998597502708435, Test_Accuracy_Rate: 54.74%, Test_Ones_Portion: 0.000337478268193081\n",
      "Epoch 29, Total Loss: 1.8850446939468384, Clf Loss: 1.8850388526916504, Filter Loss: 0.0006529077654704452, Seq Loss: 0.9998813271522522, Accuracy Rate: 54.72%, Ones Portion: 0.0002959584235213697,             Test_Total_Loss: 1.8924453258514404, Test_Clf_Loss: 1.89243745803833, Test_Filter_Loss: 0.0007959945360198617, TEST_Seq_Loss: 0.9998570680618286, Test_Accuracy_Rate: 54.84%, Test_Ones_Portion: 0.00034430480445735157\n",
      "Epoch 30, Total Loss: 1.880415916442871, Clf Loss: 1.8804092407226562, Filter Loss: 0.0006568150129169226, Seq Loss: 0.9998748898506165, Accuracy Rate: 54.85%, Ones Portion: 0.0003088751982431859,             Test_Total_Loss: 1.89231538772583, Test_Clf_Loss: 1.8923075199127197, Test_Filter_Loss: 0.0007764350157231092, TEST_Seq_Loss: 0.9998562932014465, Test_Accuracy_Rate: 54.84%, Test_Ones_Portion: 0.00034645062987692654\n",
      "Epoch 31, Total Loss: 1.8820810317993164, Clf Loss: 1.8820745944976807, Filter Loss: 0.0006335010402835906, Seq Loss: 0.9998769760131836, Accuracy Rate: 54.94%, Ones Portion: 0.00030673641595058143,             Test_Total_Loss: 1.894775152206421, Test_Clf_Loss: 1.8947675228118896, Test_Filter_Loss: 0.0007652613567188382, TEST_Seq_Loss: 0.9998553991317749, Test_Accuracy_Rate: 54.79%, Test_Ones_Portion: 0.0003493932308629155\n",
      "Epoch 32, Total Loss: 1.8797893524169922, Clf Loss: 1.8797829151153564, Filter Loss: 0.0006241552182473242, Seq Loss: 0.9998762607574463, Accuracy Rate: 55.00%, Ones Portion: 0.0003089061938226223,             Test_Total_Loss: 1.896003246307373, Test_Clf_Loss: 1.895995855331421, Test_Filter_Loss: 0.0007459056796506047, TEST_Seq_Loss: 0.9998576045036316, Test_Accuracy_Rate: 54.65%, Test_Ones_Portion: 0.00034501825575716794\n",
      "Epoch 33, Total Loss: 1.8813648223876953, Clf Loss: 1.8813585042953491, Filter Loss: 0.0006144108483567834, Seq Loss: 0.9998791217803955, Accuracy Rate: 54.84%, Ones Portion: 0.00030490756034851074,             Test_Total_Loss: 1.8930048942565918, Test_Clf_Loss: 1.8929977416992188, Test_Filter_Loss: 0.0007238095859065652, TEST_Seq_Loss: 0.9998578429222107, Test_Accuracy_Rate: 54.65%, Test_Ones_Portion: 0.00034587335539981723\n",
      "Epoch 34, Total Loss: 1.878629207611084, Clf Loss: 1.8786228895187378, Filter Loss: 0.0005871199537068605, Seq Loss: 0.9998806118965149, Accuracy Rate: 54.92%, Ones Portion: 0.00030098389834165573,             Test_Total_Loss: 1.8951256275177002, Test_Clf_Loss: 1.8951184749603271, Test_Filter_Loss: 0.0007113629253581166, TEST_Seq_Loss: 0.9998584985733032, Test_Accuracy_Rate: 54.74%, Test_Ones_Portion: 0.0003422954468987882\n",
      "Epoch 35, Total Loss: 1.8773193359375, Clf Loss: 1.8773136138916016, Filter Loss: 0.0005841306992806494, Seq Loss: 0.9998791217803955, Accuracy Rate: 54.87%, Ones Portion: 0.0003050215309485793,             Test_Total_Loss: 1.894732117652893, Test_Clf_Loss: 1.8947253227233887, Test_Filter_Loss: 0.0007063139928504825, TEST_Seq_Loss: 0.9998556971549988, Test_Accuracy_Rate: 54.65%, Test_Ones_Portion: 0.00035048980498686433\n",
      "Epoch 36, Total Loss: 1.875927448272705, Clf Loss: 1.875921607017517, Filter Loss: 0.0005749717820435762, Seq Loss: 0.9998780488967896, Accuracy Rate: 54.88%, Ones Portion: 0.0003105580690316856,             Test_Total_Loss: 1.89242422580719, Test_Clf_Loss: 1.8924171924591064, Test_Filter_Loss: 0.0006916174315847456, TEST_Seq_Loss: 0.9998593926429749, Test_Accuracy_Rate: 54.65%, Test_Ones_Portion: 0.0003434605023358017\n",
      "Epoch 37, Total Loss: 1.874644160270691, Clf Loss: 1.8746386766433716, Filter Loss: 0.0005571126821450889, Seq Loss: 0.999880850315094, Accuracy Rate: 54.85%, Ones Portion: 0.0003041917225345969,             Test_Total_Loss: 1.8917557001113892, Test_Clf_Loss: 1.8917492628097534, Test_Filter_Loss: 0.000671926187351346, TEST_Seq_Loss: 0.9998618960380554, Test_Accuracy_Rate: 54.84%, Test_Ones_Portion: 0.00033708568662405014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Total Loss: 1.8755055665969849, Clf Loss: 1.8755004405975342, Filter Loss: 0.0005298747564665973, Seq Loss: 0.999884843826294, Accuracy Rate: 54.90%, Ones Portion: 0.00029422398074530065,             Test_Total_Loss: 1.8943923711776733, Test_Clf_Loss: 1.894385814666748, Test_Filter_Loss: 0.0006582250352948904, TEST_Seq_Loss: 0.9998626708984375, Test_Accuracy_Rate: 54.74%, Test_Ones_Portion: 0.0003375898231752217\n",
      "Epoch 39, Total Loss: 1.8736717700958252, Clf Loss: 1.873666524887085, Filter Loss: 0.0005310473497956991, Seq Loss: 0.9998833537101746, Accuracy Rate: 54.83%, Ones Portion: 0.00030386840808205307,             Test_Total_Loss: 1.8913118839263916, Test_Clf_Loss: 1.8913053274154663, Test_Filter_Loss: 0.0006486288039013743, TEST_Seq_Loss: 0.9998601078987122, Test_Accuracy_Rate: 54.70%, Test_Ones_Portion: 0.0003498829319141805\n",
      "Epoch 40, Total Loss: 1.8658586740493774, Clf Loss: 1.8658534288406372, Filter Loss: 0.0005186658818274736, Seq Loss: 0.9998825192451477, Accuracy Rate: 54.81%, Ones Portion: 0.00031220880919136107,             Test_Total_Loss: 1.8603994846343994, Test_Clf_Loss: 1.8603934049606323, Test_Filter_Loss: 0.0006364091532304883, TEST_Seq_Loss: 0.9998587369918823, Test_Accuracy_Rate: 54.70%, Test_Ones_Portion: 0.00035833835136145353\n",
      "Epoch 41, Total Loss: 1.8588786125183105, Clf Loss: 1.8588732481002808, Filter Loss: 0.0005133204394951463, Seq Loss: 0.9998786449432373, Accuracy Rate: 54.77%, Ones Portion: 0.00033371392055414617,             Test_Total_Loss: 1.8983008861541748, Test_Clf_Loss: 1.8982943296432495, Test_Filter_Loss: 0.0006500041927210987, TEST_Seq_Loss: 0.9998484253883362, Test_Accuracy_Rate: 54.50%, Test_Ones_Portion: 0.00040341270505450666\n",
      "Epoch 42, Total Loss: 1.83367919921875, Clf Loss: 1.8336741924285889, Filter Loss: 0.0004980620578862727, Seq Loss: 0.9998809695243835, Accuracy Rate: 54.66%, Ones Portion: 0.00032873902819119394,             Test_Total_Loss: 1.8208329677581787, Test_Clf_Loss: 1.8208266496658325, Test_Filter_Loss: 0.000618583697360009, TEST_Seq_Loss: 0.9998588562011719, Test_Accuracy_Rate: 54.79%, Test_Ones_Portion: 0.000363313767593354\n",
      "Epoch 43, Total Loss: 1.8095217943191528, Clf Loss: 1.8095167875289917, Filter Loss: 0.0004950378788635135, Seq Loss: 0.9998776912689209, Accuracy Rate: 55.64%, Ones Portion: 0.00033375597558915615,             Test_Total_Loss: 1.8235081434249878, Test_Clf_Loss: 1.8235020637512207, Test_Filter_Loss: 0.0006181867211125791, TEST_Seq_Loss: 0.9998545050621033, Test_Accuracy_Rate: 55.32%, Test_Ones_Portion: 0.0003788663598243147\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 44, Total Loss: 1.79763925075531, Clf Loss: 1.797634243965149, Filter Loss: 0.0004822640330530703, Seq Loss: 0.9998804926872253, Accuracy Rate: 55.98%, Ones Portion: 0.0003300479438621551,             Test_Total_Loss: 1.8231384754180908, Test_Clf_Loss: 1.8231322765350342, Test_Filter_Loss: 0.0006096201832406223, TEST_Seq_Loss: 0.9998552799224854, Test_Accuracy_Rate: 55.18%, Test_Ones_Portion: 0.00037832805537618697\n",
      "Epoch 45, Total Loss: 1.7986894845962524, Clf Loss: 1.7986841201782227, Filter Loss: 0.0004790152015630156, Seq Loss: 0.9998807311058044, Accuracy Rate: 55.88%, Ones Portion: 0.00032997902599163353,             Test_Total_Loss: 1.8286195993423462, Test_Clf_Loss: 1.8286139965057373, Test_Filter_Loss: 0.0005916218506172299, TEST_Seq_Loss: 0.9998630285263062, Test_Accuracy_Rate: 55.13%, Test_Ones_Portion: 0.0003675866755656898\n",
      "Epoch 46, Total Loss: 1.8001586198806763, Clf Loss: 1.8001536130905151, Filter Loss: 0.00046580316848121583, Seq Loss: 0.999886155128479, Accuracy Rate: 55.97%, Ones Portion: 0.00031861758907325566,             Test_Total_Loss: 1.8106462955474854, Test_Clf_Loss: 1.8106403350830078, Test_Filter_Loss: 0.0005721529014408588, TEST_Seq_Loss: 0.9998676180839539, Test_Accuracy_Rate: 54.36%, Test_Ones_Portion: 0.00034988834522664547\n",
      "Epoch 47, Total Loss: 1.793282389640808, Clf Loss: 1.7932783365249634, Filter Loss: 0.00044414555304683745, Seq Loss: 0.9998924732208252, Accuracy Rate: 55.91%, Ones Portion: 0.0003020676667802036,             Test_Total_Loss: 1.814941644668579, Test_Clf_Loss: 1.8149361610412598, Test_Filter_Loss: 0.0005542456055991352, TEST_Seq_Loss: 0.9998728632926941, Test_Accuracy_Rate: 56.29%, Test_Ones_Portion: 0.0003432703379075974\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 48, Total Loss: 1.7930588722229004, Clf Loss: 1.7930546998977661, Filter Loss: 0.0004309459764044732, Seq Loss: 0.9998952746391296, Accuracy Rate: 56.26%, Ones Portion: 0.0002992920344695449,             Test_Total_Loss: 1.8198754787445068, Test_Clf_Loss: 1.819870114326477, Test_Filter_Loss: 0.0005352116422727704, TEST_Seq_Loss: 0.9998807311058044, Test_Accuracy_Rate: 55.08%, Test_Ones_Portion: 0.00032157482928596437\n",
      "Epoch 49, Total Loss: 1.7903789281845093, Clf Loss: 1.790374517440796, Filter Loss: 0.00042260714690200984, Seq Loss: 0.9998977780342102, Accuracy Rate: 55.96%, Ones Portion: 0.0002902580890804529,             Test_Total_Loss: 1.8218008279800415, Test_Clf_Loss: 1.8217953443527222, Test_Filter_Loss: 0.0005313545116223395, TEST_Seq_Loss: 0.9998764991760254, Test_Accuracy_Rate: 55.47%, Test_Ones_Portion: 0.00033847943996079266\n",
      "Epoch 50, Total Loss: 1.7907978296279907, Clf Loss: 1.7907936573028564, Filter Loss: 0.00041319680167362094, Seq Loss: 0.9998967051506042, Accuracy Rate: 55.76%, Ones Portion: 0.00029442552477121353,             Test_Total_Loss: 1.8173012733459473, Test_Clf_Loss: 1.817296028137207, Test_Filter_Loss: 0.0005215769051574171, TEST_Seq_Loss: 0.999878466129303, Test_Accuracy_Rate: 55.37%, Test_Ones_Portion: 0.0003336989611852914\n",
      "Epoch 51, Total Loss: 1.7913264036178589, Clf Loss: 1.7913225889205933, Filter Loss: 0.0004077285120729357, Seq Loss: 0.9998968243598938, Accuracy Rate: 56.10%, Ones Portion: 0.0002967002510558814,             Test_Total_Loss: 1.811292052268982, Test_Clf_Loss: 1.8112869262695312, Test_Filter_Loss: 0.0005173311219550669, TEST_Seq_Loss: 0.9998774528503418, Test_Accuracy_Rate: 55.42%, Test_Ones_Portion: 0.0003379911067895591\n",
      "Epoch 52, Total Loss: 1.7969870567321777, Clf Loss: 1.7969831228256226, Filter Loss: 0.0004159436793997884, Seq Loss: 0.9998929500579834, Accuracy Rate: 55.98%, Ones Portion: 0.000307629321468994,             Test_Total_Loss: 1.8056426048278809, Test_Clf_Loss: 1.8056374788284302, Test_Filter_Loss: 0.0005113089573569596, TEST_Seq_Loss: 0.9998743534088135, Test_Accuracy_Rate: 55.08%, Test_Ones_Portion: 0.00034212172613479197\n",
      "Epoch 53, Total Loss: 1.790302038192749, Clf Loss: 1.7902976274490356, Filter Loss: 0.0004020719788968563, Seq Loss: 0.9998947381973267, Accuracy Rate: 55.91%, Ones Portion: 0.00030128902290016413,             Test_Total_Loss: 1.8180766105651855, Test_Clf_Loss: 1.8180716037750244, Test_Filter_Loss: 0.0005054644425399601, TEST_Seq_Loss: 0.9998732209205627, Test_Accuracy_Rate: 55.66%, Test_Ones_Portion: 0.00034881732426583767\n",
      "Epoch 54, Total Loss: 1.8162789344787598, Clf Loss: 1.8162752389907837, Filter Loss: 0.00040007586358115077, Seq Loss: 0.9998903870582581, Accuracy Rate: 56.14%, Ones Portion: 0.00031737788231112063,             Test_Total_Loss: 1.8099384307861328, Test_Clf_Loss: 1.8099335432052612, Test_Filter_Loss: 0.0004923156811855733, TEST_Seq_Loss: 0.999875545501709, Test_Accuracy_Rate: 56.39%, Test_Ones_Portion: 0.00034167151898145676\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 55, Total Loss: 1.798831582069397, Clf Loss: 1.7988280057907104, Filter Loss: 0.0003941433096770197, Seq Loss: 0.9998916387557983, Accuracy Rate: 56.22%, Ones Portion: 0.00031268762541003525,             Test_Total_Loss: 1.8114771842956543, Test_Clf_Loss: 1.8114720582962036, Test_Filter_Loss: 0.0004951857263222337, TEST_Seq_Loss: 0.9998699426651001, Test_Accuracy_Rate: 55.37%, Test_Ones_Portion: 0.0003535498399287462\n",
      "Epoch 56, Total Loss: 1.8109445571899414, Clf Loss: 1.8109402656555176, Filter Loss: 0.0003932033432647586, Seq Loss: 0.9998894333839417, Accuracy Rate: 56.02%, Ones Portion: 0.0003214097523596138,             Test_Total_Loss: 1.8465814590454102, Test_Clf_Loss: 1.846576452255249, Test_Filter_Loss: 0.00048325638636015356, TEST_Seq_Loss: 0.9998729825019836, Test_Accuracy_Rate: 56.20%, Test_Ones_Portion: 0.00035485741682350636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, Total Loss: 1.807052493095398, Clf Loss: 1.8070485591888428, Filter Loss: 0.0003847687621600926, Seq Loss: 0.9998884797096252, Accuracy Rate: 56.20%, Ones Portion: 0.0003226779808755964,             Test_Total_Loss: 1.807955265045166, Test_Clf_Loss: 1.8079506158828735, Test_Filter_Loss: 0.00047994163469411433, TEST_Seq_Loss: 0.9998703002929688, Test_Accuracy_Rate: 55.37%, Test_Ones_Portion: 0.0003530959947966039\n",
      "Epoch 58, Total Loss: 1.7878166437149048, Clf Loss: 1.7878130674362183, Filter Loss: 0.00038386942469514906, Seq Loss: 0.9998912811279297, Accuracy Rate: 56.21%, Ones Portion: 0.00031277735251933336,             Test_Total_Loss: 1.8214620351791382, Test_Clf_Loss: 1.8214573860168457, Test_Filter_Loss: 0.00047186537995003164, TEST_Seq_Loss: 0.9998756647109985, Test_Accuracy_Rate: 55.81%, Test_Ones_Portion: 0.0003454731195233762\n",
      "Epoch 59, Total Loss: 1.7938635349273682, Clf Loss: 1.793859839439392, Filter Loss: 0.00037331003113649786, Seq Loss: 0.9998937845230103, Accuracy Rate: 56.53%, Ones Portion: 0.00031053333077579737,             Test_Total_Loss: 1.8345576524734497, Test_Clf_Loss: 1.8345531225204468, Test_Filter_Loss: 0.0004702457517851144, TEST_Seq_Loss: 0.9998695254325867, Test_Accuracy_Rate: 55.32%, Test_Ones_Portion: 0.0003589487459976226\n",
      "Epoch 60, Total Loss: 1.818607211112976, Clf Loss: 1.8186030387878418, Filter Loss: 0.0003734336351044476, Seq Loss: 0.9998921155929565, Accuracy Rate: 56.23%, Ones Portion: 0.00031669792952015996,             Test_Total_Loss: 1.8419052362442017, Test_Clf_Loss: 1.8419008255004883, Test_Filter_Loss: 0.00045385584235191345, TEST_Seq_Loss: 0.9998779296875, Test_Accuracy_Rate: 56.49%, Test_Ones_Portion: 0.0003392765356693417\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 61, Total Loss: 1.8009454011917114, Clf Loss: 1.800942301750183, Filter Loss: 0.00036584900226444006, Seq Loss: 0.9998957514762878, Accuracy Rate: 56.39%, Ones Portion: 0.0003098089073318988,             Test_Total_Loss: 1.8302240371704102, Test_Clf_Loss: 1.8302196264266968, Test_Filter_Loss: 0.0004618186503648758, TEST_Seq_Loss: 0.9998714327812195, Test_Accuracy_Rate: 56.58%, Test_Ones_Portion: 0.00035819175536744297\n",
      "===MODEL WEIGHTS SAVED=== ./model/2019110602/model1\n",
      "Epoch 62, Total Loss: 1.803728699684143, Clf Loss: 1.8037248849868774, Filter Loss: 0.00036461171112023294, Seq Loss: 0.999894380569458, Accuracy Rate: 56.44%, Ones Portion: 0.0003145766968373209,             Test_Total_Loss: 1.826253890991211, Test_Clf_Loss: 1.826249122619629, Test_Filter_Loss: 0.0004598853993229568, TEST_Seq_Loss: 0.999868631362915, Test_Accuracy_Rate: 55.28%, Test_Ones_Portion: 0.00036336734774522483\n",
      "Epoch 63, Total Loss: 1.801702618598938, Clf Loss: 1.8016985654830933, Filter Loss: 0.000368224224075675, Seq Loss: 0.9998887181282043, Accuracy Rate: 56.33%, Ones Portion: 0.00032623595325276256,             Test_Total_Loss: 1.8385121822357178, Test_Clf_Loss: 1.8385076522827148, Test_Filter_Loss: 0.0004479365888983011, TEST_Seq_Loss: 0.9998724460601807, Test_Accuracy_Rate: 56.29%, Test_Ones_Portion: 0.00035896763438358903\n",
      "Epoch 64, Total Loss: 1.8199952840805054, Clf Loss: 1.819991946220398, Filter Loss: 0.00035460470826365054, Seq Loss: 0.9998956322669983, Accuracy Rate: 56.20%, Ones Portion: 0.0003144364454783499,             Test_Total_Loss: 1.8526092767715454, Test_Clf_Loss: 1.852604866027832, Test_Filter_Loss: 0.00043819384882226586, TEST_Seq_Loss: 0.9998734593391418, Test_Accuracy_Rate: 55.81%, Test_Ones_Portion: 0.00035861178184859455\n",
      "Epoch 65, Total Loss: 1.8371469974517822, Clf Loss: 1.8371436595916748, Filter Loss: 0.00035611895145848393, Seq Loss: 0.9998935461044312, Accuracy Rate: 55.73%, Ones Portion: 0.00032580929109826684,             Test_Total_Loss: 1.8556642532348633, Test_Clf_Loss: 1.8556596040725708, Test_Filter_Loss: 0.0004399906611070037, TEST_Seq_Loss: 0.9998705387115479, Test_Accuracy_Rate: 55.52%, Test_Ones_Portion: 0.00036614714190363884\n",
      "Epoch 66, Total Loss: 1.8345959186553955, Clf Loss: 1.8345931768417358, Filter Loss: 0.00035056000342592597, Seq Loss: 0.9998941421508789, Accuracy Rate: 55.94%, Ones Portion: 0.00032358369207940996,             Test_Total_Loss: 1.8585197925567627, Test_Clf_Loss: 1.8585156202316284, Test_Filter_Loss: 0.0004422753117978573, TEST_Seq_Loss: 0.9998670220375061, Test_Accuracy_Rate: 55.42%, Test_Ones_Portion: 0.00037664241972379386\n",
      "Epoch 67, Total Loss: 1.8381372690200806, Clf Loss: 1.8381339311599731, Filter Loss: 0.0003523049526847899, Seq Loss: 0.999891996383667, Accuracy Rate: 55.85%, Ones Portion: 0.0003301421529613435,             Test_Total_Loss: 1.855428695678711, Test_Clf_Loss: 1.8554242849349976, Test_Filter_Loss: 0.00043428814387880266, TEST_Seq_Loss: 0.999868631362915, Test_Accuracy_Rate: 55.57%, Test_Ones_Portion: 0.00037578196497634053\n",
      "Epoch 68, Total Loss: 1.8351260423660278, Clf Loss: 1.8351229429244995, Filter Loss: 0.0003501833416521549, Seq Loss: 0.9998912215232849, Accuracy Rate: 56.03%, Ones Portion: 0.00033162921317853034,             Test_Total_Loss: 1.866513967514038, Test_Clf_Loss: 1.8665094375610352, Test_Filter_Loss: 0.00042873548227362335, TEST_Seq_Loss: 0.9998709559440613, Test_Accuracy_Rate: 55.76%, Test_Ones_Portion: 0.0003731531323865056\n",
      "Epoch 69, Total Loss: 1.8368899822235107, Clf Loss: 1.8368865251541138, Filter Loss: 0.0003349797916598618, Seq Loss: 0.9998988509178162, Accuracy Rate: 55.92%, Ones Portion: 0.0003131888515781611,             Test_Total_Loss: 1.8634634017944336, Test_Clf_Loss: 1.8634591102600098, Test_Filter_Loss: 0.0004150870081502944, TEST_Seq_Loss: 0.9998769164085388, Test_Accuracy_Rate: 55.42%, Test_Ones_Portion: 0.0003575328446459025\n",
      "Epoch 70, Total Loss: 1.832170844078064, Clf Loss: 1.8321675062179565, Filter Loss: 0.0003373010258655995, Seq Loss: 0.9998956322669983, Accuracy Rate: 55.96%, Ones Portion: 0.00032542506232857704,             Test_Total_Loss: 1.8647795915603638, Test_Clf_Loss: 1.8647756576538086, Test_Filter_Loss: 0.0004188816819805652, TEST_Seq_Loss: 0.999870777130127, Test_Accuracy_Rate: 55.52%, Test_Ones_Portion: 0.00037473352858796716\n",
      "Epoch 71, Total Loss: 1.8328468799591064, Clf Loss: 1.8328430652618408, Filter Loss: 0.0003414857492316514, Seq Loss: 0.9998918771743774, Accuracy Rate: 55.93%, Ones Portion: 0.00033289645216427743,             Test_Total_Loss: 1.8685107231140137, Test_Clf_Loss: 1.868506669998169, Test_Filter_Loss: 0.0004007068637292832, TEST_Seq_Loss: 0.9998793005943298, Test_Accuracy_Rate: 55.71%, Test_Ones_Portion: 0.0003495666605886072\n",
      "Epoch 72, Total Loss: 1.8336315155029297, Clf Loss: 1.8336281776428223, Filter Loss: 0.00032638394623063505, Seq Loss: 0.9999001026153564, Accuracy Rate: 55.76%, Ones Portion: 0.00031244431738741696,             Test_Total_Loss: 1.8683855533599854, Test_Clf_Loss: 1.868381381034851, Test_Filter_Loss: 0.0003975841391365975, TEST_Seq_Loss: 0.9998779296875, Test_Accuracy_Rate: 55.52%, Test_Ones_Portion: 0.00035382318310439587\n",
      "Epoch 73, Total Loss: 1.8343323469161987, Clf Loss: 1.8343291282653809, Filter Loss: 0.00032666363404132426, Seq Loss: 0.999896228313446, Accuracy Rate: 55.87%, Ones Portion: 0.0003234655305277556,             Test_Total_Loss: 1.866945743560791, Test_Clf_Loss: 1.8669419288635254, Test_Filter_Loss: 0.00040127907413989305, TEST_Seq_Loss: 0.9998741149902344, Test_Accuracy_Rate: 55.61%, Test_Ones_Portion: 0.0003695412306115031\n",
      "Epoch 74, Total Loss: 1.8309738636016846, Clf Loss: 1.8309701681137085, Filter Loss: 0.0003281777899246663, Seq Loss: 0.9998968243598938, Accuracy Rate: 55.79%, Ones Portion: 0.0003246301203034818,             Test_Total_Loss: 1.8676891326904297, Test_Clf_Loss: 1.8676851987838745, Test_Filter_Loss: 0.00039072291110642254, TEST_Seq_Loss: 0.999878466129303, Test_Accuracy_Rate: 55.57%, Test_Ones_Portion: 0.00035335440770722926\n",
      "Epoch 75, Total Loss: 1.8348928689956665, Clf Loss: 1.8348898887634277, Filter Loss: 0.0003181630454491824, Seq Loss: 0.999901294708252, Accuracy Rate: 55.81%, Ones Portion: 0.0003110210527665913,             Test_Total_Loss: 1.8745585680007935, Test_Clf_Loss: 1.8745547533035278, Test_Filter_Loss: 0.00038070048321969807, TEST_Seq_Loss: 0.9998817443847656, Test_Accuracy_Rate: 55.23%, Test_Ones_Portion: 0.0003416398831177503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, Total Loss: 1.8279365301132202, Clf Loss: 1.8279335498809814, Filter Loss: 0.0003070587117690593, Seq Loss: 0.9999092817306519, Accuracy Rate: 55.90%, Ones Portion: 0.00028587583801709116,             Test_Total_Loss: 1.867802619934082, Test_Clf_Loss: 1.867799162864685, Test_Filter_Loss: 0.0003772728960029781, TEST_Seq_Loss: 0.999880313873291, Test_Accuracy_Rate: 55.37%, Test_Ones_Portion: 0.0003436746192164719\n",
      "Epoch 77, Total Loss: 1.8247759342193604, Clf Loss: 1.824772596359253, Filter Loss: 0.00031101348577067256, Seq Loss: 0.9999048113822937, Accuracy Rate: 55.90%, Ones Portion: 0.0002996392722707242,             Test_Total_Loss: 1.86435866355896, Test_Clf_Loss: 1.8643549680709839, Test_Filter_Loss: 0.00037710671313107014, TEST_Seq_Loss: 0.9998819828033447, Test_Accuracy_Rate: 55.76%, Test_Ones_Portion: 0.0003403415612410754\n",
      "Epoch 78, Total Loss: 1.8295243978500366, Clf Loss: 1.8295210599899292, Filter Loss: 0.0003052152751479298, Seq Loss: 0.9999050498008728, Accuracy Rate: 55.82%, Ones Portion: 0.0002980848657898605,             Test_Total_Loss: 1.865116834640503, Test_Clf_Loss: 1.8651130199432373, Test_Filter_Loss: 0.00037461513420566916, TEST_Seq_Loss: 0.9998810887336731, Test_Accuracy_Rate: 55.08%, Test_Ones_Portion: 0.00034508996759541333\n",
      "Epoch 79, Total Loss: 1.8288049697875977, Clf Loss: 1.8288023471832275, Filter Loss: 0.00030078162671998143, Seq Loss: 0.9999062418937683, Accuracy Rate: 55.91%, Ones Portion: 0.00029652813100256026,             Test_Total_Loss: 1.8671456575393677, Test_Clf_Loss: 1.8671419620513916, Test_Filter_Loss: 0.0003685703850351274, TEST_Seq_Loss: 0.9998853206634521, Test_Accuracy_Rate: 55.13%, Test_Ones_Portion: 0.0003366703458596021\n",
      "Epoch 80, Total Loss: 1.8302032947540283, Clf Loss: 1.8302005529403687, Filter Loss: 0.00028654286870732903, Seq Loss: 0.9999108910560608, Accuracy Rate: 55.88%, Ones Portion: 0.0002828585566021502,             Test_Total_Loss: 1.8724629878997803, Test_Clf_Loss: 1.8724592924118042, Test_Filter_Loss: 0.0003563473292160779, TEST_Seq_Loss: 0.9998881220817566, Test_Accuracy_Rate: 55.28%, Test_Ones_Portion: 0.000329934642650187\n",
      "Epoch 81, Total Loss: 1.8247417211532593, Clf Loss: 1.8247390985488892, Filter Loss: 0.0002668840461410582, Seq Loss: 0.9999227523803711, Accuracy Rate: 55.86%, Ones Portion: 0.00025068680406548083,             Test_Total_Loss: 1.8652421236038208, Test_Clf_Loss: 1.8652387857437134, Test_Filter_Loss: 0.0003340061812195927, TEST_Seq_Loss: 0.9999014735221863, Test_Accuracy_Rate: 55.42%, Test_Ones_Portion: 0.0002943690924439579\n",
      "Epoch 82, Total Loss: 1.8282475471496582, Clf Loss: 1.8282454013824463, Filter Loss: 0.0002619312144815922, Seq Loss: 0.9999233484268188, Accuracy Rate: 55.96%, Ones Portion: 0.0002480671973899007,             Test_Total_Loss: 1.8682143688201904, Test_Clf_Loss: 1.8682111501693726, Test_Filter_Loss: 0.0003214083262719214, TEST_Seq_Loss: 0.9999102354049683, Test_Accuracy_Rate: 55.47%, Test_Ones_Portion: 0.0002749210689216852\n",
      "Epoch 83, Total Loss: 1.8236563205718994, Clf Loss: 1.823653221130371, Filter Loss: 0.0002696762094274163, Seq Loss: 0.9999184012413025, Accuracy Rate: 55.97%, Ones Portion: 0.0002630214730743319,             Test_Total_Loss: 1.8690024614334106, Test_Clf_Loss: 1.8689992427825928, Test_Filter_Loss: 0.0003185136301908642, TEST_Seq_Loss: 0.999911367893219, Test_Accuracy_Rate: 55.23%, Test_Ones_Portion: 0.00027099656290374696\n",
      "Epoch 84, Total Loss: 1.8219786882400513, Clf Loss: 1.8219763040542603, Filter Loss: 0.00023974923533387482, Seq Loss: 0.9999371767044067, Accuracy Rate: 56.04%, Ones Portion: 0.0002136739349225536,             Test_Total_Loss: 1.8677802085876465, Test_Clf_Loss: 1.8677772283554077, Test_Filter_Loss: 0.0003048062790185213, TEST_Seq_Loss: 0.9999187588691711, Test_Accuracy_Rate: 55.37%, Test_Ones_Portion: 0.00025376660050824285\n",
      "Epoch 85, Total Loss: 1.8189945220947266, Clf Loss: 1.8189916610717773, Filter Loss: 0.00024189960095100105, Seq Loss: 0.9999327659606934, Accuracy Rate: 56.02%, Ones Portion: 0.00022123637609183788,             Test_Total_Loss: 1.8749351501464844, Test_Clf_Loss: 1.874932050704956, Test_Filter_Loss: 0.0002994713431689888, TEST_Seq_Loss: 0.9999253749847412, Test_Accuracy_Rate: 55.52%, Test_Ones_Portion: 0.0002433961199130863\n",
      "Epoch 86, Total Loss: 1.8177529573440552, Clf Loss: 1.8177508115768433, Filter Loss: 0.00023310795950237662, Seq Loss: 0.9999383687973022, Accuracy Rate: 56.03%, Ones Portion: 0.000209572070161812,             Test_Total_Loss: 1.8728915452957153, Test_Clf_Loss: 1.8728886842727661, Test_Filter_Loss: 0.0002976638206746429, TEST_Seq_Loss: 0.9999232292175293, Test_Accuracy_Rate: 55.52%, Test_Ones_Portion: 0.0002441561664454639\n",
      "Epoch 87, Total Loss: 1.8219629526138306, Clf Loss: 1.8219603300094604, Filter Loss: 0.0002311357093276456, Seq Loss: 0.9999382495880127, Accuracy Rate: 56.02%, Ones Portion: 0.00020898081129416823,             Test_Total_Loss: 1.8737388849258423, Test_Clf_Loss: 1.873736023902893, Test_Filter_Loss: 0.00029373454162850976, TEST_Seq_Loss: 0.9999216794967651, Test_Accuracy_Rate: 55.52%, Test_Ones_Portion: 0.00024458803818561137\n",
      "Epoch 88, Total Loss: 1.8196299076080322, Clf Loss: 1.8196276426315308, Filter Loss: 0.00022743470617569983, Seq Loss: 0.9999401569366455, Accuracy Rate: 56.11%, Ones Portion: 0.00020567096362356097,             Test_Total_Loss: 1.8723994493484497, Test_Clf_Loss: 1.8723965883255005, Test_Filter_Loss: 0.00029330136021599174, TEST_Seq_Loss: 0.9999291896820068, Test_Accuracy_Rate: 55.37%, Test_Ones_Portion: 0.00023461754608433694\n",
      "Epoch 89, Total Loss: 1.8188565969467163, Clf Loss: 1.8188544511795044, Filter Loss: 0.00022637067013420165, Seq Loss: 0.99994295835495, Accuracy Rate: 56.09%, Ones Portion: 0.0002019333915086463,             Test_Total_Loss: 1.8688929080963135, Test_Clf_Loss: 1.8688898086547852, Test_Filter_Loss: 0.0002870230528060347, TEST_Seq_Loss: 0.9999286532402039, Test_Accuracy_Rate: 55.57%, Test_Ones_Portion: 0.0002356943441554904\n",
      "Epoch 90, Total Loss: 1.8191492557525635, Clf Loss: 1.819146990776062, Filter Loss: 0.00021811046462971717, Seq Loss: 0.999945878982544, Accuracy Rate: 56.03%, Ones Portion: 0.00019679426623042673,             Test_Total_Loss: 1.8671773672103882, Test_Clf_Loss: 1.867174744606018, Test_Filter_Loss: 0.00028064424986951053, TEST_Seq_Loss: 0.9999322295188904, Test_Accuracy_Rate: 55.57%, Test_Ones_Portion: 0.00022865987557452172\n",
      "Epoch 91, Total Loss: 1.818156361579895, Clf Loss: 1.818153977394104, Filter Loss: 0.00020748423412442207, Seq Loss: 0.999951183795929, Accuracy Rate: 56.05%, Ones Portion: 0.000185954209882766,             Test_Total_Loss: 1.874532699584961, Test_Clf_Loss: 1.87453031539917, Test_Filter_Loss: 0.00026350212283432484, TEST_Seq_Loss: 0.999941885471344, Test_Accuracy_Rate: 55.86%, Test_Ones_Portion: 0.00020687670621555299\n",
      "Epoch 92, Total Loss: 1.8169949054718018, Clf Loss: 1.8169931173324585, Filter Loss: 0.00019030680414289236, Seq Loss: 0.9999589323997498, Accuracy Rate: 56.20%, Ones Portion: 0.0001677481923252344,             Test_Total_Loss: 1.8933964967727661, Test_Clf_Loss: 1.8933942317962646, Test_Filter_Loss: 0.00024266463879030198, TEST_Seq_Loss: 0.9999509453773499, Test_Accuracy_Rate: 54.84%, Test_Ones_Portion: 0.0001887558464659378\n",
      "Epoch 93, Total Loss: 1.8472779989242554, Clf Loss: 1.8472760915756226, Filter Loss: 0.00017397041665390134, Seq Loss: 0.999965488910675, Accuracy Rate: 55.16%, Ones Portion: 0.00015230404096655548,             Test_Total_Loss: 1.8939334154129028, Test_Clf_Loss: 1.893931269645691, Test_Filter_Loss: 0.0002208756050094962, TEST_Seq_Loss: 0.9999570250511169, Test_Accuracy_Rate: 54.99%, Test_Ones_Portion: 0.0001767715293681249\n",
      "Epoch 94, Total Loss: 1.848081111907959, Clf Loss: 1.8480795621871948, Filter Loss: 0.00015564558270853013, Seq Loss: 0.9999679327011108, Accuracy Rate: 55.21%, Ones Portion: 0.0001447770046070218,             Test_Total_Loss: 1.895850419998169, Test_Clf_Loss: 1.8958486318588257, Test_Filter_Loss: 0.00020257422875147313, TEST_Seq_Loss: 0.9999591708183289, Test_Accuracy_Rate: 54.99%, Test_Ones_Portion: 0.00016972543380688876\n",
      "Epoch 95, Total Loss: 1.8474626541137695, Clf Loss: 1.847461462020874, Filter Loss: 0.00014169130008667707, Seq Loss: 0.9999696016311646, Accuracy Rate: 55.16%, Ones Portion: 0.00014023613766767085,             Test_Total_Loss: 1.903754472732544, Test_Clf_Loss: 1.9037524461746216, Test_Filter_Loss: 0.000183145995833911, TEST_Seq_Loss: 0.9999626278877258, Test_Accuracy_Rate: 54.79%, Test_Ones_Portion: 0.00015856963000260293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, Total Loss: 1.853301763534546, Clf Loss: 1.8533008098602295, Filter Loss: 0.0001303756725974381, Seq Loss: 0.9999711513519287, Accuracy Rate: 55.13%, Ones Portion: 0.00013469974510371685,             Test_Total_Loss: 1.9002306461334229, Test_Clf_Loss: 1.9002288579940796, Test_Filter_Loss: 0.0001785185741027817, TEST_Seq_Loss: 0.9999620914459229, Test_Accuracy_Rate: 54.79%, Test_Ones_Portion: 0.00016067660180851817\n",
      "Epoch 97, Total Loss: 1.8513916730880737, Clf Loss: 1.8513904809951782, Filter Loss: 0.00012628959666471928, Seq Loss: 0.9999721646308899, Accuracy Rate: 55.10%, Ones Portion: 0.0001320384326390922,             Test_Total_Loss: 1.9077727794647217, Test_Clf_Loss: 1.9077709913253784, Test_Filter_Loss: 0.0001682578877080232, TEST_Seq_Loss: 0.9999634027481079, Test_Accuracy_Rate: 54.79%, Test_Ones_Portion: 0.00015634906594641507\n",
      "Epoch 98, Total Loss: 1.8640010356903076, Clf Loss: 1.8639997243881226, Filter Loss: 0.00011729118705261499, Seq Loss: 0.9999761581420898, Accuracy Rate: 54.76%, Ones Portion: 0.00012085222988389432,             Test_Total_Loss: 1.9748936891555786, Test_Clf_Loss: 1.9748921394348145, Test_Filter_Loss: 0.0001581575779709965, TEST_Seq_Loss: 0.9999828338623047, Test_Accuracy_Rate: 52.47%, Test_Ones_Portion: 0.0001107483622035943\n",
      "Epoch 99, Total Loss: 1.9522815942764282, Clf Loss: 1.9522804021835327, Filter Loss: 0.00010778733849292621, Seq Loss: 0.999988853931427, Accuracy Rate: 51.03%, Ones Portion: 8.533865184290335e-05,             Test_Total_Loss: 2.043138027191162, Test_Clf_Loss: 2.0431365966796875, Test_Filter_Loss: 0.00013912750000599772, TEST_Seq_Loss: 0.9999861121177673, Test_Accuracy_Rate: 48.60%, Test_Ones_Portion: 9.309863526141271e-05\n",
      "Epoch 100, Total Loss: 2.02074933052063, Clf Loss: 2.0207483768463135, Filter Loss: 9.077085996977985e-05, Seq Loss: 0.999991774559021, Accuracy Rate: 47.59%, Ones Portion: 6.792436761315912e-05,             Test_Total_Loss: 2.070507764816284, Test_Clf_Loss: 2.0705065727233887, Test_Filter_Loss: 0.00011663874465739354, TEST_Seq_Loss: 0.9999889135360718, Test_Accuracy_Rate: 47.82%, Test_Ones_Portion: 8.098854596028104e-05\n",
      "Epoch 101, Total Loss: 2.0291390419006348, Clf Loss: 2.0291385650634766, Filter Loss: 7.493617886211723e-05, Seq Loss: 0.9999920129776001, Accuracy Rate: 46.97%, Ones Portion: 6.404623127309605e-05,             Test_Total_Loss: 2.0693740844726562, Test_Clf_Loss: 2.069373369216919, Test_Filter_Loss: 0.00010255587403662503, TEST_Seq_Loss: 0.9999892115592957, Test_Accuracy_Rate: 47.39%, Test_Ones_Portion: 7.995878695510328e-05\n",
      "Epoch 102, Total Loss: 2.029040575027466, Clf Loss: 2.0290393829345703, Filter Loss: 6.794666842324659e-05, Seq Loss: 0.9999927282333374, Accuracy Rate: 47.18%, Ones Portion: 6.199976633070037e-05,             Test_Total_Loss: 2.076306104660034, Test_Clf_Loss: 2.0763051509857178, Test_Filter_Loss: 9.459591819904745e-05, TEST_Seq_Loss: 0.9999901056289673, Test_Accuracy_Rate: 47.58%, Test_Ones_Portion: 7.423997158184648e-05\n",
      "Epoch 103, Total Loss: 2.0286896228790283, Clf Loss: 2.028688907623291, Filter Loss: 6.322471745079383e-05, Seq Loss: 0.9999935626983643, Accuracy Rate: 47.03%, Ones Portion: 5.8801841078093275e-05,             Test_Total_Loss: 2.076491117477417, Test_Clf_Loss: 2.0764901638031006, Test_Filter_Loss: 9.416784450877458e-05, TEST_Seq_Loss: 0.9999903440475464, Test_Accuracy_Rate: 47.34%, Test_Ones_Portion: 7.395014108624309e-05\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2000\n",
    "DateID = '2019110602'\n",
    "\n",
    "saveP1 = './model/'+DateID+'/model1'\n",
    "saveP2 = './model/'+DateID+'/model2'\n",
    "train_loss_acc = []\n",
    "train_loss_filter = []\n",
    "train_loss_seq = []\n",
    "train_weighted_loss = []\n",
    "train_acc_rate = []\n",
    "train_ones_num = []\n",
    "\n",
    "test_loss_acc = []\n",
    "test_loss_filter = []\n",
    "test_loss_seq = []\n",
    "test_weighted_loss = []\n",
    "test_acc_rate = []\n",
    "test_ones_num = []\n",
    "\n",
    "# signature_dict = {'att':model1.att}\n",
    "\n",
    "gc.collect()\n",
    "best_clf = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    for text, labels in train_ds:\n",
    "        train_step(text, labels)\n",
    "\n",
    "    for test_text, test_labels in valid_ds:\n",
    "        test_step(test_text, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Total Loss: {}, Clf Loss: {}, Filter Loss: {}, Seq Loss: {}, Accuracy Rate: {:5.2f}%, Ones Portion: {}, \\\n",
    "            Test_Total_Loss: {}, Test_Clf_Loss: {}, Test_Filter_Loss: {}, TEST_Seq_Loss: {}, Test_Accuracy_Rate: {:5.2f}%, Test_Ones_Portion: {}'\n",
    "    print(template.format(epoch+1,train_loss.result(),\n",
    "                          train_accloss.result(),train_filloss.result(),train_seqloss.result(),\n",
    "                        train_accuracy.result()*100,train_ones.result(),\n",
    "                        test_loss.result(),\n",
    "                        test_accloss.result(),test_filloss.result(),test_seqloss.result(),\n",
    "                        test_accuracy.result()*100,test_ones.result(),\n",
    "                        ))\n",
    "\n",
    "    train_loss_acc.append( train_accloss.result().numpy())\n",
    "    train_loss_filter.append( train_filloss.result().numpy())\n",
    "    train_loss_seq.append( train_seqloss.result().numpy())\n",
    "    train_weighted_loss.append( train_loss.result().numpy())\n",
    "    train_acc_rate.append( train_accuracy.result().numpy())\n",
    "    train_ones_num.append( train_ones.result().numpy())\n",
    "    \n",
    "    test_loss_acc.append( test_accloss.result().numpy())\n",
    "    test_loss_filter.append( test_filloss.result().numpy())\n",
    "    test_loss_seq.append( test_seqloss.result().numpy())\n",
    "    test_weighted_loss.append( test_loss.result().numpy())\n",
    "    test_acc_rate.append( test_accuracy.result().numpy())\n",
    "    test_ones_num.append( test_ones.result().numpy())\n",
    "    if best_clf<=test_accuracy.result()*100:\n",
    "        #tf.saved_model.save(model1,saveP1+'_all')\n",
    "        #model1.save(saveP1,save_format='h5')\n",
    "        #model2.save(saveP2,save_format='h5')\n",
    "        #tf.saved_model.save(model2,saveP2+'_all')\n",
    "        model1.save_weights(saveP1,save_format='tf')\n",
    "        model2.save_weights(saveP2,save_format='tf')\n",
    "        best_clf = test_accuracy.result()*100\n",
    "        print('===MODEL WEIGHTS SAVED===',saveP1,saveP2)\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accloss.reset_states()\n",
    "    train_filloss.reset_states()\n",
    "    train_seqloss.reset_states()\n",
    "    train_ones.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    test_accloss.reset_states()\n",
    "    test_filloss.reset_states()\n",
    "    test_seqloss.reset_states()\n",
    "    test_ones.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train total loss</th>\n",
       "      <th>train acc loss</th>\n",
       "      <th>train filter loss</th>\n",
       "      <th>train seq loss</th>\n",
       "      <th>train acc rate</th>\n",
       "      <th>train ones num</th>\n",
       "      <th>test total loss</th>\n",
       "      <th>test acc loss</th>\n",
       "      <th>test filter loss</th>\n",
       "      <th>test seq loss</th>\n",
       "      <th>test acc rate</th>\n",
       "      <th>test ones num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.704375</td>\n",
       "      <td>2.644375</td>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.421834</td>\n",
       "      <td>2.361834</td>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371733</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.345854</td>\n",
       "      <td>2.285854</td>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.141412</td>\n",
       "      <td>2.081412</td>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372701</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.920609</td>\n",
       "      <td>1.860609</td>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483293</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.886240</td>\n",
       "      <td>1.826240</td>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546951</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.851359</td>\n",
       "      <td>1.792340</td>\n",
       "      <td>5.901964e-01</td>\n",
       "      <td>0.041586</td>\n",
       "      <td>0.552058</td>\n",
       "      <td>0.958572</td>\n",
       "      <td>2.067819</td>\n",
       "      <td>2.055877</td>\n",
       "      <td>1.194174e-01</td>\n",
       "      <td>0.985861</td>\n",
       "      <td>0.516941</td>\n",
       "      <td>0.016080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.023761</td>\n",
       "      <td>2.022295</td>\n",
       "      <td>1.466692e-02</td>\n",
       "      <td>0.996965</td>\n",
       "      <td>0.491162</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>1.971825</td>\n",
       "      <td>1.970777</td>\n",
       "      <td>1.048062e-02</td>\n",
       "      <td>0.996862</td>\n",
       "      <td>0.516941</td>\n",
       "      <td>0.003956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>2.337544</td>\n",
       "      <td>2.337544</td>\n",
       "      <td>1.596824e-11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.361200</td>\n",
       "      <td>2.361200</td>\n",
       "      <td>1.630760e-13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.371733</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>2.337815</td>\n",
       "      <td>2.337815</td>\n",
       "      <td>1.596694e-11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.360485</td>\n",
       "      <td>2.360485</td>\n",
       "      <td>1.630046e-13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.371733</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>2.336691</td>\n",
       "      <td>2.336691</td>\n",
       "      <td>1.596567e-11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.361217</td>\n",
       "      <td>2.361217</td>\n",
       "      <td>1.629373e-13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.371733</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>2.336771</td>\n",
       "      <td>2.336771</td>\n",
       "      <td>1.596441e-11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.361580</td>\n",
       "      <td>2.361580</td>\n",
       "      <td>1.628630e-13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.371733</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2.336331</td>\n",
       "      <td>2.336331</td>\n",
       "      <td>1.596299e-11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.360876</td>\n",
       "      <td>2.360876</td>\n",
       "      <td>1.628124e-13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.371733</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      train total loss  train acc loss  train filter loss  train seq loss  \\\n",
       "0             2.704375        2.644375       6.000000e-01        0.000000   \n",
       "1             2.345854        2.285854       6.000000e-01        0.000000   \n",
       "2             1.920609        1.860609       6.000000e-01        0.000000   \n",
       "3             1.851359        1.792340       5.901964e-01        0.041586   \n",
       "4             2.023761        2.022295       1.466692e-02        0.996965   \n",
       "...                ...             ...                ...             ...   \n",
       "1995          2.337544        2.337544       1.596824e-11        1.000000   \n",
       "1996          2.337815        2.337815       1.596694e-11        1.000000   \n",
       "1997          2.336691        2.336691       1.596567e-11        1.000000   \n",
       "1998          2.336771        2.336771       1.596441e-11        1.000000   \n",
       "1999          2.336331        2.336331       1.596299e-11        1.000000   \n",
       "\n",
       "      train acc rate  train ones num  test total loss  test acc loss  \\\n",
       "0           0.371186        1.000000         2.421834       2.361834   \n",
       "1           0.378087        1.000000         2.141412       2.081412   \n",
       "2           0.483293        1.000000         1.886240       1.826240   \n",
       "3           0.552058        0.958572         2.067819       2.055877   \n",
       "4           0.491162        0.003815         1.971825       1.970777   \n",
       "...              ...             ...              ...            ...   \n",
       "1995        0.377240        0.000000         2.361200       2.361200   \n",
       "1996        0.377240        0.000000         2.360485       2.360485   \n",
       "1997        0.377240        0.000000         2.361217       2.361217   \n",
       "1998        0.377240        0.000000         2.361580       2.361580   \n",
       "1999        0.377240        0.000000         2.360876       2.360876   \n",
       "\n",
       "      test filter loss  test seq loss  test acc rate  test ones num  \n",
       "0         6.000000e-01       0.000000       0.371733       1.000000  \n",
       "1         6.000000e-01       0.000000       0.372701       1.000000  \n",
       "2         6.000000e-01       0.000000       0.546951       1.000000  \n",
       "3         1.194174e-01       0.985861       0.516941       0.016080  \n",
       "4         1.048062e-02       0.996862       0.516941       0.003956  \n",
       "...                ...            ...            ...            ...  \n",
       "1995      1.630760e-13       1.000000       0.371733       0.000000  \n",
       "1996      1.630046e-13       1.000000       0.371733       0.000000  \n",
       "1997      1.629373e-13       1.000000       0.371733       0.000000  \n",
       "1998      1.628630e-13       1.000000       0.371733       0.000000  \n",
       "1999      1.628124e-13       1.000000       0.371733       0.000000  \n",
       "\n",
       "[2000 rows x 12 columns]"
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = './results/'+DateID+'/'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "saveR = save_dir + 'losses_metrics.xlsx'\n",
    "data = {'train total loss':train_weighted_loss, 'train acc loss':train_loss_acc,\n",
    "        'train filter loss':train_loss_filter,'train seq loss':train_loss_seq,\n",
    "        'train acc rate':train_acc_rate, 'train ones num':train_ones_num,\n",
    "        'test total loss':test_weighted_loss, 'test acc loss':test_loss_acc,\n",
    "        'test filter loss':test_loss_filter, 'test seq loss': test_loss_seq,\n",
    "        'test acc rate':test_acc_rate, 'test ones num':test_ones_num\n",
    "       }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(saveR)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019110601'"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DateID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(None, dtype=object)"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_weighted_loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(2.3583207, dtype=float32)"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk = np.array(test_accloss.result())\n",
    "kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3583207"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accloss.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1125\u001b[0m                                        \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m                                        sigcls=Signature)\n\u001b[0m\u001b[1;32m   1127\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2287\u001b[0m         return _signature_from_builtin(sigcls, obj,\n\u001b[0;32m-> 2288\u001b[0;31m                                        skip_bound_arg=skip_bound_arg)\n\u001b[0m\u001b[1;32m   2289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36m_signature_from_builtin\u001b[0;34m(cls, func, skip_bound_arg)\u001b[0m\n\u001b[1;32m   2111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2112\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no signature found for builtin {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: no signature found for builtin <tensorflow.python.keras.saving.saved_model.save_impl.LayerCall object at 0x7f052aadf350>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-523-1449814dd641>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model1.save('./model/emb_layer') #tf.keras.models.load_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./model/emb_layer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/emb_layer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[1;32m    868\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msignatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     signatures = signature_serialization.find_function_to_export(\n\u001b[0;32m--> 870\u001b[0;31m         checkpoint_graph_view)\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m   \u001b[0msignatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature_serialization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_signatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_serialization.py\u001b[0m in \u001b[0;36mfind_function_to_export\u001b[0;34m(saveable_view)\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;31m# If the user did not specify signatures, check the root object for a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;31m# that can be made into a signature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0mfunctions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaveable_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveable_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEFAULT_SIGNATURE_ATTR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msignature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\u001b[0m in \u001b[0;36mlist_functions\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobj_functions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access\n\u001b[0;32m--> 141\u001b[0;31m           self._serialization_cache)\n\u001b[0m\u001b[1;32m    142\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_list_functions_for_serialization\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m   2420\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_list_functions_for_serialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m     return (self._trackable_saved_model_saver\n\u001b[0;32m-> 2422\u001b[0;31m             .list_functions_for_serialization(serialization_cache))\n\u001b[0m\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/base_serialization.py\u001b[0m in \u001b[0;36mlist_functions_for_serialization\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mConcreteFunction\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mfns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions_to_serialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialization_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# The parent AutoTrackable class saves all user-defined tf.functions, and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\u001b[0m in \u001b[0;36mfunctions_to_serialize\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfunctions_to_serialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     return (self._get_serialized_attributes(\n\u001b[0;32m---> 79\u001b[0;31m         serialization_cache).functions_to_serialize)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_serialized_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\u001b[0m in \u001b[0;36m_get_serialized_attributes\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     object_dict, function_dict = self._get_serialized_attributes_internal(\n\u001b[0;32m---> 94\u001b[0;31m         serialization_cache)\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mserialized_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_and_validate_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/model_serialization.py\u001b[0m in \u001b[0;36m_get_serialized_attributes_internal\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# cache (i.e. this is the root level object).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialization_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKERAS_CACHE_KEY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m       \u001b[0mdefault_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_save_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Other than the default signature function, all other attributes match with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\u001b[0m in \u001b[0;36mdefault_save_signature\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[0moriginal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reset_layer_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_model_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m   \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m   \u001b[0m_restore_layer_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0minitializer_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    406\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    407\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 408\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36m_wrapped_model\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    141\u001b[0m     with base_layer_utils.call_context().enter(\n\u001b[1;32m    142\u001b[0m         model, inputs=inputs, build_graph=False, training=False, saving=True):\n\u001b[0;32m--> 143\u001b[0;31m       \u001b[0moutputs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-498-f3457470eec4>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#x = self.rnn1(x1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#x = self.bn1(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    218\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mhas_arg\u001b[0;34m(fn, name, accept_all)\u001b[0m\n\u001b[1;32m    302\u001b[0m       \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhether\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0maccepts\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mname\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m   \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m   \u001b[0marg_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maccept_all\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvarkw\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_maybe_argspec_to_fullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_getfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m# else. So to be fully backwards compatible, we catch all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# possible exceptions here, and reraise a TypeError.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unsupported callable'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported callable"
     ]
    }
   ],
   "source": [
    "# model1.save('./model/emb_layer') #tf.keras.models.load_model\n",
    "tf.saved_model.save(model1,'./model/emb_layer')\n",
    "model3 = tf.keras.models.load_model('./model/emb_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CheckpointLoadStatus' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-524-07c6ede4bf0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/emb_layer_weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'CheckpointLoadStatus' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "model3 = model1.load_weights('./model/emb_layer_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.emb.set_weights(model3.emb.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"base_model_1_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_58 (Masking)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "glove_emb (Embedding)        multiple                  2138112   \n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You tried to call `count_params` on common_extract, but the layer isn't built. You can build it manually via: `common_extract.build(batch_input_shape)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-526-cdf54a019e40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/emb_layer_weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   1263\u001b[0m                               \u001b[0mline_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m                               \u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m                               print_fn=print_fn)\n\u001b[0m\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_validate_graph_inputs_and_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/layer_utils.py\u001b[0m in \u001b[0;36mprint_summary\u001b[0;34m(model, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m    224\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequential_like\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m       \u001b[0mprint_layer_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m       \u001b[0mprint_layer_summary_with_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/layer_utils.py\u001b[0m in \u001b[0;36mprint_layer_summary\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mcls_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' ('\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m')'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0mprint_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mcount_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1627\u001b[0m                          \u001b[0;34m', but the layer isn\\'t built. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m                          \u001b[0;34m'You can build it manually via: `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m                          '.build(batch_input_shape)`.')\n\u001b[0m\u001b[1;32m   1630\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You tried to call `count_params` on common_extract, but the layer isn't built. You can build it manually via: `common_extract.build(batch_input_shape)`."
     ]
    }
   ],
   "source": [
    "model3 = model1\n",
    "model3.load_weights('./model/emb_layer_weight')\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 在某些參數的情況下0 1是train不起來的但是weight可以\n",
    "    * alpha = 0.0, beta=0.6\n",
    "    * init_w = tensorflow.keras.initializers.Constant(value=0.9), init_b = tensorflow.keras.initializers.Constant(value=0.7)\n",
    "* 0/1 with emb比較容易overfit。如果是weight的比較沒那麼嚴重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 不用sigmoid或是hard_sigmoid。改良relu + linear，並拆成兩個model，把round前面多加上clip\n",
    "    * sigmoid中間的變化太快(一瞬間就會掉到0或是1)，改成relu在>0~無限大(linear為了還在0~1)再去clip再round，可以看到每個epoch的變化\n",
    "* 1st phase的beta一定要>=0.6否則不會動，ones#都會是0\n",
    "    * beta=1 (放0 1進去): weight init設成1也沒用，但把bias設成1就會一開始都是ones#=1了。weight=1 bias=0.2都匯市0，bias=0.3會是0.87。0.6/0.3都是0。0.8/0.3差不多是0.5(但是train很慢acc進步很慢就是了)。0.5/0.5是從0.01開始往上升 (0 1放進去會比較難train是因為它的變化量太大，一下就是有或沒有，所以clf可能學不好，但如果是weight每次gradient進步的都是一小點就會比較容易上升)\n",
    "    * beta=0.6 (放0 1進去): 0.8/0.5都是0。0.9/0.8 從0.9一直到0。0.9/0.6差不多是從0.5但又有時候到0.7都是0.0(很難train，Nadam換個opt有時候沒用。EX變成adam 0.9/0.8才有0.96開始但如果0.9/0.75變成0開始。Rmsprop 0.9/0.8又是從0.95開始往下)。但如果都改成傳入weight就都沒問題。ones一開始大概0.5 weight平均，也不會卡住\n",
    "* 建議: 先訓練embedding weight matrix，但是要看goal是要怎樣的matrix\n",
    "* 其實他不管幾%都會train得很好，除非固定embedding，或是用更弱的clf\n",
    "* 若設定alpha，就像是regularizer term (penalty)，設越大drop越多\n",
    "* 一開始先很多ones，再越來越少個"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 同一個opt若加入transform就會train不起來\n",
    "* 兩個不同的opt加入transform也會train不起來 (persistent、non-persis都不行)，且與BN無關"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
