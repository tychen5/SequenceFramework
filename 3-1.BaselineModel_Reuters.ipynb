{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.text import *\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import gc\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(all_length):\n",
    "    '''\n",
    "    input: length list of elements e.g.[1,1,1,3,5,9,4,2,1,3,54,78,5...]\n",
    "    output1: mean、std、mode、min、q1、median(q2)、q3、max、iqr、outlier、far out\n",
    "    output2: statistics graph、10%~90% form\n",
    "    '''\n",
    "    stat_dict = {}\n",
    "    stat_dict['mean'] = np.mean(all_length)\n",
    "    stat_dict['std'] = np.std(all_length)\n",
    "    stat_dict['mode'] = np.argmax(np.bincount(all_length))\n",
    "    stat_dict['min'] = np.min(all_length)\n",
    "    stat_dict['q1'] = np.quantile(all_length,0.25)\n",
    "    stat_dict['median'] = np.quantile(all_length,0.5)\n",
    "    stat_dict['q3'] = np.quantile(all_length,0.75)\n",
    "    stat_dict['max'] = np.max(all_length)\n",
    "    stat_dict['iqr'] = stat_dict['q3'] - stat_dict['q1']\n",
    "    stat_dict['outlier'] = stat_dict['q3'] + 1.5*stat_dict['iqr']\n",
    "    stat_dict['far_out'] = stat_dict['q3'] + 3*stat_dict['iqr']\n",
    "    for i in [10,20,30,40,50,60,70,80,90,100]:\n",
    "        stat_dict[str(i)+'%'] = np.percentile(all_length,i)\n",
    "    return pd.DataFrame.from_dict(stat_dict,orient='index',columns=['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 8352#8352 #Top most frequent words to consider. Any less frequent word will appear as oov_char value in the sequence data.\n",
    "max_length = 360#360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words#: 30979\n",
      "8260 train sequences\n",
      "2066 test sequences\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "print('all_words#:',len(word_index))\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,maxlen=max_length,\n",
    "                                                         test_split=0.2,seed=830913)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>145.964197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>145.878476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mode</th>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q1</th>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q3</th>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2376.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iqr</th>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outlier</th>\n",
       "      <td>360.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far_out</th>\n",
       "      <td>540.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30%</th>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40%</th>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60%</th>\n",
       "      <td>112.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70%</th>\n",
       "      <td>154.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>206.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>315.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>2376.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              length\n",
       "mean      145.964197\n",
       "std       145.878476\n",
       "mode       17.000000\n",
       "min         2.000000\n",
       "q1         60.000000\n",
       "median     95.000000\n",
       "q3        180.000000\n",
       "max      2376.000000\n",
       "iqr       120.000000\n",
       "outlier   360.000000\n",
       "far_out   540.000000\n",
       "10%        35.000000\n",
       "20%        53.000000\n",
       "30%        67.000000\n",
       "40%        81.000000\n",
       "50%        95.000000\n",
       "60%       112.000000\n",
       "70%       154.000000\n",
       "80%       206.000000\n",
       "90%       315.000000\n",
       "100%     2376.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_len = [len(x) for x in x_train]\n",
    "# test_len = [len(x) for x in x_test]\n",
    "# all_len = train_len\n",
    "# all_len.extend(test_len)\n",
    "# basic_statistics(all_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(all_len)\n",
    "# df.to_excel('./results/length_dist.xlsx', header=False, index=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 11228,\n",
       "         53: 4213,\n",
       "         352: 647,\n",
       "         26: 8451,\n",
       "         14: 15015,\n",
       "         279: 801,\n",
       "         39: 5818,\n",
       "         72: 3091,\n",
       "         4497: 26,\n",
       "         18: 11039,\n",
       "         83: 2597,\n",
       "         5291: 21,\n",
       "         88: 2381,\n",
       "         5397: 20,\n",
       "         11: 20141,\n",
       "         3412: 37,\n",
       "         19: 10755,\n",
       "         151: 1363,\n",
       "         230: 962,\n",
       "         831: 253,\n",
       "         15: 13329,\n",
       "         165: 1232,\n",
       "         318: 707,\n",
       "         3780: 33,\n",
       "         124: 1676,\n",
       "         1527: 117,\n",
       "         1424: 128,\n",
       "         35: 6588,\n",
       "         5302: 20,\n",
       "         12: 16668,\n",
       "         17: 11191,\n",
       "         486: 459,\n",
       "         341: 663,\n",
       "         142: 1466,\n",
       "         255: 870,\n",
       "         219: 997,\n",
       "         429: 528,\n",
       "         68: 3363,\n",
       "         146: 1402,\n",
       "         252: 882,\n",
       "         191: 1098,\n",
       "         15448: 3,\n",
       "         3631: 35,\n",
       "         2283: 65,\n",
       "         71: 3120,\n",
       "         10: 29581,\n",
       "         342: 660,\n",
       "         49: 4565,\n",
       "         1977: 80,\n",
       "         324: 695,\n",
       "         27: 8311,\n",
       "         9222: 8,\n",
       "         672: 330,\n",
       "         450: 506,\n",
       "         5: 42393,\n",
       "         547: 406,\n",
       "         40: 5593,\n",
       "         471: 476,\n",
       "         4810: 24,\n",
       "         149: 1371,\n",
       "         26639: 1,\n",
       "         794: 266,\n",
       "         734: 300,\n",
       "         8976: 8,\n",
       "         8975: 8,\n",
       "         4007: 31,\n",
       "         9: 29956,\n",
       "         25113: 1,\n",
       "         247: 904,\n",
       "         8: 29978,\n",
       "         1299: 144,\n",
       "         381: 618,\n",
       "         34: 7010,\n",
       "         385: 604,\n",
       "         13: 15224,\n",
       "         109: 1810,\n",
       "         167: 1224,\n",
       "         4: 82723,\n",
       "         60: 3654,\n",
       "         130: 1617,\n",
       "         1461: 123,\n",
       "         3366: 38,\n",
       "         1896: 86,\n",
       "         204: 1046,\n",
       "         33: 7037,\n",
       "         3512: 36,\n",
       "         888: 234,\n",
       "         25: 8579,\n",
       "         423: 535,\n",
       "         256: 866,\n",
       "         7: 33157,\n",
       "         1075: 185,\n",
       "         329: 685,\n",
       "         769: 281,\n",
       "         302: 742,\n",
       "         113: 1749,\n",
       "         446: 508,\n",
       "         107: 1841,\n",
       "         226: 979,\n",
       "         240: 923,\n",
       "         817: 258,\n",
       "         101: 1923,\n",
       "         36: 6436,\n",
       "         189: 1107,\n",
       "         6501: 15,\n",
       "         57: 3756,\n",
       "         6: 40350,\n",
       "         481: 464,\n",
       "         239: 927,\n",
       "         118: 1704,\n",
       "         246: 905,\n",
       "         424: 535,\n",
       "         386: 601,\n",
       "         415: 545,\n",
       "         1677: 103,\n",
       "         24: 9022,\n",
       "         291: 767,\n",
       "         662: 337,\n",
       "         85: 2474,\n",
       "         2243: 67,\n",
       "         880: 236,\n",
       "         224: 988,\n",
       "         1077: 184,\n",
       "         54: 3858,\n",
       "         29: 7797,\n",
       "         530: 419,\n",
       "         30: 7627,\n",
       "         1500: 119,\n",
       "         4175: 29,\n",
       "         69: 3203,\n",
       "         12040: 5,\n",
       "         84: 2506,\n",
       "         442: 509,\n",
       "         31: 7288,\n",
       "         373: 628,\n",
       "         159: 1298,\n",
       "         7370: 12,\n",
       "         41: 5553,\n",
       "         351: 648,\n",
       "         343: 656,\n",
       "         6880: 14,\n",
       "         21: 10377,\n",
       "         840: 249,\n",
       "         184: 1129,\n",
       "         5463: 20,\n",
       "         526: 420,\n",
       "         87: 2405,\n",
       "         108: 1812,\n",
       "         140: 1471,\n",
       "         168: 1207,\n",
       "         231: 961,\n",
       "         286: 779,\n",
       "         1351: 138,\n",
       "         397: 574,\n",
       "         105: 1866,\n",
       "         3497: 36,\n",
       "         120: 1694,\n",
       "         1456: 124,\n",
       "         161: 1272,\n",
       "         64: 3453,\n",
       "         1909: 84,\n",
       "         762: 283,\n",
       "         550: 403,\n",
       "         325: 694,\n",
       "         1766: 95,\n",
       "         613: 365,\n",
       "         548: 406,\n",
       "         3205: 41,\n",
       "         16: 12395,\n",
       "         5700: 18,\n",
       "         15663: 3,\n",
       "         51: 4256,\n",
       "         562: 396,\n",
       "         299: 753,\n",
       "         45: 5081,\n",
       "         306: 733,\n",
       "         194: 1086,\n",
       "         572: 388,\n",
       "         1222: 156,\n",
       "         3586: 35,\n",
       "         7718: 11,\n",
       "         22: 9345,\n",
       "         3297: 39,\n",
       "         3380: 38,\n",
       "         66: 3380,\n",
       "         2794: 50,\n",
       "         1163: 167,\n",
       "         178: 1160,\n",
       "         74: 3060,\n",
       "         865: 242,\n",
       "         46: 4836,\n",
       "         353: 645,\n",
       "         134: 1564,\n",
       "         70: 3184,\n",
       "         4596: 25,\n",
       "         86: 2434,\n",
       "         47: 4688,\n",
       "         4315: 27,\n",
       "         597: 377,\n",
       "         688: 323,\n",
       "         5242: 21,\n",
       "         32: 7100,\n",
       "         4215: 28,\n",
       "         63: 3492,\n",
       "         180: 1143,\n",
       "         183: 1133,\n",
       "         61: 3652,\n",
       "         2979: 46,\n",
       "         59: 3689,\n",
       "         4999: 22,\n",
       "         123: 1680,\n",
       "         235: 946,\n",
       "         131: 1607,\n",
       "         891: 233,\n",
       "         4088: 30,\n",
       "         98: 2063,\n",
       "         1025: 196,\n",
       "         633: 351,\n",
       "         2543: 58,\n",
       "         150: 1366,\n",
       "         710: 313,\n",
       "         220: 994,\n",
       "         48: 4574,\n",
       "         864: 243,\n",
       "         276: 809,\n",
       "         360: 641,\n",
       "         211: 1026,\n",
       "         1004: 203,\n",
       "         128: 1619,\n",
       "         10485: 6,\n",
       "         5769: 18,\n",
       "         651: 343,\n",
       "         574: 387,\n",
       "         400: 571,\n",
       "         3673: 34,\n",
       "         6189: 16,\n",
       "         186: 1121,\n",
       "         3879: 32,\n",
       "         968: 212,\n",
       "         90: 2331,\n",
       "         1081: 183,\n",
       "         5604: 19,\n",
       "         4167: 29,\n",
       "         14223: 3,\n",
       "         265: 850,\n",
       "         861: 244,\n",
       "         483: 461,\n",
       "         2858: 48,\n",
       "         3044: 44,\n",
       "         2576: 56,\n",
       "         551: 401,\n",
       "         50: 4383,\n",
       "         5702: 18,\n",
       "         1594: 110,\n",
       "         2161: 71,\n",
       "         111: 1753,\n",
       "         304: 735,\n",
       "         44: 5082,\n",
       "         2128: 72,\n",
       "         3632: 35,\n",
       "         62: 3646,\n",
       "         4200: 29,\n",
       "         2535: 58,\n",
       "         160: 1298,\n",
       "         294: 764,\n",
       "         76: 3019,\n",
       "         67: 3374,\n",
       "         1872: 87,\n",
       "         915: 227,\n",
       "         89: 2371,\n",
       "         135: 1519,\n",
       "         312: 715,\n",
       "         117: 1705,\n",
       "         225: 979,\n",
       "         206: 1042,\n",
       "         152: 1363,\n",
       "         372: 629,\n",
       "         680: 325,\n",
       "         37: 6266,\n",
       "         38: 6157,\n",
       "         387: 600,\n",
       "         516: 431,\n",
       "         500: 447,\n",
       "         729: 301,\n",
       "         838: 250,\n",
       "         52: 4244,\n",
       "         846: 247,\n",
       "         458: 487,\n",
       "         757: 286,\n",
       "         1605: 109,\n",
       "         3963: 31,\n",
       "         317: 708,\n",
       "         55: 3838,\n",
       "         528: 420,\n",
       "         2457: 60,\n",
       "         16383: 2,\n",
       "         260: 859,\n",
       "         2735: 51,\n",
       "         681: 325,\n",
       "         1195: 159,\n",
       "         4779: 24,\n",
       "         1204: 158,\n",
       "         28: 8056,\n",
       "         196: 1072,\n",
       "         1735: 97,\n",
       "         523: 424,\n",
       "         145: 1407,\n",
       "         2068: 75,\n",
       "         420: 538,\n",
       "         73: 3061,\n",
       "         418: 541,\n",
       "         525: 421,\n",
       "         8255: 10,\n",
       "         102: 1921,\n",
       "         289: 772,\n",
       "         1474: 121,\n",
       "         93: 2246,\n",
       "         1926: 83,\n",
       "         273: 819,\n",
       "         542: 410,\n",
       "         202: 1049,\n",
       "         876: 237,\n",
       "         331: 684,\n",
       "         208: 1034,\n",
       "         147: 1400,\n",
       "         126: 1644,\n",
       "         28507: 1,\n",
       "         661: 339,\n",
       "         16629: 2,\n",
       "         768: 281,\n",
       "         3186: 42,\n",
       "         77: 3011,\n",
       "         238: 931,\n",
       "         43: 5369,\n",
       "         133: 1566,\n",
       "         91: 2277,\n",
       "         410: 550,\n",
       "         132: 1586,\n",
       "         663: 337,\n",
       "         233: 950,\n",
       "         5900: 18,\n",
       "         6404: 16,\n",
       "         4855: 23,\n",
       "         625: 355,\n",
       "         42: 5379,\n",
       "         438: 516,\n",
       "         80: 2749,\n",
       "         1901: 85,\n",
       "         158: 1318,\n",
       "         20: 10746,\n",
       "         355: 645,\n",
       "         6529: 15,\n",
       "         56: 3810,\n",
       "         17636: 2,\n",
       "         938: 220,\n",
       "         316: 709,\n",
       "         100: 1956,\n",
       "         261: 856,\n",
       "         439: 516,\n",
       "         7675: 11,\n",
       "         1286: 145,\n",
       "         29406: 1,\n",
       "         5073: 22,\n",
       "         4755: 24,\n",
       "         2827: 49,\n",
       "         19901: 2,\n",
       "         12035: 5,\n",
       "         9454: 7,\n",
       "         1452: 124,\n",
       "         986: 207,\n",
       "         148: 1374,\n",
       "         732: 301,\n",
       "         310: 724,\n",
       "         281: 793,\n",
       "         200: 1050,\n",
       "         11347: 5,\n",
       "         19454: 2,\n",
       "         1649: 105,\n",
       "         21032: 1,\n",
       "         903: 230,\n",
       "         963: 214,\n",
       "         675: 326,\n",
       "         21760: 1,\n",
       "         4254: 28,\n",
       "         18761: 2,\n",
       "         187: 1121,\n",
       "         188: 1114,\n",
       "         4163: 29,\n",
       "         3114: 43,\n",
       "         546: 406,\n",
       "         3611: 35,\n",
       "         5630: 19,\n",
       "         502: 445,\n",
       "         11066: 5,\n",
       "         701: 316,\n",
       "         765: 281,\n",
       "         1354: 136,\n",
       "         251: 882,\n",
       "         335: 670,\n",
       "         79: 2883,\n",
       "         4117: 29,\n",
       "         22496: 1,\n",
       "         6845: 14,\n",
       "         894: 232,\n",
       "         2302: 65,\n",
       "         205: 1045,\n",
       "         2430: 61,\n",
       "         362: 640,\n",
       "         524: 424,\n",
       "         78: 2905,\n",
       "         3063: 44,\n",
       "         863: 243,\n",
       "         582: 382,\n",
       "         832: 252,\n",
       "         323: 699,\n",
       "         195: 1081,\n",
       "         18932: 2,\n",
       "         22172: 1,\n",
       "         8947: 8,\n",
       "         1178: 162,\n",
       "         144: 1430,\n",
       "         671: 332,\n",
       "         396: 577,\n",
       "         6379: 16,\n",
       "         175: 1176,\n",
       "         1041: 193,\n",
       "         1379: 134,\n",
       "         735: 300,\n",
       "         788: 271,\n",
       "         2763: 51,\n",
       "         270: 826,\n",
       "         23: 9113,\n",
       "         515: 432,\n",
       "         472: 474,\n",
       "         272: 823,\n",
       "         4242: 28,\n",
       "         284: 783,\n",
       "         750: 291,\n",
       "         6115: 17,\n",
       "         5201: 21,\n",
       "         462: 484,\n",
       "         482: 464,\n",
       "         780: 275,\n",
       "         122: 1690,\n",
       "         1047: 191,\n",
       "         138: 1500,\n",
       "         2885: 48,\n",
       "         2528: 58,\n",
       "         4455: 26,\n",
       "         1069: 186,\n",
       "         207: 1038,\n",
       "         2938: 47,\n",
       "         8418: 9,\n",
       "         726: 302,\n",
       "         3711: 34,\n",
       "         2276: 66,\n",
       "         1402: 131,\n",
       "         1088: 181,\n",
       "         203: 1047,\n",
       "         5473: 20,\n",
       "         258: 861,\n",
       "         2819: 49,\n",
       "         3688: 34,\n",
       "         162: 1267,\n",
       "         1643: 105,\n",
       "         5553: 19,\n",
       "         4328: 27,\n",
       "         5453: 20,\n",
       "         921: 225,\n",
       "         139: 1491,\n",
       "         245: 906,\n",
       "         1271: 147,\n",
       "         1555: 114,\n",
       "         156: 1323,\n",
       "         2933: 47,\n",
       "         7392: 12,\n",
       "         749: 291,\n",
       "         6077: 17,\n",
       "         4313: 27,\n",
       "         875: 238,\n",
       "         314: 714,\n",
       "         2080: 74,\n",
       "         5237: 21,\n",
       "         2132: 72,\n",
       "         5529: 19,\n",
       "         1415: 129,\n",
       "         2178: 70,\n",
       "         296: 760,\n",
       "         5405: 20,\n",
       "         3806: 32,\n",
       "         1044: 192,\n",
       "         3952: 31,\n",
       "         363: 640,\n",
       "         842: 248,\n",
       "         852: 246,\n",
       "         4601: 25,\n",
       "         127: 1629,\n",
       "         591: 379,\n",
       "         262: 856,\n",
       "         5030: 22,\n",
       "         13989: 3,\n",
       "         5182: 21,\n",
       "         7018: 13,\n",
       "         3126: 43,\n",
       "         5569: 19,\n",
       "         5963: 17,\n",
       "         11427: 5,\n",
       "         1480: 120,\n",
       "         141: 1468,\n",
       "         2425: 61,\n",
       "         1038: 194,\n",
       "         851: 246,\n",
       "         311: 718,\n",
       "         512: 435,\n",
       "         519: 430,\n",
       "         2385: 62,\n",
       "         497: 450,\n",
       "         580: 383,\n",
       "         6682: 15,\n",
       "         332: 678,\n",
       "         1161: 167,\n",
       "         798: 265,\n",
       "         121: 1693,\n",
       "         1358: 136,\n",
       "         232: 956,\n",
       "         6360: 16,\n",
       "         829: 253,\n",
       "         6082: 17,\n",
       "         356: 643,\n",
       "         179: 1148,\n",
       "         14601: 3,\n",
       "         1139: 173,\n",
       "         2131: 72,\n",
       "         7590: 12,\n",
       "         1094: 180,\n",
       "         1458: 124,\n",
       "         1843: 89,\n",
       "         6084: 17,\n",
       "         7151: 13,\n",
       "         13785: 4,\n",
       "         7931: 11,\n",
       "         645: 346,\n",
       "         2510: 59,\n",
       "         1841: 89,\n",
       "         371: 630,\n",
       "         8891: 8,\n",
       "         297: 759,\n",
       "         900: 231,\n",
       "         3130: 42,\n",
       "         1064: 187,\n",
       "         988: 207,\n",
       "         679: 326,\n",
       "         278: 809,\n",
       "         65: 3440,\n",
       "         2753: 51,\n",
       "         401: 569,\n",
       "         5722: 18,\n",
       "         4821: 24,\n",
       "         201: 1050,\n",
       "         12192: 4,\n",
       "         1384: 132,\n",
       "         2732: 52,\n",
       "         699: 316,\n",
       "         4662: 25,\n",
       "         2307: 65,\n",
       "         9116: 8,\n",
       "         17694: 2,\n",
       "         652: 343,\n",
       "         6989: 13,\n",
       "         1109: 178,\n",
       "         155: 1328,\n",
       "         4957: 22,\n",
       "         1714: 99,\n",
       "         1895: 86,\n",
       "         1545: 115,\n",
       "         1485: 120,\n",
       "         778: 277,\n",
       "         354: 645,\n",
       "         1200: 158,\n",
       "         215: 1017,\n",
       "         181: 1140,\n",
       "         907: 229,\n",
       "         106: 1859,\n",
       "         1669: 104,\n",
       "         464: 482,\n",
       "         359: 643,\n",
       "         222: 990,\n",
       "         480: 464,\n",
       "         4216: 28,\n",
       "         3451: 37,\n",
       "         895: 232,\n",
       "         567: 392,\n",
       "         985: 207,\n",
       "         1434: 127,\n",
       "         18534: 2,\n",
       "         15146: 3,\n",
       "         16226: 3,\n",
       "         824: 256,\n",
       "         669: 333,\n",
       "         670: 333,\n",
       "         114: 1737,\n",
       "         1533: 116,\n",
       "         1365: 135,\n",
       "         1056: 188,\n",
       "         569: 391,\n",
       "         2594: 56,\n",
       "         250: 884,\n",
       "         1393: 131,\n",
       "         2291: 65,\n",
       "         12908: 4,\n",
       "         870: 240,\n",
       "         13684: 4,\n",
       "         19776: 2,\n",
       "         1022: 196,\n",
       "         5331: 20,\n",
       "         3080: 44,\n",
       "         422: 535,\n",
       "         337: 666,\n",
       "         1348: 138,\n",
       "         3207: 41,\n",
       "         15548: 3,\n",
       "         15395: 3,\n",
       "         8487: 9,\n",
       "         13131: 4,\n",
       "         2743: 51,\n",
       "         1679: 103,\n",
       "         1657: 105,\n",
       "         691: 321,\n",
       "         24421: 1,\n",
       "         249: 892,\n",
       "         5901: 18,\n",
       "         979: 208,\n",
       "         1304: 143,\n",
       "         4958: 22,\n",
       "         1449: 125,\n",
       "         4492: 26,\n",
       "         529: 419,\n",
       "         5356: 20,\n",
       "         1601: 109,\n",
       "         1446: 126,\n",
       "         1008: 201,\n",
       "         4040: 30,\n",
       "         104: 1883,\n",
       "         7775: 11,\n",
       "         257: 862,\n",
       "         217: 1001,\n",
       "         553: 400,\n",
       "         2781: 50,\n",
       "         5821: 18,\n",
       "         110: 1758,\n",
       "         8016: 10,\n",
       "         4185: 29,\n",
       "         777: 277,\n",
       "         4034: 30,\n",
       "         1215: 156,\n",
       "         193: 1093,\n",
       "         9162: 8,\n",
       "         58: 3710,\n",
       "         27814: 1,\n",
       "         1614: 108,\n",
       "         3326: 39,\n",
       "         1324: 141,\n",
       "         6046: 17,\n",
       "         709: 313,\n",
       "         16640: 2,\n",
       "         8808: 8,\n",
       "         4374: 27,\n",
       "         4932: 23,\n",
       "         4766: 24,\n",
       "         2005: 79,\n",
       "         9694: 7,\n",
       "         3533: 36,\n",
       "         3388: 38,\n",
       "         3343: 39,\n",
       "         27085: 1,\n",
       "         1833: 89,\n",
       "         321: 703,\n",
       "         9451: 7,\n",
       "         19750: 2,\n",
       "         1745: 96,\n",
       "         1965: 81,\n",
       "         2662: 54,\n",
       "         1262: 149,\n",
       "         2019: 78,\n",
       "         29713: 1,\n",
       "         3467: 37,\n",
       "         3860: 32,\n",
       "         599: 376,\n",
       "         182: 1137,\n",
       "         3138: 42,\n",
       "         1046: 191,\n",
       "         479: 466,\n",
       "         236: 945,\n",
       "         564: 395,\n",
       "         1997: 79,\n",
       "         1209: 157,\n",
       "         736: 299,\n",
       "         2148: 71,\n",
       "         9184: 8,\n",
       "         12187: 4,\n",
       "         308: 730,\n",
       "         1607: 109,\n",
       "         1082: 182,\n",
       "         328: 687,\n",
       "         1226: 155,\n",
       "         210: 1027,\n",
       "         961: 214,\n",
       "         4985: 22,\n",
       "         2023: 77,\n",
       "         1016: 199,\n",
       "         448: 506,\n",
       "         724: 305,\n",
       "         303: 736,\n",
       "         909: 229,\n",
       "         223: 989,\n",
       "         2040: 76,\n",
       "         3016: 45,\n",
       "         11303: 5,\n",
       "         287: 776,\n",
       "         8269: 10,\n",
       "         3227: 41,\n",
       "         198: 1058,\n",
       "         3235: 41,\n",
       "         1898: 85,\n",
       "         1024: 196,\n",
       "         1974: 80,\n",
       "         12497: 4,\n",
       "         11381: 5,\n",
       "         2563: 57,\n",
       "         478: 467,\n",
       "         9633: 7,\n",
       "         97: 2077,\n",
       "         305: 734,\n",
       "         1066: 186,\n",
       "         585: 380,\n",
       "         177: 1166,\n",
       "         6794: 14,\n",
       "         4759: 24,\n",
       "         253: 877,\n",
       "         228: 964,\n",
       "         1457: 124,\n",
       "         1927: 83,\n",
       "         112: 1753,\n",
       "         1349: 138,\n",
       "         616: 363,\n",
       "         1873: 87,\n",
       "         214: 1022,\n",
       "         212: 1025,\n",
       "         75: 3040,\n",
       "         1572: 112,\n",
       "         2505: 59,\n",
       "         190: 1100,\n",
       "         447: 507,\n",
       "         508: 437,\n",
       "         1171: 165,\n",
       "         358: 643,\n",
       "         1414: 129,\n",
       "         654: 343,\n",
       "         1218: 156,\n",
       "         967: 212,\n",
       "         457: 489,\n",
       "         1136: 173,\n",
       "         1622: 107,\n",
       "         1007: 202,\n",
       "         1237: 153,\n",
       "         3447: 37,\n",
       "         9263: 8,\n",
       "         657: 342,\n",
       "         820: 257,\n",
       "         1673: 103,\n",
       "         6851: 14,\n",
       "         1521: 117,\n",
       "         95: 2162,\n",
       "         23859: 1,\n",
       "         711: 312,\n",
       "         444: 508,\n",
       "         744: 294,\n",
       "         1515: 117,\n",
       "         2009: 78,\n",
       "         6847: 14,\n",
       "         3850: 32,\n",
       "         378: 621,\n",
       "         2825: 49,\n",
       "         958: 215,\n",
       "         1207: 157,\n",
       "         129: 1617,\n",
       "         485: 460,\n",
       "         154: 1331,\n",
       "         3030: 45,\n",
       "         992: 205,\n",
       "         1242: 152,\n",
       "         816: 258,\n",
       "         2985: 45,\n",
       "         571: 388,\n",
       "         520: 427,\n",
       "         725: 304,\n",
       "         115: 1737,\n",
       "         14289: 3,\n",
       "         10148: 6,\n",
       "         6768: 14,\n",
       "         11543: 5,\n",
       "         1524: 117,\n",
       "         7321: 12,\n",
       "         608: 369,\n",
       "         1176: 162,\n",
       "         3596: 35,\n",
       "         173: 1182,\n",
       "         563: 395,\n",
       "         488: 458,\n",
       "         4533: 26,\n",
       "         1413: 129,\n",
       "         1807: 91,\n",
       "         2666: 53,\n",
       "         408: 552,\n",
       "         164: 1243,\n",
       "         586: 380,\n",
       "         2335: 63,\n",
       "         1111: 178,\n",
       "         1666: 104,\n",
       "         1322: 141,\n",
       "         1290: 145,\n",
       "         4756: 24,\n",
       "         404: 563,\n",
       "         908: 229,\n",
       "         740: 295,\n",
       "         621: 359,\n",
       "         1316: 142,\n",
       "         15632: 3,\n",
       "         3815: 32,\n",
       "         650: 343,\n",
       "         1825: 90,\n",
       "         4234: 28,\n",
       "         673: 330,\n",
       "         3145: 42,\n",
       "         192: 1095,\n",
       "         510: 436,\n",
       "         604: 371,\n",
       "         2147: 71,\n",
       "         506: 440,\n",
       "         443: 508,\n",
       "         469: 476,\n",
       "         11887: 5,\n",
       "         2564: 57,\n",
       "         125: 1665,\n",
       "         454: 493,\n",
       "         19191: 2,\n",
       "         1152: 169,\n",
       "         96: 2097,\n",
       "         25255: 1,\n",
       "         6087: 17,\n",
       "         847: 247,\n",
       "         3530: 36,\n",
       "         3427: 37,\n",
       "         2852: 49,\n",
       "         1107: 178,\n",
       "         1742: 96,\n",
       "         81: 2664,\n",
       "         5207: 21,\n",
       "         2542: 58,\n",
       "         1450: 125,\n",
       "         116: 1706,\n",
       "         1498: 119,\n",
       "         2256: 66,\n",
       "         1467: 122,\n",
       "         1476: 121,\n",
       "         5163: 21,\n",
       "         1435: 127,\n",
       "         395: 578,\n",
       "         4568: 25,\n",
       "         4214: 28,\n",
       "         3031: 44,\n",
       "         6719: 14,\n",
       "         4341: 27,\n",
       "         1009: 200,\n",
       "         716: 307,\n",
       "         1578: 112,\n",
       "         4616: 25,\n",
       "         738: 297,\n",
       "         5060: 22,\n",
       "         3857: 32,\n",
       "         1423: 128,\n",
       "         507: 439,\n",
       "         384: 605,\n",
       "         1281: 146,\n",
       "         615: 363,\n",
       "         1540: 115,\n",
       "         1026: 196,\n",
       "         899: 231,\n",
       "         715: 307,\n",
       "         602: 373,\n",
       "         290: 769,\n",
       "         349: 653,\n",
       "         737: 297,\n",
       "         614: 365,\n",
       "         850: 246,\n",
       "         452: 496,\n",
       "         552: 401,\n",
       "         170: 1196,\n",
       "         5668: 19,\n",
       "         2582: 56,\n",
       "         761: 283,\n",
       "         266: 842,\n",
       "         3244: 40,\n",
       "         941: 219,\n",
       "         242: 920,\n",
       "         1225: 155,\n",
       "         477: 467,\n",
       "         2575: 56,\n",
       "         24857: 1,\n",
       "         12612: 4,\n",
       "         7005: 13,\n",
       "         2673: 53,\n",
       "         4081: 30,\n",
       "         4131: 29,\n",
       "         1360: 135,\n",
       "         884: 235,\n",
       "         8520: 9,\n",
       "         13719: 4,\n",
       "         12262: 4,\n",
       "         2888: 48,\n",
       "         339: 665,\n",
       "         609: 368,\n",
       "         812: 259,\n",
       "         13737: 4,\n",
       "         419: 539,\n",
       "         434: 520,\n",
       "         277: 809,\n",
       "         295: 761,\n",
       "         92: 2248,\n",
       "         7315: 12,\n",
       "         3748: 33,\n",
       "         280: 798,\n",
       "         209: 1033,\n",
       "         2142: 72,\n",
       "         12833: 4,\n",
       "         390: 589,\n",
       "         473: 474,\n",
       "         248: 901,\n",
       "         1362: 135,\n",
       "         20092: 2,\n",
       "         747: 291,\n",
       "         218: 998,\n",
       "         5589: 19,\n",
       "         1475: 121,\n",
       "         2669: 53,\n",
       "         476: 469,\n",
       "         800: 264,\n",
       "         3522: 36,\n",
       "         6930: 14,\n",
       "         4296: 28,\n",
       "         3367: 38,\n",
       "         487: 459,\n",
       "         594: 378,\n",
       "         6929: 14,\n",
       "         14191: 3,\n",
       "         810: 260,\n",
       "         7949: 11,\n",
       "         1061: 187,\n",
       "         1158: 168,\n",
       "         3505: 36,\n",
       "         2918: 47,\n",
       "         3026: 45,\n",
       "         3218: 41,\n",
       "         559: 397,\n",
       "         2014: 78,\n",
       "         1129: 175,\n",
       "         7346: 12,\n",
       "         853: 245,\n",
       "         15507: 3,\n",
       "         581: 382,\n",
       "         103: 1911,\n",
       "         10487: 6,\n",
       "         1208: 157,\n",
       "         7764: 11,\n",
       "         719: 307,\n",
       "         13098: 4,\n",
       "         6696: 14,\n",
       "         166: 1226,\n",
       "         730: 301,\n",
       "         15939: 3,\n",
       "         6122: 17,\n",
       "         3272: 40,\n",
       "         94: 2181,\n",
       "         1159: 167,\n",
       "         793: 267,\n",
       "         4252: 28,\n",
       "         176: 1172,\n",
       "         1112: 177,\n",
       "         791: 267,\n",
       "         3520: 36,\n",
       "         2248: 66,\n",
       "         1327: 141,\n",
       "         8776: 9,\n",
       "         605: 370,\n",
       "         172: 1182,\n",
       "         1202: 158,\n",
       "         897: 232,\n",
       "         1516: 117,\n",
       "         1059: 188,\n",
       "         157: 1319,\n",
       "         ...})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_words = []\n",
    "# for x in x_train:\n",
    "#     train_words.extend(x)\n",
    "# test_words = []\n",
    "# for x in x_test:\n",
    "#     test_words.extend(x)\n",
    "# all_words = train_words\n",
    "# all_words.extend(test_words)\n",
    "# all_statistcs = Counter(all_words)\n",
    "# all_statistcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24452</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18567</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27222</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26864</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24794</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30980 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "1      11228\n",
       "53      4213\n",
       "352      647\n",
       "26      8451\n",
       "14     15015\n",
       "...      ...\n",
       "24452      1\n",
       "18567      2\n",
       "27222      1\n",
       "26864      1\n",
       "24794      1\n",
       "\n",
       "[30980 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.DataFrame.from_dict(dict(all_statistcs), orient = 'index')\n",
    "# df.to_excel('./results/words_dist2.xlsx', header=False, index=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8260, 360) (2066, 360)\n"
     ]
    }
   ],
   "source": [
    "trainX = tf.keras.preprocessing.sequence.pad_sequences(x_train,maxlen=max_length,padding='post',value=0)\n",
    "testX = tf.keras.preprocessing.sequence.pad_sequences(x_test,maxlen=max_length,padding='post',value=0)\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "do = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph execution\n",
    "### Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_id = Input(shape=(max_length,), dtype='int32', name='int_ids') # 輸入的api funvtion name ID\n",
    "int_ids = Masking(mask_value=0)(int_id)\n",
    "sent_emb = Embedding(max_words, hidden_dim,input_length=max_length\n",
    "                    ,trainable=True,name='glove_emb')(int_ids) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = GRU(int(hidden_dim/2),return_sequences=True,return_state=False,name='common_extract'\n",
    "                      ,trainable=True)(sent_emb)\n",
    "rnn = BatchNormalization(name='bn')(rnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil = TimeDistributed(Dense(1,activation='sigmoid',\n",
    "                             name='filter_out'),name='TD2')(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul = Multiply()([fil,sent_emb])\n",
    "clf = LSTM(int(hidden_dim/2),dropout=do,recurrent_dropout=do,name='lstm')(mul)\n",
    "clf = BatchNormalization(name='bn3')(clf)\n",
    "clf = Dense(max(y_train)+1,activation='softmax',name='clf')(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "int_ids (InputLayer)            [(None, 358)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 358)          0           int_ids[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "glove_emb (Embedding)           (None, 358, 128)     1280000     masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "common_extract (GRU)            (None, 358, 64)      37248       glove_emb[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 358, 64)      256         common_extract[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TD2 (TimeDistributed)           (None, 358, 1)       65          bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 358, 128)     0           TD2[0][0]                        \n",
      "                                                                 glove_emb[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 64)           49408       multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn3 (BatchNormalization)        (None, 64)           256         lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "clf (Dense)                     (None, 46)           2990        bn3[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 1,370,223\n",
      "Trainable params: 1,369,967\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=int_id, outputs = clf)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-189e30c4af05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     return K.sum(layer.output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# kk = tf.keras.backend.ea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TD2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       self._track_trackable(\n\u001b[1;32m    294\u001b[0m           self.optimizer, name='optimizer', overwrite=True)\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weight_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \"\"\"\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_bool_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_bool_casting\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m       \u001b[0;31m# Default: V1-style Graph execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_in_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using a `tf.Tensor` as a Python `bool`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_disallow_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_in_graph_mode\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    521\u001b[0m     raise errors.OperatorNotAllowedInGraphError(\n\u001b[1;32m    522\u001b[0m         \u001b[0;34m\"{} is not allowed in Graph execution. Use Eager execution or decorate\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \" this function with @tf.function.\".format(task))\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_disallow_bool_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function."
     ]
    }
   ],
   "source": [
    "# loss\n",
    "import keras.backend as K\n",
    "def custom_objective(layer):\n",
    "    return K.sum(layer.output)\n",
    "#     return K.sum(layer.output)\n",
    "# kk = tf.keras.backend.ea\n",
    "model.compile(loss=custom_objective(model.get_layer(name='TD2')),optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole model\n",
    "do = 0\n",
    "init = tensorflow.keras.initializers.Ones()\n",
    "class base_model(Model):\n",
    "    def __init__(self):\n",
    "        super(base_model, self).__init__()\n",
    "        self.mask = Masking(mask_value=0)\n",
    "        self.emb = Embedding(max_words, hidden_dim,input_length=max_length\n",
    "                    ,trainable=True,name='glove_emb')\n",
    "        self.rnn1 = GRU(int(hidden_dim/2),return_sequences=True,return_state=False,name='common_extract'\n",
    "                      ,trainable=True)\n",
    "        self.bn1 = BatchNormalization(name='bn1')\n",
    "        self.fil = Dense(1,activation='hard_sigmoid',kernel_initializer=init,bias_initializer=init,name='filter_out')\n",
    "        #self.fil = TimeDistributed(Dense(1,activation='sigmoid', name='filter_out'),name='TD2')\n",
    "        self.mul = Multiply()\n",
    "        self.rnn2 = Bidirectional(GRU(int(hidden_dim/2),dropout=do,recurrent_dropout=do,name='lstm'))\n",
    "        self.rnn3 = LSTM(int(hidden_dim/2))\n",
    "        self.bn2 = BatchNormalization(name='bn2')\n",
    "        self.out = Dense(max(y_train)+1,activation='softmax',name='clf')\n",
    "    def transform(self,x):\n",
    "        return tf.math.round(x)\n",
    "    def call(self,x):\n",
    "        x = self.mask(x)\n",
    "        x1 = self.emb(x)\n",
    "        x = self.rnn1(x1)\n",
    "        x = self.bn1(x)\n",
    "        y = self.fil(x)\n",
    "        y1 = self.transform(y)\n",
    "        x2 = self.mul([y1,x1])\n",
    "        x = self.rnn2(x2) #x\n",
    "        x = self.bn2(x)\n",
    "        y2 = self.out(x)\n",
    "        return y,y1,y2\n",
    "        #return y,y1,y2,x2\n",
    "        \n",
    "model = base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial1 model\n",
    "init_w = tensorflow.keras.initializers.Constant(value=0.9)\n",
    "init_b = tensorflow.keras.initializers.Constant(value=0.7)\n",
    "def onezero(x):\n",
    "    beta = 0.6#0.6 #0.6~1\n",
    "    z = tf.where(x>=1.0, x - x + 1.0, x)\n",
    "    y = tf.where(z<=0.0, z - z + 0.0, beta*z)\n",
    "    return y\n",
    "\n",
    "class base_model_1(Model):\n",
    "    def __init__(self):\n",
    "        super(base_model_1, self).__init__()\n",
    "        self.mask = Masking(mask_value=0)\n",
    "        self.emb = Embedding(max_words, hidden_dim,input_length=max_length\n",
    "                    ,trainable=True,name='glove_emb')\n",
    "        self.rnn1 = GRU(int(hidden_dim/2),return_sequences=True,return_state=False,name='common_extract'\n",
    "                      ,trainable=True)\n",
    "        self.att = Attention(name='selfatt')\n",
    "        self.bn1 = BatchNormalization(name='bn1')\n",
    "        #self.fil = Dense(1,activation='sigmoid',name='filter_out')\n",
    "        self.fil = TimeDistributed(Dense(1,activation=onezero,kernel_initializer=init_w,bias_initializer=init_b, name='filter_out'),name='TD2') #relu/linear/step function\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.mask(x)\n",
    "        x1 = self.emb(x)\n",
    "        x = self.att([x1,x1])\n",
    "        #x = self.rnn1(x1)\n",
    "        #x = self.bn1(x)\n",
    "        y = self.fil(x)\n",
    "        return x1,y\n",
    "\n",
    "model1 = base_model_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial2 model\n",
    "class base_model_2(Model):\n",
    "    def __init__(self):\n",
    "        super(base_model_2, self).__init__()\n",
    "        self.mul = Multiply()\n",
    "        self.rnn2 = Bidirectional(GRU(int(hidden_dim/2),dropout=do,recurrent_dropout=do,name='lstm'))\n",
    "        self.rnn3 = GRU(int(hidden_dim/2))\n",
    "        self.bn2 = BatchNormalization(name='bn2')\n",
    "        self.out = Dense(max(y_train)+1,activation='softmax',name='clf')\n",
    "\n",
    "    def call(self,x1,y1):\n",
    "        x2 = self.mul([y1,x1])\n",
    "        x = self.rnn3(x2) #x\n",
    "        x = self.bn2(x)\n",
    "        y2 = self.out(x)\n",
    "        return y2\n",
    "    \n",
    "model2 = base_model_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=264354, shape=(1, 358, 1), dtype=float32, numpy=\n",
       "array([[[0.50008374],\n",
       "        [0.50039524],\n",
       "        [0.50061977],\n",
       "        [0.50072163],\n",
       "        [0.5007394 ],\n",
       "        [0.50071514],\n",
       "        [0.5006769 ],\n",
       "        [0.50063914],\n",
       "        [0.50060797],\n",
       "        [0.5005844 ],\n",
       "        [0.50056773],\n",
       "        [0.50055635],\n",
       "        [0.5005488 ],\n",
       "        [0.5005439 ],\n",
       "        [0.50054073],\n",
       "        [0.50053877],\n",
       "        [0.5005376 ],\n",
       "        [0.5005368 ],\n",
       "        [0.5005363 ],\n",
       "        [0.500536  ],\n",
       "        [0.50053585],\n",
       "        [0.5005358 ],\n",
       "        [0.5005357 ],\n",
       "        [0.50053567],\n",
       "        [0.50053567],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ],\n",
       "        [0.5005356 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "# x = tf.random.uniform((1, max_length))\n",
    "# out1,out2,out3,out4 = model(x)\n",
    "# out1, out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #,reshuffle_each_iteration=True\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((trainX,y_train)).shuffle(trainX.shape[0]).batch(batch_size)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((testX,y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_object1(predictions):\n",
    "    mask = tf.math.logical_not(tf.math.equal(predictions, 0))\n",
    "    loss_ = tf.reduce_mean(predictions)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "def one_percentage(predictions):\n",
    "    mask = tf.math.logical_not(tf.math.equal(predictions, 0))\n",
    "    loss_ = tf.reduce_mean(predictions)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "loss_object2 = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer1 = tf.keras.optimizers.Nadam()\n",
    "optimizer2 = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_ones = tf.keras.metrics.Mean(name='train_ones')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_ones = tf.keras.metrics.Mean(name='test_ones')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate partial model\n",
    "alpha = 0.0 #pahse1: -0.1 / 0.0 ; phase2: 0.01~0.05~0.1\n",
    "@tf.function\n",
    "def train_step(x,yc):\n",
    "    with tf.GradientTape(persistent=False) as tape:\n",
    "        emb, pred_imp = model1(x)\n",
    "        #loss1 = alpha*loss_object1(pred_imp) #phase1\n",
    "        #pred_imp2 = tf.math.round(pred_imp)\n",
    "        #pred_imp3 = tf.clip_by_value(pred_imp,clip_value_max=1,clip_value_min=0)\n",
    "        pred_imp2 = tf.math.round(pred_imp)\n",
    "        loss1 = alpha*(1-loss_object1(pred_imp)) #pahse2:alpha*loss_object1(pred_imp) ; phase1: alpha*(1-loss_object1(pred_imp))\n",
    "        pred_cat = model2(emb,pred_imp) #pahse1: pred_imp; phase2; pred_imp2\n",
    "        loss2 = loss_object2(yc, pred_cat)\n",
    "        loss = loss1 + loss2\n",
    "    trainable_variable = model1.trainable_variables\n",
    "    trainable_variable.extend(model2.trainable_variables)\n",
    "    gradients = tape.gradient(loss,trainable_variable)\n",
    "    optimizer1.apply_gradients(zip(gradients,trainable_variable))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(yc, pred_cat)\n",
    "    ones = one_percentage(pred_imp)\n",
    "    train_ones(ones)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(x,yc):\n",
    "    emb, pred_imp = model1(x)\n",
    "    #loss1 = alpha*loss_object1(pred_imp) #phase1\n",
    "    #pred_imp2 = tf.math.round(pred_imp)\n",
    "    #pred_imp3 = tf.clip_by_value(pred_imp,clip_value_max=1,clip_value_min=0)\n",
    "    pred_imp2 = tf.math.round(pred_imp)\n",
    "    loss1 = alpha*(1-loss_object1(pred_imp)) #phase2\n",
    "    pred_cat = model2(emb,pred_imp) #phase1: pred_imp ; phase2:pred_imp2\n",
    "    loss2 = loss_object2(yc, pred_cat)\n",
    "    #t_loss = loss1 + loss2\n",
    "    t_loss = loss1 + loss2\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(yc, pred_cat)\n",
    "    t_ones = one_percentage(pred_imp)\n",
    "    test_ones(t_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AIO\n",
    "alpha = 0.1\n",
    "@tf.function\n",
    "def train_step(x,yc):\n",
    "    with tf.GradientTape(persistent=True) as tape: #persistent=True\n",
    "        pred_imp,pred_round , pred_cat = model(x)\n",
    "#         pred_cat = model(x)\n",
    "#         loss = alpha*loss_object1(pred_imp) + loss_object2(yc,pred_cat)\n",
    "        loss1 = alpha*loss_object1(pred_imp)\n",
    "        loss2 = loss_object2(yc,pred_cat)\n",
    "        loss = loss_object2(yc,pred_cat)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    grad1 = tape.gradient(loss1, model.trainable_variables)\n",
    "    grad2 = tape.gradient(loss2, model.trainable_variables)\n",
    "#     optimizer1.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    optimizer1.apply_gradients(zip(grad1, model.trainable_variables))\n",
    "    optimizer2.apply_gradients(zip(grad2, model.trainable_variables))\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         pred_imp , pred_cat = model(x)\n",
    "#         loss2 = loss_object2(yc,pred_cat)\n",
    "#         loss = alpha*loss_object1(pred_imp) + loss_object2(yc,pred_cat)\n",
    "#     grad2 = tape.gradient(loss2, model.trainable_variables)\n",
    "#     optimizer2.apply_gradients(zip(grad2, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(yc, pred_cat)\n",
    "    ones = one_percentage(pred_round)\n",
    "    train_ones(ones)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(x,yc):\n",
    "    pred_imp,pred_round, pred_cat = model(x)\n",
    "#     pred_cat = model(x)\n",
    "    t_loss = alpha*loss_object1(pred_imp) + loss_object2(yc,pred_cat) \n",
    "#     t_loss = loss_object2(yc,pred_cat)\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(yc, pred_cat)\n",
    "    t_ones = one_percentage(pred_round)\n",
    "    test_ones(t_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.5489280223846436,Ones#: 0.586088240146637, Accuracy: 37.16706848144531, Test Loss: 2.3674263954162598,Test Ones#: 0.6000000238418579, Test Accuracy: 37.173282623291016\n",
      "Epoch 2, Loss: 2.3109850883483887,Ones#: 0.5999999642372131, Accuracy: 37.772396087646484, Test Loss: 2.138742685317993,Test Ones#: 0.6000000238418579, Test Accuracy: 37.94772720336914\n",
      "Epoch 3, Loss: 1.917340874671936,Ones#: 0.5999906659126282, Accuracy: 52.8087158203125, Test Loss: 1.852415680885315,Test Ones#: 0.5999950766563416, Test Accuracy: 55.03388214111328\n",
      "Epoch 4, Loss: 1.7821989059448242,Ones#: 0.5995811223983765, Accuracy: 55.375301361083984, Test Loss: 1.7438809871673584,Test Ones#: 0.5996185541152954, Test Accuracy: 56.4859619140625\n",
      "Epoch 5, Loss: 1.6110013723373413,Ones#: 0.5996609926223755, Accuracy: 58.849876403808594, Test Loss: 1.6141921281814575,Test Ones#: 0.5997176170349121, Test Accuracy: 59.34172439575195\n",
      "Epoch 6, Loss: 1.6280409097671509,Ones#: 0.5994797945022583, Accuracy: 57.82081985473633, Test Loss: 1.583901286125183,Test Ones#: 0.5993664264678955, Test Accuracy: 60.60019302368164\n",
      "Epoch 7, Loss: 1.516135573387146,Ones#: 0.5990171432495117, Accuracy: 60.8958854675293, Test Loss: 1.6165978908538818,Test Ones#: 0.5995772480964661, Test Accuracy: 60.30978012084961\n",
      "Epoch 8, Loss: 1.4892784357070923,Ones#: 0.5997395515441895, Accuracy: 61.7070198059082, Test Loss: 1.5757379531860352,Test Ones#: 0.599967360496521, Test Accuracy: 60.648597717285156\n",
      "Epoch 9, Loss: 1.4979020357131958,Ones#: 0.5998166799545288, Accuracy: 61.24697494506836, Test Loss: 1.693110466003418,Test Ones#: 0.5999489426612854, Test Accuracy: 59.535335540771484\n",
      "Epoch 10, Loss: 1.456608772277832,Ones#: 0.5997902154922485, Accuracy: 63.11138153076172, Test Loss: 1.5502153635025024,Test Ones#: 0.5999650955200195, Test Accuracy: 62.149078369140625\n",
      "Epoch 11, Loss: 1.3909868001937866,Ones#: 0.5997692942619324, Accuracy: 65.02421569824219, Test Loss: 1.53125,Test Ones#: 0.599947988986969, Test Accuracy: 62.342689514160156\n",
      "Epoch 12, Loss: 1.3301855325698853,Ones#: 0.5997765064239502, Accuracy: 66.34382629394531, Test Loss: 1.5215635299682617,Test Ones#: 0.5999607443809509, Test Accuracy: 63.891578674316406\n",
      "Epoch 13, Loss: 1.267564296722412,Ones#: 0.599845290184021, Accuracy: 68.22034454345703, Test Loss: 1.5738954544067383,Test Ones#: 0.5999558568000793, Test Accuracy: 62.100677490234375\n",
      "Epoch 14, Loss: 1.2345975637435913,Ones#: 0.5998458862304688, Accuracy: 68.47457885742188, Test Loss: 1.507110595703125,Test Ones#: 0.5999470949172974, Test Accuracy: 65.0532455444336\n",
      "Epoch 15, Loss: 1.173998236656189,Ones#: 0.5997524261474609, Accuracy: 70.03632354736328, Test Loss: 1.501142978668213,Test Ones#: 0.5999118685722351, Test Accuracy: 64.61761474609375\n",
      "Epoch 16, Loss: 1.5357475280761719,Ones#: 0.5997959971427917, Accuracy: 62.6755485534668, Test Loss: 2.022475004196167,Test Ones#: 0.5999401807785034, Test Accuracy: 50.91965103149414\n",
      "Epoch 17, Loss: 1.7836743593215942,Ones#: 0.5997273325920105, Accuracy: 55.6779670715332, Test Loss: 1.635435938835144,Test Ones#: 0.5999561548233032, Test Accuracy: 58.03485107421875\n",
      "Epoch 18, Loss: 1.4985947608947754,Ones#: 0.5997524857521057, Accuracy: 63.171913146972656, Test Loss: 1.489585280418396,Test Ones#: 0.5999290943145752, Test Accuracy: 63.93998336791992\n",
      "Epoch 19, Loss: 1.3359882831573486,Ones#: 0.5997687578201294, Accuracy: 66.94915008544922, Test Loss: 1.377256989479065,Test Ones#: 0.5999543070793152, Test Accuracy: 66.60212707519531\n",
      "Epoch 20, Loss: 1.222102165222168,Ones#: 0.5997722744941711, Accuracy: 69.62469482421875, Test Loss: 1.3446699380874634,Test Ones#: 0.5999484062194824, Test Accuracy: 67.47337341308594\n",
      "Epoch 21, Loss: 1.1372849941253662,Ones#: 0.599772036075592, Accuracy: 71.53752899169922, Test Loss: 1.2640472650527954,Test Ones#: 0.5999506711959839, Test Accuracy: 70.52275085449219\n",
      "Epoch 22, Loss: 1.0355618000030518,Ones#: 0.5997854471206665, Accuracy: 73.4382553100586, Test Loss: 1.2414615154266357,Test Ones#: 0.599943995475769, Test Accuracy: 70.23233032226562\n",
      "Epoch 23, Loss: 0.9543972015380859,Ones#: 0.5997901558876038, Accuracy: 75.4237289428711, Test Loss: 1.2702765464782715,Test Ones#: 0.5999694466590881, Test Accuracy: 70.86156463623047\n",
      "Epoch 24, Loss: 0.8773307204246521,Ones#: 0.5997776985168457, Accuracy: 77.5907974243164, Test Loss: 1.1992312669754028,Test Ones#: 0.5999491214752197, Test Accuracy: 72.70086669921875\n",
      "Epoch 25, Loss: 0.7920132279396057,Ones#: 0.5997529625892639, Accuracy: 80.38740539550781, Test Loss: 1.225521206855774,Test Ones#: 0.5999419093132019, Test Accuracy: 72.60406494140625\n",
      "Epoch 26, Loss: 0.7512209415435791,Ones#: 0.5997502207756042, Accuracy: 80.84745788574219, Test Loss: 1.1945242881774902,Test Ones#: 0.5999385118484497, Test Accuracy: 73.5721206665039\n",
      "Epoch 27, Loss: 0.6841915249824524,Ones#: 0.5997752547264099, Accuracy: 83.00242614746094, Test Loss: 1.2024704217910767,Test Ones#: 0.5999695658683777, Test Accuracy: 73.62052154541016\n",
      "Epoch 28, Loss: 0.6237373948097229,Ones#: 0.599798858165741, Accuracy: 84.24939727783203, Test Loss: 1.1829288005828857,Test Ones#: 0.5999526977539062, Test Accuracy: 74.78218841552734\n",
      "Epoch 29, Loss: 0.5707201361656189,Ones#: 0.5997782945632935, Accuracy: 85.70217895507812, Test Loss: 1.1724278926849365,Test Ones#: 0.5999533534049988, Test Accuracy: 75.60503387451172\n",
      "Epoch 30, Loss: 0.5346907377243042,Ones#: 0.5997775793075562, Accuracy: 86.67070007324219, Test Loss: 1.1693079471588135,Test Ones#: 0.5999563336372375, Test Accuracy: 75.89545440673828\n",
      "Epoch 31, Loss: 0.4838271737098694,Ones#: 0.59978187084198, Accuracy: 87.97821044921875, Test Loss: 1.1627697944641113,Test Ones#: 0.5999431610107422, Test Accuracy: 76.57308959960938\n",
      "Epoch 32, Loss: 0.45107609033584595,Ones#: 0.5997815728187561, Accuracy: 88.48668670654297, Test Loss: 1.1532526016235352,Test Ones#: 0.599952220916748, Test Accuracy: 76.7667007446289\n",
      "Epoch 33, Loss: 0.4137265086174011,Ones#: 0.5997828841209412, Accuracy: 89.6004867553711, Test Loss: 1.2180265188217163,Test Ones#: 0.5999655723571777, Test Accuracy: 76.04065704345703\n",
      "Epoch 34, Loss: 0.38036495447158813,Ones#: 0.5997726321220398, Accuracy: 90.37530517578125, Test Loss: 1.1838213205337524,Test Ones#: 0.5999565720558167, Test Accuracy: 77.00871276855469\n",
      "Epoch 35, Loss: 0.3467230200767517,Ones#: 0.5997712016105652, Accuracy: 91.4527816772461, Test Loss: 1.1895478963851929,Test Ones#: 0.5999337434768677, Test Accuracy: 77.15392303466797\n",
      "Epoch 36, Loss: 0.31763172149658203,Ones#: 0.5997533798217773, Accuracy: 91.9854736328125, Test Loss: 1.2460353374481201,Test Ones#: 0.5999369621276855, Test Accuracy: 76.62149047851562\n",
      "Epoch 37, Loss: 0.2934723198413849,Ones#: 0.5997558236122131, Accuracy: 92.91767120361328, Test Loss: 1.2435965538024902,Test Ones#: 0.5999298691749573, Test Accuracy: 76.7667007446289\n",
      "Epoch 38, Loss: 0.2709498405456543,Ones#: 0.599739134311676, Accuracy: 93.2082290649414, Test Loss: 1.2536463737487793,Test Ones#: 0.5999065041542053, Test Accuracy: 77.63794708251953\n",
      "Epoch 39, Loss: 0.2554786503314972,Ones#: 0.5997327566146851, Accuracy: 93.65617370605469, Test Loss: 1.2488963603973389,Test Ones#: 0.5999171137809753, Test Accuracy: 76.52468872070312\n",
      "Epoch 40, Loss: 0.2386661022901535,Ones#: 0.5997314453125, Accuracy: 93.80145263671875, Test Loss: 1.2993087768554688,Test Ones#: 0.5998762845993042, Test Accuracy: 77.25072479248047\n",
      "Epoch 41, Loss: 0.21867816150188446,Ones#: 0.5997315645217896, Accuracy: 94.07990264892578, Test Loss: 1.2942649126052856,Test Ones#: 0.5998855233192444, Test Accuracy: 77.15392303466797\n",
      "Epoch 42, Loss: 0.20572416484355927,Ones#: 0.5997101068496704, Accuracy: 94.12832641601562, Test Loss: 1.486391305923462,Test Ones#: 0.5998734831809998, Test Accuracy: 74.92739868164062\n",
      "Epoch 43, Loss: 0.19760368764400482,Ones#: 0.5997219085693359, Accuracy: 94.6004867553711, Test Loss: 1.3218873739242554,Test Ones#: 0.599868893623352, Test Accuracy: 78.17037963867188\n",
      "Epoch 44, Loss: 0.1874484419822693,Ones#: 0.5997235178947449, Accuracy: 94.75786590576172, Test Loss: 1.324676513671875,Test Ones#: 0.5998674035072327, Test Accuracy: 76.57308959960938\n",
      "Epoch 45, Loss: 0.18579694628715515,Ones#: 0.5997254848480225, Accuracy: 94.6731185913086, Test Loss: 1.340539574623108,Test Ones#: 0.5998742580413818, Test Accuracy: 77.39593505859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Loss: 0.17085577547550201,Ones#: 0.5997239351272583, Accuracy: 94.89104461669922, Test Loss: 1.3644200563430786,Test Ones#: 0.5998729467391968, Test Accuracy: 77.87995910644531\n",
      "Epoch 47, Loss: 0.15510308742523193,Ones#: 0.5989909768104553, Accuracy: 95.0, Test Loss: 1.3750325441360474,Test Ones#: 0.596525251865387, Test Accuracy: 77.15392303466797\n",
      "Epoch 48, Loss: 0.152989000082016,Ones#: 0.5964509844779968, Accuracy: 95.16948699951172, Test Loss: 1.3945766687393188,Test Ones#: 0.5965962409973145, Test Accuracy: 77.39593505859375\n",
      "Epoch 49, Loss: 0.1406218707561493,Ones#: 0.5964705944061279, Accuracy: 95.27845001220703, Test Loss: 1.433890700340271,Test Ones#: 0.5966086387634277, Test Accuracy: 77.10551452636719\n",
      "Epoch 50, Loss: 0.13099265098571777,Ones#: 0.5964703559875488, Accuracy: 95.32687377929688, Test Loss: 1.4065110683441162,Test Ones#: 0.5966078639030457, Test Accuracy: 77.15392303466797\n",
      "Epoch 51, Loss: 0.12933249771595,Ones#: 0.5964680910110474, Accuracy: 95.37530517578125, Test Loss: 1.4408434629440308,Test Ones#: 0.5966120958328247, Test Accuracy: 76.8635025024414\n",
      "Epoch 52, Loss: 0.12325600534677505,Ones#: 0.5964735150337219, Accuracy: 95.38741302490234, Test Loss: 1.419913649559021,Test Ones#: 0.59660404920578, Test Accuracy: 77.54114532470703\n",
      "Epoch 53, Loss: 0.1214735209941864,Ones#: 0.5964678525924683, Accuracy: 95.58111572265625, Test Loss: 1.448009729385376,Test Ones#: 0.5965938568115234, Test Accuracy: 77.15392303466797\n",
      "Epoch 54, Loss: 0.1149945929646492,Ones#: 0.5964562892913818, Accuracy: 95.55690002441406, Test Loss: 1.5180904865264893,Test Ones#: 0.5965871214866638, Test Accuracy: 77.00871276855469\n",
      "Epoch 55, Loss: 0.11524510383605957,Ones#: 0.5964473485946655, Accuracy: 95.36319732666016, Test Loss: 1.4955487251281738,Test Ones#: 0.596589982509613, Test Accuracy: 78.0251693725586\n",
      "Epoch 56, Loss: 0.12930384278297424,Ones#: 0.5964522957801819, Accuracy: 95.30266571044922, Test Loss: 1.4931327104568481,Test Ones#: 0.5965865850448608, Test Accuracy: 77.25072479248047\n",
      "Epoch 57, Loss: 0.11951397359371185,Ones#: 0.5964145660400391, Accuracy: 95.205810546875, Test Loss: 1.5513020753860474,Test Ones#: 0.5963242053985596, Test Accuracy: 76.6698989868164\n",
      "Epoch 58, Loss: 0.11705528199672699,Ones#: 0.5962832570075989, Accuracy: 95.32687377929688, Test Loss: 1.4925620555877686,Test Ones#: 0.5963955521583557, Test Accuracy: 77.68634796142578\n",
      "Epoch 59, Loss: 0.1070137619972229,Ones#: 0.5962749719619751, Accuracy: 95.49636840820312, Test Loss: 1.5518102645874023,Test Ones#: 0.5963907241821289, Test Accuracy: 76.57308959960938\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5000\n",
    "gc.collect()\n",
    "for epoch in range(EPOCHS):\n",
    "    for text, labels in train_ds:\n",
    "        train_step(text, labels)\n",
    "\n",
    "    for test_text, test_labels in valid_ds:\n",
    "        test_step(test_text, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {},Ones#: {}, Accuracy: {}, Test Loss: {},Test Ones#: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                        train_loss.result(),\n",
    "                        train_ones.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_ones.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_ones.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_ones.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 不用sigmoid或是hard_sigmoid。改良relu + linear，並拆成兩個model，把round前面多加上clip\n",
    "    * sigmoid中間的變化太快(一瞬間就會掉到0或是1)，改成relu在>0~無限大(linear為了還在0~1)再去clip再round，可以看到每個epoch的變化\n",
    "* 1st phase的beta一定要>=0.6否則不會動，ones#都會是0\n",
    "    * beta=1 (放0 1進去): weight init設成1也沒用，但把bias設成1就會一開始都是ones#=1了。weight=1 bias=0.2都匯市0，bias=0.3會是0.87。0.6/0.3都是0。0.8/0.3差不多是0.5(但是train很慢acc進步很慢就是了)。0.5/0.5是從0.01開始往上升 (0 1放進去會比較難train是因為它的變化量太大，一下就是有或沒有，所以clf可能學不好，但如果是weight每次gradient進步的都是一小點就會比較容易上升)\n",
    "    * beta=0.6 (放0 1進去): 0.8/0.5都是0。0.9/0.8 從0.9一直到0。0.9/0.6差不多是從0.5但又有時候到0.7都是0.0(很難train，Nadam換個opt有時候沒用。EX變成adam 0.9/0.8才有0.96開始但如果0.9/0.75變成0開始。Rmsprop 0.9/0.8又是從0.95開始往下)。但如果都改成傳入weight就都沒問題。ones一開始大概0.5 weight平均，也不會卡住\n",
    "* 建議: 先訓練embedding weight matrix，但是要看goal是要怎樣的matrix\n",
    "* 若設定alpha，就像是regularizer term (penalty)，設越大drop越多\n",
    "* 一開始先很多ones，再越來越少個"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 同一個opt若加入transform就會train不起來\n",
    "* 兩個不同的opt加入transform也會train不起來 (persistent、non-persis都不行)，且與BN無關"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
