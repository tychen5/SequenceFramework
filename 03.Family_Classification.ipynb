{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.text import *\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import gc, math\n",
    "from math import log, floor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import *\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1377 #dict size (tri-gram總數量)\n",
    "max_length = 182 #tri-gram num (最靠近outlier數字的斜率變化轉折點) 超過此數量的hkl拿掉\n",
    "min_length = 35 #Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 28.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2365 2365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"./data/Hooklog/trace_picked_fam/\"\n",
    "fam_dirs = next(os.walk(root_dir))[1]\n",
    "trainX = []\n",
    "trainY = []\n",
    "for fam in tqdm(fam_dirs):\n",
    "    in_directory = root_dir + fam + '/'\n",
    "    hl_list = next(os.walk(in_directory))[2] # get all filenames in the in_directory\n",
    "    hl_list = [os.path.join(in_directory, f) for f in hl_list] # filepathname list\n",
    "    hl_list = list(filter(lambda f: f.endswith(\".enc.npy\"), hl_list)) # in case some non-hooklog file in the folder\n",
    "    for npy in hl_list:\n",
    "        tri_enc = np.load(npy)\n",
    "        if len(tri_enc)>max_length:\n",
    "            tri_enc = tri_enc[:max_length]\n",
    "        if len(tri_enc)<min_length:\n",
    "            continue\n",
    "        trainY.append(fam)\n",
    "        trainX.append(tri_enc)\n",
    "print(len(trainY),len(trainX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 13, 13, ...,  9,  9,  9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fam_code = {}\n",
    "for i,v in enumerate(list(set(trainY))):\n",
    "    fam_code[v] = i\n",
    "# fam_code = pickle.load(file=open(root_dir+\"fam_dict_enc.pkl\",'rb'))\n",
    "train_Y = []\n",
    "for fam in trainY:\n",
    "    train_Y.append(fam_code[fam])\n",
    "train_Y = np.array(train_Y)\n",
    "pickle.dump(obj=fam_code,file=open(root_dir+\"fam_dict_enc.pkl\",'wb'))\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2365, 182) (2365,)\n"
     ]
    }
   ],
   "source": [
    "train_X = tf.keras.preprocessing.sequence.pad_sequences(trainX,maxlen=max_length,padding='post',value=0)\n",
    "print(train_X.shape, train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_valid_set(X_all, Y_all, percentage):\n",
    "    all_data_size = len(X_all)\n",
    "    valid_data_size = int(floor(all_data_size * percentage))\n",
    "\n",
    "    X_all, Y_all = _shuffle(X_all, Y_all)\n",
    "\n",
    "    X_train, Y_train = X_all[0:valid_data_size], Y_all[0:valid_data_size]\n",
    "    X_valid, Y_valid = X_all[valid_data_size:], Y_all[valid_data_size:]\n",
    "\n",
    "    return X_train, Y_train, X_valid, Y_valid\n",
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], Y[randomize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2128, 182) (2128,) (237, 182) (237,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_valid_set(train_X, train_Y, 0.9)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "assert len(set(y_test)) == len(set(y_train))\n",
    "print(set(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for Mike ML data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [00:00<00:00, 981.62it/s]\n",
      "100%|██████████| 2128/2128 [00:01<00:00, 1069.03it/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_onehot(X_train=X_train):\n",
    "    \"\"\"\n",
    "    input: padded numpy array encode by int\n",
    "    output: one hot list(samples) of list(tri-gram terms)\n",
    "    \"\"\"\n",
    "    X_train_li = []\n",
    "    for arr_npy in tqdm(X_train):\n",
    "        arr_li = []\n",
    "        for element in arr_npy:\n",
    "            init_npy = np.zeros((max_words,))\n",
    "            one_hot = init_npy\n",
    "            if element ==0:\n",
    "                arr_li.append(init_npy)\n",
    "                continue\n",
    "            one_hot[element] = 1\n",
    "            arr_li.append(one_hot)\n",
    "        X_train_li.append(arr_li)\n",
    "    return X_train_li\n",
    "X_test_li = convert_onehot(X_test)\n",
    "X_train_li = convert_onehot(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "237it [00:07, 30.39it/s]\n",
      "2128it [01:35, 22.28it/s]\n"
     ]
    }
   ],
   "source": [
    "def conver2df(X_test_li=X_test_li, y_test=y_test):\n",
    "    \"\"\"\n",
    "    Input: output of convert_onehot function &　it;s corresponding label\n",
    "    Output: df\n",
    "    \"\"\"\n",
    "    col = ['word_'+str(i) for i in range(max_length)]\n",
    "    col.append('LABEL')\n",
    "    df = pd.DataFrame(columns=col)\n",
    "    for one_hot_li, label in tqdm(zip(X_test_li, y_test)):\n",
    "        temp = [list(element) for element in one_hot_li]\n",
    "        temp.extend([label])\n",
    "        temp = pd.Series(temp,col)\n",
    "        df = df.append(temp,ignore_index=True)\n",
    "    return df\n",
    "test_df = conver2df(X_test_li,y_test)\n",
    "train_df = conver2df(X_train_li,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_0</th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>word_5</th>\n",
       "      <th>word_6</th>\n",
       "      <th>word_7</th>\n",
       "      <th>word_8</th>\n",
       "      <th>word_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_173</th>\n",
       "      <th>word_174</th>\n",
       "      <th>word_175</th>\n",
       "      <th>word_176</th>\n",
       "      <th>word_177</th>\n",
       "      <th>word_178</th>\n",
       "      <th>word_179</th>\n",
       "      <th>word_180</th>\n",
       "      <th>word_181</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows × 183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                word_0  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                word_1  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                word_2  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                word_3  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                word_4  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                word_5  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                word_6  \\\n",
       "0    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                word_7  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                word_8  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                word_9  ...  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  ...   \n",
       "..                                                 ...  ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  ...   \n",
       "\n",
       "                                              word_173  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              word_174  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              word_175  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              word_176  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              word_177  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              word_178  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              word_179  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              word_180  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                 ...   \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "235  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              word_181 LABEL  \n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     5  \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    11  \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     4  \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     9  \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     8  \n",
       "..                                                 ...   ...  \n",
       "232  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     9  \n",
       "233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     0  \n",
       "234  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     3  \n",
       "235  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     2  \n",
       "236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     9  \n",
       "\n",
       "[237 rows x 183 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.to_csv(\"./data/Hooklog/trace_picked_fam/testing_onehot.csv\",index=False)\n",
    "train_df.to_csv(\"./data/Hooklog/trace_picked_fam/training_onehot.csv\",index=False)\n",
    "test_df = pd.read_csv(\"./data/Hooklog/trace_picked_fam/testing_onehot.csv\")\n",
    "test_df #check saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128#validXr.shape[-1] \n",
    "max_length = X_test.shape[-1] # max sequence length\n",
    "cat_num = y_test.shape[0]\n",
    "do = 0.1\n",
    "vocabulary_size = max_words+1\n",
    "hidden_dim = 64\n",
    "\n",
    "# traina = True #改\n",
    "batch_size = 128 #改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_w = tensorflow.keras.initializers.Constant(value=0.9) #portyion=0.6, w=0.9, b = 0.8-0.85 (0.83從0開始)\n",
    "init_w = tensorflow.keras.initializers.RandomNormal(mean=0.9,stddev=0.06)\n",
    "# init_b = tensorflow.keras.initializers.Constant(value=0.84) #w=1 ; b=0.499, portion=1\n",
    "init_b = tensorflow.keras.initializers.RandomNormal(mean=0.7,stddev=0.2)\n",
    "def onezero(x):\n",
    "    portion = 1#0.6#0.6 #0.6~1\n",
    "    z = tf.where(x>=1.0, x - x + 1.0, x)\n",
    "    y = tf.where(z<=0.0, z - z + 0.0, portion*z)\n",
    "    return y\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train,y_train)).shuffle(X_train.shape[0],reshuffle_each_iteration=True).batch(batch_size)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Total Loss: 2.5643908977508545, Accuracy Rate: 14.10%,            Test_Total_Loss: 2.498419761657715, Test_Accuracy_Rate: 16.88%\n",
      "Epoch 2, Total Loss: 2.4464213848114014, Accuracy Rate: 18.23%,            Test_Total_Loss: 2.473076581954956, Test_Accuracy_Rate: 17.30%\n",
      "Epoch 3, Total Loss: 2.411496162414551, Accuracy Rate: 20.07%,            Test_Total_Loss: 2.443120002746582, Test_Accuracy_Rate: 22.36%\n",
      "Epoch 4, Total Loss: 2.380523443222046, Accuracy Rate: 21.85%,            Test_Total_Loss: 2.4430036544799805, Test_Accuracy_Rate: 24.05%\n",
      "Epoch 5, Total Loss: 2.3474206924438477, Accuracy Rate: 22.89%,            Test_Total_Loss: 2.4296228885650635, Test_Accuracy_Rate: 24.05%\n",
      "Epoch 6, Total Loss: 2.3184473514556885, Accuracy Rate: 23.92%,            Test_Total_Loss: 2.433103561401367, Test_Accuracy_Rate: 20.68%\n",
      "Epoch 7, Total Loss: 2.28834867477417, Accuracy Rate: 24.86%,            Test_Total_Loss: 2.437082529067993, Test_Accuracy_Rate: 19.83%\n",
      "Epoch 8, Total Loss: 2.2656795978546143, Accuracy Rate: 26.55%,            Test_Total_Loss: 2.447564125061035, Test_Accuracy_Rate: 24.05%\n",
      "Epoch 9, Total Loss: 2.2374415397644043, Accuracy Rate: 26.93%,            Test_Total_Loss: 2.418802261352539, Test_Accuracy_Rate: 24.05%\n",
      "Epoch 10, Total Loss: 2.210339307785034, Accuracy Rate: 28.29%,            Test_Total_Loss: 2.442265033721924, Test_Accuracy_Rate: 19.83%\n",
      "Epoch 11, Total Loss: 2.1933329105377197, Accuracy Rate: 27.82%,            Test_Total_Loss: 2.4393062591552734, Test_Accuracy_Rate: 21.52%\n",
      "Epoch 12, Total Loss: 2.1757431030273438, Accuracy Rate: 28.57%,            Test_Total_Loss: 2.4065191745758057, Test_Accuracy_Rate: 25.74%\n",
      "Epoch 13, Total Loss: 2.153245210647583, Accuracy Rate: 29.51%,            Test_Total_Loss: 2.4309465885162354, Test_Accuracy_Rate: 26.58%\n",
      "Epoch 14, Total Loss: 2.1440560817718506, Accuracy Rate: 29.32%,            Test_Total_Loss: 2.4428181648254395, Test_Accuracy_Rate: 21.52%\n",
      "Epoch 15, Total Loss: 2.1260881423950195, Accuracy Rate: 31.63%,            Test_Total_Loss: 2.43585467338562, Test_Accuracy_Rate: 22.78%\n",
      "Epoch 16, Total Loss: 2.1209425926208496, Accuracy Rate: 32.33%,            Test_Total_Loss: 2.4116876125335693, Test_Accuracy_Rate: 25.74%\n",
      "Epoch 17, Total Loss: 2.0984573364257812, Accuracy Rate: 31.67%,            Test_Total_Loss: 2.4487290382385254, Test_Accuracy_Rate: 20.25%\n",
      "Epoch 18, Total Loss: 2.1034741401672363, Accuracy Rate: 31.77%,            Test_Total_Loss: 2.481529712677002, Test_Accuracy_Rate: 25.74%\n",
      "Epoch 19, Total Loss: 2.09477162361145, Accuracy Rate: 30.97%,            Test_Total_Loss: 2.461982488632202, Test_Accuracy_Rate: 26.58%\n",
      "Epoch 20, Total Loss: 2.0649518966674805, Accuracy Rate: 33.04%,            Test_Total_Loss: 2.451098680496216, Test_Accuracy_Rate: 25.32%\n",
      "Epoch 21, Total Loss: 2.0307822227478027, Accuracy Rate: 35.10%,            Test_Total_Loss: 2.4540412425994873, Test_Accuracy_Rate: 25.74%\n",
      "Epoch 22, Total Loss: 2.015225887298584, Accuracy Rate: 36.28%,            Test_Total_Loss: 2.4761078357696533, Test_Accuracy_Rate: 26.58%\n",
      "Epoch 23, Total Loss: 1.983664631843567, Accuracy Rate: 37.41%,            Test_Total_Loss: 2.4202566146850586, Test_Accuracy_Rate: 27.85%\n",
      "Epoch 24, Total Loss: 1.9689971208572388, Accuracy Rate: 37.31%,            Test_Total_Loss: 2.4302749633789062, Test_Accuracy_Rate: 27.85%\n",
      "Epoch 25, Total Loss: 1.9390348196029663, Accuracy Rate: 38.25%,            Test_Total_Loss: 2.585771083831787, Test_Accuracy_Rate: 20.25%\n",
      "Epoch 26, Total Loss: 1.9734902381896973, Accuracy Rate: 37.08%,            Test_Total_Loss: 2.4512124061584473, Test_Accuracy_Rate: 28.27%\n",
      "Epoch 27, Total Loss: 1.8990014791488647, Accuracy Rate: 40.18%,            Test_Total_Loss: 2.4490528106689453, Test_Accuracy_Rate: 26.58%\n",
      "Epoch 28, Total Loss: 1.8638546466827393, Accuracy Rate: 40.65%,            Test_Total_Loss: 2.43570876121521, Test_Accuracy_Rate: 29.54%\n",
      "Epoch 29, Total Loss: 1.847617745399475, Accuracy Rate: 42.72%,            Test_Total_Loss: 2.4786112308502197, Test_Accuracy_Rate: 29.11%\n",
      "Epoch 30, Total Loss: 1.8574010133743286, Accuracy Rate: 41.02%,            Test_Total_Loss: 2.426866292953491, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 31, Total Loss: 1.8062372207641602, Accuracy Rate: 43.23%,            Test_Total_Loss: 2.5190889835357666, Test_Accuracy_Rate: 25.74%\n",
      "Epoch 32, Total Loss: 1.799747347831726, Accuracy Rate: 42.67%,            Test_Total_Loss: 2.463944911956787, Test_Accuracy_Rate: 25.74%\n",
      "Epoch 33, Total Loss: 1.7756520509719849, Accuracy Rate: 44.50%,            Test_Total_Loss: 2.4673523902893066, Test_Accuracy_Rate: 27.00%\n",
      "Epoch 34, Total Loss: 1.750795602798462, Accuracy Rate: 45.25%,            Test_Total_Loss: 2.4572043418884277, Test_Accuracy_Rate: 29.11%\n",
      "Epoch 35, Total Loss: 1.730336308479309, Accuracy Rate: 44.88%,            Test_Total_Loss: 2.5632171630859375, Test_Accuracy_Rate: 27.00%\n",
      "Epoch 36, Total Loss: 1.7271002531051636, Accuracy Rate: 45.39%,            Test_Total_Loss: 2.4580421447753906, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 37, Total Loss: 1.70976984500885, Accuracy Rate: 46.01%,            Test_Total_Loss: 2.4397315979003906, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 38, Total Loss: 1.6880532503128052, Accuracy Rate: 46.66%,            Test_Total_Loss: 2.4712655544281006, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 39, Total Loss: 1.6820087432861328, Accuracy Rate: 46.71%,            Test_Total_Loss: 2.473130702972412, Test_Accuracy_Rate: 29.96%\n",
      "Epoch 40, Total Loss: 1.679317593574524, Accuracy Rate: 47.09%,            Test_Total_Loss: 2.466975688934326, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 41, Total Loss: 1.6686363220214844, Accuracy Rate: 46.66%,            Test_Total_Loss: 2.453712224960327, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 42, Total Loss: 1.640866756439209, Accuracy Rate: 48.59%,            Test_Total_Loss: 2.4490699768066406, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 43, Total Loss: 1.610732078552246, Accuracy Rate: 48.45%,            Test_Total_Loss: 2.4391934871673584, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 44, Total Loss: 1.6071956157684326, Accuracy Rate: 49.06%,            Test_Total_Loss: 2.4694087505340576, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 45, Total Loss: 1.59781014919281, Accuracy Rate: 48.64%,            Test_Total_Loss: 2.5111162662506104, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 46, Total Loss: 1.5901843309402466, Accuracy Rate: 49.91%,            Test_Total_Loss: 2.4919514656066895, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 47, Total Loss: 1.61588716506958, Accuracy Rate: 48.17%,            Test_Total_Loss: 2.4521119594573975, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 48, Total Loss: 1.5956026315689087, Accuracy Rate: 49.15%,            Test_Total_Loss: 2.4577183723449707, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 49, Total Loss: 1.5677084922790527, Accuracy Rate: 50.70%,            Test_Total_Loss: 2.475033760070801, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 50, Total Loss: 1.5497676134109497, Accuracy Rate: 50.80%,            Test_Total_Loss: 2.5213348865509033, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 51, Total Loss: 1.5447317361831665, Accuracy Rate: 51.17%,            Test_Total_Loss: 2.6283483505249023, Test_Accuracy_Rate: 29.11%\n",
      "Epoch 52, Total Loss: 1.5662916898727417, Accuracy Rate: 50.52%,            Test_Total_Loss: 2.4843485355377197, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 53, Total Loss: 1.5301021337509155, Accuracy Rate: 50.75%,            Test_Total_Loss: 2.5244834423065186, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 54, Total Loss: 1.5118985176086426, Accuracy Rate: 51.46%,            Test_Total_Loss: 2.557074546813965, Test_Accuracy_Rate: 29.11%\n",
      "Epoch 55, Total Loss: 1.5625624656677246, Accuracy Rate: 50.61%,            Test_Total_Loss: 2.5084774494171143, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 56, Total Loss: 1.5367722511291504, Accuracy Rate: 50.33%,            Test_Total_Loss: 2.6987099647521973, Test_Accuracy_Rate: 27.00%\n",
      "Epoch 57, Total Loss: 1.7152283191680908, Accuracy Rate: 46.48%,            Test_Total_Loss: 2.5491089820861816, Test_Accuracy_Rate: 29.11%\n",
      "Epoch 58, Total Loss: 1.590340256690979, Accuracy Rate: 49.15%,            Test_Total_Loss: 2.517355442047119, Test_Accuracy_Rate: 29.54%\n",
      "Epoch 59, Total Loss: 1.5534820556640625, Accuracy Rate: 51.08%,            Test_Total_Loss: 2.5508852005004883, Test_Accuracy_Rate: 32.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Total Loss: 1.509658932685852, Accuracy Rate: 52.02%,            Test_Total_Loss: 2.5243351459503174, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 61, Total Loss: 1.5010696649551392, Accuracy Rate: 51.50%,            Test_Total_Loss: 2.4902091026306152, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 62, Total Loss: 1.4856373071670532, Accuracy Rate: 51.97%,            Test_Total_Loss: 2.5917999744415283, Test_Accuracy_Rate: 29.96%\n",
      "Epoch 63, Total Loss: 1.477631688117981, Accuracy Rate: 51.64%,            Test_Total_Loss: 2.5708725452423096, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 64, Total Loss: 1.4571508169174194, Accuracy Rate: 52.96%,            Test_Total_Loss: 2.5306575298309326, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 65, Total Loss: 1.44551420211792, Accuracy Rate: 53.15%,            Test_Total_Loss: 2.5772716999053955, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 66, Total Loss: 1.4463471174240112, Accuracy Rate: 53.95%,            Test_Total_Loss: 2.565058946609497, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 67, Total Loss: 1.4244476556777954, Accuracy Rate: 53.57%,            Test_Total_Loss: 2.595349073410034, Test_Accuracy_Rate: 29.54%\n",
      "Epoch 68, Total Loss: 1.4360522031784058, Accuracy Rate: 53.43%,            Test_Total_Loss: 2.5770039558410645, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 69, Total Loss: 1.4515430927276611, Accuracy Rate: 52.73%,            Test_Total_Loss: 2.550248622894287, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 70, Total Loss: 1.4589078426361084, Accuracy Rate: 53.81%,            Test_Total_Loss: 2.569927453994751, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 71, Total Loss: 1.4240628480911255, Accuracy Rate: 53.57%,            Test_Total_Loss: 2.5653748512268066, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 72, Total Loss: 1.4048948287963867, Accuracy Rate: 54.51%,            Test_Total_Loss: 2.6435387134552, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 73, Total Loss: 1.4052419662475586, Accuracy Rate: 55.08%,            Test_Total_Loss: 2.7554783821105957, Test_Accuracy_Rate: 27.85%\n",
      "Epoch 74, Total Loss: 1.4064902067184448, Accuracy Rate: 54.75%,            Test_Total_Loss: 2.7061455249786377, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 75, Total Loss: 1.4025896787643433, Accuracy Rate: 54.09%,            Test_Total_Loss: 2.6376357078552246, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 76, Total Loss: 1.3821550607681274, Accuracy Rate: 54.51%,            Test_Total_Loss: 2.6962921619415283, Test_Accuracy_Rate: 29.54%\n",
      "Epoch 77, Total Loss: 1.3751330375671387, Accuracy Rate: 55.12%,            Test_Total_Loss: 2.636739730834961, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 78, Total Loss: 1.423954725265503, Accuracy Rate: 54.28%,            Test_Total_Loss: 2.6858060359954834, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 79, Total Loss: 1.3979816436767578, Accuracy Rate: 54.65%,            Test_Total_Loss: 2.642099380493164, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 80, Total Loss: 1.3584007024765015, Accuracy Rate: 55.97%,            Test_Total_Loss: 2.6317989826202393, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 81, Total Loss: 1.3683176040649414, Accuracy Rate: 55.31%,            Test_Total_Loss: 2.6801915168762207, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 82, Total Loss: 1.3570338487625122, Accuracy Rate: 55.31%,            Test_Total_Loss: 2.714250087738037, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 83, Total Loss: 1.3567376136779785, Accuracy Rate: 55.59%,            Test_Total_Loss: 2.691824436187744, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 84, Total Loss: 1.337745189666748, Accuracy Rate: 56.25%,            Test_Total_Loss: 2.7643251419067383, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 85, Total Loss: 1.3432453870773315, Accuracy Rate: 55.26%,            Test_Total_Loss: 2.7912189960479736, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 86, Total Loss: 1.3546113967895508, Accuracy Rate: 55.45%,            Test_Total_Loss: 2.7217140197753906, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 87, Total Loss: 1.3375728130340576, Accuracy Rate: 56.86%,            Test_Total_Loss: 2.6987924575805664, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 88, Total Loss: 1.3373651504516602, Accuracy Rate: 56.06%,            Test_Total_Loss: 2.710622787475586, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 89, Total Loss: 1.3181488513946533, Accuracy Rate: 56.81%,            Test_Total_Loss: 2.7195680141448975, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 90, Total Loss: 1.316200613975525, Accuracy Rate: 57.19%,            Test_Total_Loss: 2.7188210487365723, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 91, Total Loss: 1.306960105895996, Accuracy Rate: 57.00%,            Test_Total_Loss: 2.673912763595581, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 92, Total Loss: 1.3070744276046753, Accuracy Rate: 57.28%,            Test_Total_Loss: 2.683397054672241, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 93, Total Loss: 1.2901641130447388, Accuracy Rate: 56.72%,            Test_Total_Loss: 2.789868116378784, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 94, Total Loss: 1.2910411357879639, Accuracy Rate: 57.14%,            Test_Total_Loss: 2.7510900497436523, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 95, Total Loss: 1.2961002588272095, Accuracy Rate: 57.00%,            Test_Total_Loss: 2.863919734954834, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 96, Total Loss: 1.2970361709594727, Accuracy Rate: 57.05%,            Test_Total_Loss: 2.7711658477783203, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 97, Total Loss: 1.2911124229431152, Accuracy Rate: 57.24%,            Test_Total_Loss: 2.768944263458252, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 98, Total Loss: 1.2829598188400269, Accuracy Rate: 57.66%,            Test_Total_Loss: 2.8686957359313965, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 99, Total Loss: 1.2739243507385254, Accuracy Rate: 58.08%,            Test_Total_Loss: 2.890110492706299, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 100, Total Loss: 1.2886033058166504, Accuracy Rate: 57.42%,            Test_Total_Loss: 2.8558504581451416, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 101, Total Loss: 1.2768831253051758, Accuracy Rate: 57.99%,            Test_Total_Loss: 2.752164125442505, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 102, Total Loss: 1.3023513555526733, Accuracy Rate: 56.72%,            Test_Total_Loss: 2.764909267425537, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 103, Total Loss: 1.2885366678237915, Accuracy Rate: 57.19%,            Test_Total_Loss: 2.7814278602600098, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 104, Total Loss: 1.2571812868118286, Accuracy Rate: 58.22%,            Test_Total_Loss: 2.8538269996643066, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 105, Total Loss: 1.2536232471466064, Accuracy Rate: 58.18%,            Test_Total_Loss: 2.8677773475646973, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 106, Total Loss: 1.2607858180999756, Accuracy Rate: 57.66%,            Test_Total_Loss: 2.921297788619995, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 107, Total Loss: 1.2509976625442505, Accuracy Rate: 58.32%,            Test_Total_Loss: 2.8309884071350098, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 108, Total Loss: 1.3037452697753906, Accuracy Rate: 56.48%,            Test_Total_Loss: 2.886249542236328, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 109, Total Loss: 1.253829002380371, Accuracy Rate: 58.41%,            Test_Total_Loss: 2.8318636417388916, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 110, Total Loss: 1.2355788946151733, Accuracy Rate: 58.32%,            Test_Total_Loss: 2.8336453437805176, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 111, Total Loss: 1.238218903541565, Accuracy Rate: 58.65%,            Test_Total_Loss: 2.9165761470794678, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 112, Total Loss: 1.3006943464279175, Accuracy Rate: 57.99%,            Test_Total_Loss: 2.8394510746002197, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 113, Total Loss: 1.3177131414413452, Accuracy Rate: 57.24%,            Test_Total_Loss: 3.0458953380584717, Test_Accuracy_Rate: 29.54%\n",
      "Epoch 114, Total Loss: 1.3374747037887573, Accuracy Rate: 56.20%,            Test_Total_Loss: 2.755884885787964, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 115, Total Loss: 1.2638261318206787, Accuracy Rate: 58.08%,            Test_Total_Loss: 2.8103036880493164, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 116, Total Loss: 1.2402799129486084, Accuracy Rate: 58.88%,            Test_Total_Loss: 2.789262533187866, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 117, Total Loss: 1.231664776802063, Accuracy Rate: 59.07%,            Test_Total_Loss: 2.8083739280700684, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 118, Total Loss: 1.2621269226074219, Accuracy Rate: 58.65%,            Test_Total_Loss: 2.9844961166381836, Test_Accuracy_Rate: 30.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119, Total Loss: 1.3132482767105103, Accuracy Rate: 56.16%,            Test_Total_Loss: 2.8908133506774902, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 120, Total Loss: 1.2476739883422852, Accuracy Rate: 58.55%,            Test_Total_Loss: 2.8784537315368652, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 121, Total Loss: 1.2474217414855957, Accuracy Rate: 58.27%,            Test_Total_Loss: 2.843242645263672, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 122, Total Loss: 1.257810354232788, Accuracy Rate: 57.38%,            Test_Total_Loss: 2.8757309913635254, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 123, Total Loss: 1.225333571434021, Accuracy Rate: 58.55%,            Test_Total_Loss: 2.8326377868652344, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 124, Total Loss: 1.2219234704971313, Accuracy Rate: 58.93%,            Test_Total_Loss: 2.902418375015259, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 125, Total Loss: 1.2089576721191406, Accuracy Rate: 59.77%,            Test_Total_Loss: 2.901247024536133, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 126, Total Loss: 1.2127403020858765, Accuracy Rate: 59.77%,            Test_Total_Loss: 2.884852170944214, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 127, Total Loss: 1.1941572427749634, Accuracy Rate: 59.68%,            Test_Total_Loss: 2.913841962814331, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 128, Total Loss: 1.2024428844451904, Accuracy Rate: 59.77%,            Test_Total_Loss: 2.973843574523926, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 129, Total Loss: 1.1901599168777466, Accuracy Rate: 59.40%,            Test_Total_Loss: 2.928182601928711, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 130, Total Loss: 1.196340560913086, Accuracy Rate: 59.68%,            Test_Total_Loss: 2.882079839706421, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 131, Total Loss: 1.1887469291687012, Accuracy Rate: 59.77%,            Test_Total_Loss: 2.9479165077209473, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 132, Total Loss: 1.2047059535980225, Accuracy Rate: 59.16%,            Test_Total_Loss: 2.969175338745117, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 133, Total Loss: 1.3185265064239502, Accuracy Rate: 56.95%,            Test_Total_Loss: 3.212764263153076, Test_Accuracy_Rate: 24.89%\n",
      "Epoch 134, Total Loss: 1.6381099224090576, Accuracy Rate: 48.64%,            Test_Total_Loss: 2.7944817543029785, Test_Accuracy_Rate: 29.96%\n",
      "Epoch 135, Total Loss: 1.4261817932128906, Accuracy Rate: 53.43%,            Test_Total_Loss: 2.9066619873046875, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 136, Total Loss: 1.4005284309387207, Accuracy Rate: 53.76%,            Test_Total_Loss: 2.913872003555298, Test_Accuracy_Rate: 28.69%\n",
      "Epoch 137, Total Loss: 1.3404231071472168, Accuracy Rate: 54.37%,            Test_Total_Loss: 2.8474695682525635, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 138, Total Loss: 1.3178036212921143, Accuracy Rate: 56.02%,            Test_Total_Loss: 2.8555736541748047, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 139, Total Loss: 1.3080555200576782, Accuracy Rate: 55.45%,            Test_Total_Loss: 2.9991700649261475, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 140, Total Loss: 1.276028037071228, Accuracy Rate: 57.71%,            Test_Total_Loss: 2.8658764362335205, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 141, Total Loss: 1.2343852519989014, Accuracy Rate: 59.02%,            Test_Total_Loss: 2.9366438388824463, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 142, Total Loss: 1.2240386009216309, Accuracy Rate: 58.60%,            Test_Total_Loss: 2.9697065353393555, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 143, Total Loss: 1.2058908939361572, Accuracy Rate: 59.40%,            Test_Total_Loss: 2.950517416000366, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 144, Total Loss: 1.20636785030365, Accuracy Rate: 58.98%,            Test_Total_Loss: 2.9970531463623047, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 145, Total Loss: 1.2009990215301514, Accuracy Rate: 60.34%,            Test_Total_Loss: 2.955458641052246, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 146, Total Loss: 1.1843267679214478, Accuracy Rate: 60.24%,            Test_Total_Loss: 2.9899396896362305, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 147, Total Loss: 1.18798828125, Accuracy Rate: 60.29%,            Test_Total_Loss: 3.0048911571502686, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 148, Total Loss: 1.19099760055542, Accuracy Rate: 59.77%,            Test_Total_Loss: 2.9709675312042236, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 149, Total Loss: 1.2334108352661133, Accuracy Rate: 58.36%,            Test_Total_Loss: 3.00280499458313, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 150, Total Loss: 1.171669840812683, Accuracy Rate: 60.01%,            Test_Total_Loss: 2.9726762771606445, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 151, Total Loss: 1.1691474914550781, Accuracy Rate: 60.48%,            Test_Total_Loss: 2.9699838161468506, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 152, Total Loss: 1.1810529232025146, Accuracy Rate: 60.15%,            Test_Total_Loss: 2.968451499938965, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 153, Total Loss: 1.1549497842788696, Accuracy Rate: 60.48%,            Test_Total_Loss: 3.0108823776245117, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 154, Total Loss: 1.1590380668640137, Accuracy Rate: 60.76%,            Test_Total_Loss: 2.9905247688293457, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 155, Total Loss: 1.210310697555542, Accuracy Rate: 59.49%,            Test_Total_Loss: 2.9703547954559326, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 156, Total Loss: 1.190597653388977, Accuracy Rate: 60.06%,            Test_Total_Loss: 2.9709267616271973, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 157, Total Loss: 1.3015316724777222, Accuracy Rate: 56.86%,            Test_Total_Loss: 2.979278326034546, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 158, Total Loss: 1.449460744857788, Accuracy Rate: 53.15%,            Test_Total_Loss: 2.9258713722229004, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 159, Total Loss: 1.3273046016693115, Accuracy Rate: 56.44%,            Test_Total_Loss: 2.886387825012207, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 160, Total Loss: 1.2420082092285156, Accuracy Rate: 58.60%,            Test_Total_Loss: 2.939819812774658, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 161, Total Loss: 1.209177851676941, Accuracy Rate: 59.68%,            Test_Total_Loss: 2.920656681060791, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 162, Total Loss: 1.1965793371200562, Accuracy Rate: 59.45%,            Test_Total_Loss: 3.0058610439300537, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 163, Total Loss: 1.1946194171905518, Accuracy Rate: 59.68%,            Test_Total_Loss: 2.94651460647583, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 164, Total Loss: 1.177683711051941, Accuracy Rate: 60.39%,            Test_Total_Loss: 3.030510187149048, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 165, Total Loss: 1.1563003063201904, Accuracy Rate: 60.62%,            Test_Total_Loss: 3.0312888622283936, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 166, Total Loss: 1.1939853429794312, Accuracy Rate: 59.26%,            Test_Total_Loss: 2.9861104488372803, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 167, Total Loss: 1.167521595954895, Accuracy Rate: 60.76%,            Test_Total_Loss: 3.067389726638794, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 168, Total Loss: 1.151082158088684, Accuracy Rate: 59.68%,            Test_Total_Loss: 3.038874626159668, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 169, Total Loss: 1.1452215909957886, Accuracy Rate: 61.42%,            Test_Total_Loss: 3.133312940597534, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 170, Total Loss: 1.1291526556015015, Accuracy Rate: 61.61%,            Test_Total_Loss: 3.1124072074890137, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 171, Total Loss: 1.1380523443222046, Accuracy Rate: 61.56%,            Test_Total_Loss: 3.096118450164795, Test_Accuracy_Rate: 29.96%\n",
      "Epoch 172, Total Loss: 1.1440117359161377, Accuracy Rate: 61.04%,            Test_Total_Loss: 3.1442008018493652, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 173, Total Loss: 1.141124963760376, Accuracy Rate: 60.53%,            Test_Total_Loss: 3.2201755046844482, Test_Accuracy_Rate: 29.54%\n",
      "Epoch 174, Total Loss: 1.1293877363204956, Accuracy Rate: 62.27%,            Test_Total_Loss: 3.087702989578247, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 175, Total Loss: 1.12974214553833, Accuracy Rate: 61.18%,            Test_Total_Loss: 3.1364917755126953, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 176, Total Loss: 1.142140507698059, Accuracy Rate: 61.09%,            Test_Total_Loss: 3.0751898288726807, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 177, Total Loss: 1.1309022903442383, Accuracy Rate: 60.95%,            Test_Total_Loss: 3.200432777404785, Test_Accuracy_Rate: 30.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178, Total Loss: 1.1338613033294678, Accuracy Rate: 61.18%,            Test_Total_Loss: 3.1722919940948486, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 179, Total Loss: 1.1740398406982422, Accuracy Rate: 60.57%,            Test_Total_Loss: 3.1385836601257324, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 180, Total Loss: 1.1411306858062744, Accuracy Rate: 60.90%,            Test_Total_Loss: 3.106823682785034, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 181, Total Loss: 1.1160982847213745, Accuracy Rate: 62.12%,            Test_Total_Loss: 3.139880657196045, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 182, Total Loss: 1.135185718536377, Accuracy Rate: 61.70%,            Test_Total_Loss: 3.2902634143829346, Test_Accuracy_Rate: 25.32%\n",
      "Epoch 183, Total Loss: 1.1550410985946655, Accuracy Rate: 60.43%,            Test_Total_Loss: 3.1644506454467773, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 184, Total Loss: 1.392910361289978, Accuracy Rate: 55.08%,            Test_Total_Loss: 2.9647130966186523, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 185, Total Loss: 1.3160135746002197, Accuracy Rate: 55.36%,            Test_Total_Loss: 2.9996979236602783, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 186, Total Loss: 1.4068396091461182, Accuracy Rate: 54.09%,            Test_Total_Loss: 3.040478467941284, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 187, Total Loss: 1.3228564262390137, Accuracy Rate: 55.40%,            Test_Total_Loss: 3.0615241527557373, Test_Accuracy_Rate: 29.54%\n",
      "Epoch 188, Total Loss: 1.2280731201171875, Accuracy Rate: 59.12%,            Test_Total_Loss: 2.9632489681243896, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 189, Total Loss: 1.1843844652175903, Accuracy Rate: 60.15%,            Test_Total_Loss: 3.030351400375366, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 190, Total Loss: 1.186416506767273, Accuracy Rate: 60.20%,            Test_Total_Loss: 3.0184528827667236, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 191, Total Loss: 1.1845656633377075, Accuracy Rate: 60.48%,            Test_Total_Loss: 3.0373623371124268, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 192, Total Loss: 1.172750473022461, Accuracy Rate: 60.43%,            Test_Total_Loss: 2.9976727962493896, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 193, Total Loss: 1.137436032295227, Accuracy Rate: 61.28%,            Test_Total_Loss: 3.0354394912719727, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 194, Total Loss: 1.1480717658996582, Accuracy Rate: 61.09%,            Test_Total_Loss: 3.0412425994873047, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 195, Total Loss: 1.1448121070861816, Accuracy Rate: 61.42%,            Test_Total_Loss: 3.161261558532715, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 196, Total Loss: 1.2325935363769531, Accuracy Rate: 59.35%,            Test_Total_Loss: 3.0774002075195312, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 197, Total Loss: 1.1593263149261475, Accuracy Rate: 60.39%,            Test_Total_Loss: 3.061983585357666, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 198, Total Loss: 1.1241228580474854, Accuracy Rate: 61.47%,            Test_Total_Loss: 3.0698812007904053, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 199, Total Loss: 1.3024630546569824, Accuracy Rate: 56.30%,            Test_Total_Loss: 3.075685739517212, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 200, Total Loss: 1.406601905822754, Accuracy Rate: 54.28%,            Test_Total_Loss: 3.0277154445648193, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 201, Total Loss: 1.3193819522857666, Accuracy Rate: 56.06%,            Test_Total_Loss: 3.020325183868408, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 202, Total Loss: 1.2035950422286987, Accuracy Rate: 59.73%,            Test_Total_Loss: 2.9985978603363037, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 203, Total Loss: 1.174553632736206, Accuracy Rate: 60.57%,            Test_Total_Loss: 3.0202889442443848, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 204, Total Loss: 1.1723546981811523, Accuracy Rate: 60.81%,            Test_Total_Loss: 3.12481427192688, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 205, Total Loss: 1.1992493867874146, Accuracy Rate: 59.07%,            Test_Total_Loss: 3.1473612785339355, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 206, Total Loss: 1.151834487915039, Accuracy Rate: 60.81%,            Test_Total_Loss: 3.0882344245910645, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 207, Total Loss: 1.115518569946289, Accuracy Rate: 61.84%,            Test_Total_Loss: 3.0562944412231445, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 208, Total Loss: 1.1032449007034302, Accuracy Rate: 61.51%,            Test_Total_Loss: 3.128957748413086, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 209, Total Loss: 1.096816897392273, Accuracy Rate: 61.42%,            Test_Total_Loss: 3.150847911834717, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 210, Total Loss: 1.1023074388504028, Accuracy Rate: 62.12%,            Test_Total_Loss: 3.137208938598633, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 211, Total Loss: 1.0976239442825317, Accuracy Rate: 62.41%,            Test_Total_Loss: 3.1669416427612305, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 212, Total Loss: 1.1637645959854126, Accuracy Rate: 60.10%,            Test_Total_Loss: 3.0800859928131104, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 213, Total Loss: 1.1388053894042969, Accuracy Rate: 60.62%,            Test_Total_Loss: 3.1854116916656494, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 214, Total Loss: 1.11233651638031, Accuracy Rate: 61.37%,            Test_Total_Loss: 3.1257033348083496, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 215, Total Loss: 1.1050268411636353, Accuracy Rate: 62.17%,            Test_Total_Loss: 3.0822765827178955, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 216, Total Loss: 1.0909123420715332, Accuracy Rate: 62.08%,            Test_Total_Loss: 3.1420884132385254, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 217, Total Loss: 1.11453378200531, Accuracy Rate: 62.59%,            Test_Total_Loss: 3.0600128173828125, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 218, Total Loss: 1.1134089231491089, Accuracy Rate: 62.12%,            Test_Total_Loss: 3.080622911453247, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 219, Total Loss: 1.0908902883529663, Accuracy Rate: 63.06%,            Test_Total_Loss: 3.14679217338562, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 220, Total Loss: 1.0928393602371216, Accuracy Rate: 62.08%,            Test_Total_Loss: 3.0922937393188477, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 221, Total Loss: 1.080003023147583, Accuracy Rate: 62.50%,            Test_Total_Loss: 3.148791790008545, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 222, Total Loss: 1.0736640691757202, Accuracy Rate: 63.25%,            Test_Total_Loss: 3.098407030105591, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 223, Total Loss: 1.1111727952957153, Accuracy Rate: 61.28%,            Test_Total_Loss: 3.1769344806671143, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 224, Total Loss: 1.0977833271026611, Accuracy Rate: 61.94%,            Test_Total_Loss: 3.1778481006622314, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 225, Total Loss: 1.089802622795105, Accuracy Rate: 61.61%,            Test_Total_Loss: 3.2239527702331543, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 226, Total Loss: 1.0834424495697021, Accuracy Rate: 61.80%,            Test_Total_Loss: 3.147017002105713, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 227, Total Loss: 1.0629428625106812, Accuracy Rate: 61.98%,            Test_Total_Loss: 3.1383345127105713, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 228, Total Loss: 1.055853247642517, Accuracy Rate: 63.86%,            Test_Total_Loss: 3.216485023498535, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 229, Total Loss: 1.064889907836914, Accuracy Rate: 63.06%,            Test_Total_Loss: 3.1064443588256836, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 230, Total Loss: 1.085354208946228, Accuracy Rate: 62.17%,            Test_Total_Loss: 3.188467264175415, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 231, Total Loss: 1.0757254362106323, Accuracy Rate: 62.64%,            Test_Total_Loss: 3.170335531234741, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 232, Total Loss: 1.0733014345169067, Accuracy Rate: 61.98%,            Test_Total_Loss: 3.179215669631958, Test_Accuracy_Rate: 29.96%\n",
      "Epoch 233, Total Loss: 1.0518735647201538, Accuracy Rate: 62.36%,            Test_Total_Loss: 3.2146594524383545, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 234, Total Loss: 1.0477752685546875, Accuracy Rate: 63.58%,            Test_Total_Loss: 3.1806046962738037, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 235, Total Loss: 1.0348546504974365, Accuracy Rate: 62.69%,            Test_Total_Loss: 3.2439193725585938, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 236, Total Loss: 1.0291365385055542, Accuracy Rate: 62.97%,            Test_Total_Loss: 3.2531187534332275, Test_Accuracy_Rate: 30.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237, Total Loss: 1.0373072624206543, Accuracy Rate: 63.35%,            Test_Total_Loss: 3.256664991378784, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 238, Total Loss: 1.0559027194976807, Accuracy Rate: 62.08%,            Test_Total_Loss: 3.285088300704956, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 239, Total Loss: 1.048283576965332, Accuracy Rate: 62.41%,            Test_Total_Loss: 3.314277172088623, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 240, Total Loss: 1.050055980682373, Accuracy Rate: 62.59%,            Test_Total_Loss: 3.340033531188965, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 241, Total Loss: 1.0615322589874268, Accuracy Rate: 62.50%,            Test_Total_Loss: 3.379059314727783, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 242, Total Loss: 1.0683491230010986, Accuracy Rate: 62.55%,            Test_Total_Loss: 3.373030424118042, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 243, Total Loss: 1.0572972297668457, Accuracy Rate: 63.53%,            Test_Total_Loss: 3.209301471710205, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 244, Total Loss: 1.0668028593063354, Accuracy Rate: 62.27%,            Test_Total_Loss: 3.3240861892700195, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 245, Total Loss: 1.108598232269287, Accuracy Rate: 61.56%,            Test_Total_Loss: 3.203874349594116, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 246, Total Loss: 1.1475917100906372, Accuracy Rate: 60.43%,            Test_Total_Loss: 3.2218871116638184, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 247, Total Loss: 1.0836637020111084, Accuracy Rate: 62.55%,            Test_Total_Loss: 3.1567623615264893, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 248, Total Loss: 1.102323055267334, Accuracy Rate: 61.89%,            Test_Total_Loss: 3.1639482975006104, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 249, Total Loss: 1.1047791242599487, Accuracy Rate: 61.09%,            Test_Total_Loss: 3.10300612449646, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 250, Total Loss: 1.086903691291809, Accuracy Rate: 62.45%,            Test_Total_Loss: 3.2005467414855957, Test_Accuracy_Rate: 29.96%\n",
      "Epoch 251, Total Loss: 1.0753072500228882, Accuracy Rate: 62.64%,            Test_Total_Loss: 3.1865572929382324, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 252, Total Loss: 1.0789023637771606, Accuracy Rate: 61.94%,            Test_Total_Loss: 3.125199317932129, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 253, Total Loss: 1.0806641578674316, Accuracy Rate: 62.31%,            Test_Total_Loss: 3.1479270458221436, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 254, Total Loss: 1.0516769886016846, Accuracy Rate: 62.78%,            Test_Total_Loss: 3.1480491161346436, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 255, Total Loss: 1.033452033996582, Accuracy Rate: 63.72%,            Test_Total_Loss: 3.2256476879119873, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 256, Total Loss: 1.192402720451355, Accuracy Rate: 59.68%,            Test_Total_Loss: 3.2306697368621826, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 257, Total Loss: 1.2050259113311768, Accuracy Rate: 58.88%,            Test_Total_Loss: 3.173328161239624, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 258, Total Loss: 1.1127171516418457, Accuracy Rate: 60.90%,            Test_Total_Loss: 3.2578654289245605, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 259, Total Loss: 1.0784337520599365, Accuracy Rate: 62.12%,            Test_Total_Loss: 3.231344699859619, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 260, Total Loss: 1.037498950958252, Accuracy Rate: 63.30%,            Test_Total_Loss: 3.2518181800842285, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 261, Total Loss: 1.0323971509933472, Accuracy Rate: 63.58%,            Test_Total_Loss: 3.2485172748565674, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 262, Total Loss: 1.0215611457824707, Accuracy Rate: 63.53%,            Test_Total_Loss: 3.266934633255005, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 263, Total Loss: 1.0095324516296387, Accuracy Rate: 63.30%,            Test_Total_Loss: 3.2457938194274902, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 264, Total Loss: 1.0165907144546509, Accuracy Rate: 63.20%,            Test_Total_Loss: 3.3465335369110107, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 265, Total Loss: 1.0295575857162476, Accuracy Rate: 63.11%,            Test_Total_Loss: 3.2852447032928467, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 266, Total Loss: 1.0258052349090576, Accuracy Rate: 63.39%,            Test_Total_Loss: 3.2635786533355713, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 267, Total Loss: 1.0484347343444824, Accuracy Rate: 63.25%,            Test_Total_Loss: 3.237596273422241, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 268, Total Loss: 1.0855309963226318, Accuracy Rate: 61.94%,            Test_Total_Loss: 3.253692150115967, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 269, Total Loss: 1.0653866529464722, Accuracy Rate: 62.22%,            Test_Total_Loss: 3.1487948894500732, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 270, Total Loss: 1.0222808122634888, Accuracy Rate: 64.05%,            Test_Total_Loss: 3.2143197059631348, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 271, Total Loss: 1.026787519454956, Accuracy Rate: 63.44%,            Test_Total_Loss: 3.2288618087768555, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 272, Total Loss: 1.0205469131469727, Accuracy Rate: 64.33%,            Test_Total_Loss: 3.2294185161590576, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 273, Total Loss: 1.001705288887024, Accuracy Rate: 63.58%,            Test_Total_Loss: 3.2586793899536133, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 274, Total Loss: 1.004490613937378, Accuracy Rate: 64.47%,            Test_Total_Loss: 3.2693123817443848, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 275, Total Loss: 0.991764485836029, Accuracy Rate: 63.91%,            Test_Total_Loss: 3.2916548252105713, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 276, Total Loss: 0.9985035061836243, Accuracy Rate: 64.38%,            Test_Total_Loss: 3.431396961212158, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 277, Total Loss: 0.9879931807518005, Accuracy Rate: 64.57%,            Test_Total_Loss: 3.338085889816284, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 278, Total Loss: 0.9976851344108582, Accuracy Rate: 64.10%,            Test_Total_Loss: 3.3303446769714355, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 279, Total Loss: 1.0061155557632446, Accuracy Rate: 63.30%,            Test_Total_Loss: 3.3508100509643555, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 280, Total Loss: 1.0496230125427246, Accuracy Rate: 62.92%,            Test_Total_Loss: 3.413825273513794, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 281, Total Loss: 1.0944145917892456, Accuracy Rate: 61.18%,            Test_Total_Loss: 3.2971181869506836, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 282, Total Loss: 1.0733816623687744, Accuracy Rate: 62.55%,            Test_Total_Loss: 3.2522149085998535, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 283, Total Loss: 1.0764440298080444, Accuracy Rate: 63.06%,            Test_Total_Loss: 3.251844644546509, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 284, Total Loss: 1.0353049039840698, Accuracy Rate: 63.58%,            Test_Total_Loss: 3.3034591674804688, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 285, Total Loss: 1.0098474025726318, Accuracy Rate: 63.11%,            Test_Total_Loss: 3.212317943572998, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 286, Total Loss: 1.0234142541885376, Accuracy Rate: 63.49%,            Test_Total_Loss: 3.3226563930511475, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 287, Total Loss: 1.0137965679168701, Accuracy Rate: 63.77%,            Test_Total_Loss: 3.3622870445251465, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 288, Total Loss: 1.1364959478378296, Accuracy Rate: 61.04%,            Test_Total_Loss: 3.276278495788574, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 289, Total Loss: 1.2653518915176392, Accuracy Rate: 58.32%,            Test_Total_Loss: 3.3439629077911377, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 290, Total Loss: 1.369401454925537, Accuracy Rate: 56.25%,            Test_Total_Loss: 3.1347432136535645, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 291, Total Loss: 1.2719589471817017, Accuracy Rate: 57.38%,            Test_Total_Loss: 3.036468029022217, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 292, Total Loss: 1.2074326276779175, Accuracy Rate: 58.69%,            Test_Total_Loss: 3.066298723220825, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 293, Total Loss: 1.1391005516052246, Accuracy Rate: 61.37%,            Test_Total_Loss: 3.130398988723755, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 294, Total Loss: 1.11690354347229, Accuracy Rate: 62.27%,            Test_Total_Loss: 3.171334743499756, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 295, Total Loss: 1.0807075500488281, Accuracy Rate: 62.78%,            Test_Total_Loss: 3.205911636352539, Test_Accuracy_Rate: 32.07%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296, Total Loss: 1.0793179273605347, Accuracy Rate: 62.69%,            Test_Total_Loss: 3.1392569541931152, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 297, Total Loss: 1.1321969032287598, Accuracy Rate: 60.34%,            Test_Total_Loss: 3.2733421325683594, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 298, Total Loss: 1.1042959690093994, Accuracy Rate: 61.00%,            Test_Total_Loss: 3.192837715148926, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 299, Total Loss: 1.1226133108139038, Accuracy Rate: 61.14%,            Test_Total_Loss: 3.158285140991211, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 300, Total Loss: 1.069016695022583, Accuracy Rate: 63.16%,            Test_Total_Loss: 3.3078906536102295, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 301, Total Loss: 1.0560418367385864, Accuracy Rate: 62.31%,            Test_Total_Loss: 3.303471088409424, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 302, Total Loss: 1.0396933555603027, Accuracy Rate: 63.06%,            Test_Total_Loss: 3.2660093307495117, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 303, Total Loss: 1.0217092037200928, Accuracy Rate: 63.20%,            Test_Total_Loss: 3.34594988822937, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 304, Total Loss: 1.0036940574645996, Accuracy Rate: 63.25%,            Test_Total_Loss: 3.311269521713257, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 305, Total Loss: 1.0043672323226929, Accuracy Rate: 63.20%,            Test_Total_Loss: 3.307410478591919, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 306, Total Loss: 1.0190802812576294, Accuracy Rate: 63.16%,            Test_Total_Loss: 3.347384452819824, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 307, Total Loss: 1.0061225891113281, Accuracy Rate: 64.71%,            Test_Total_Loss: 3.4204554557800293, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 308, Total Loss: 1.0011663436889648, Accuracy Rate: 63.58%,            Test_Total_Loss: 3.284670829772949, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 309, Total Loss: 1.0226378440856934, Accuracy Rate: 63.20%,            Test_Total_Loss: 3.36698579788208, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 310, Total Loss: 1.0090795755386353, Accuracy Rate: 63.86%,            Test_Total_Loss: 3.3692739009857178, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 311, Total Loss: 1.0080986022949219, Accuracy Rate: 63.91%,            Test_Total_Loss: 3.356844425201416, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 312, Total Loss: 1.0227384567260742, Accuracy Rate: 63.91%,            Test_Total_Loss: 3.3223931789398193, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 313, Total Loss: 1.0347996950149536, Accuracy Rate: 63.06%,            Test_Total_Loss: 3.3590621948242188, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 314, Total Loss: 1.0050996541976929, Accuracy Rate: 63.16%,            Test_Total_Loss: 3.397490978240967, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 315, Total Loss: 1.0020357370376587, Accuracy Rate: 64.00%,            Test_Total_Loss: 3.3768105506896973, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 316, Total Loss: 1.0020431280136108, Accuracy Rate: 63.44%,            Test_Total_Loss: 3.369183301925659, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 317, Total Loss: 1.0004242658615112, Accuracy Rate: 63.72%,            Test_Total_Loss: 3.361421823501587, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 318, Total Loss: 0.989416778087616, Accuracy Rate: 64.47%,            Test_Total_Loss: 3.4056880474090576, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 319, Total Loss: 1.0421812534332275, Accuracy Rate: 62.55%,            Test_Total_Loss: 3.372684955596924, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 320, Total Loss: 1.0035548210144043, Accuracy Rate: 64.90%,            Test_Total_Loss: 3.396759033203125, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 321, Total Loss: 0.9861518740653992, Accuracy Rate: 64.33%,            Test_Total_Loss: 3.368143320083618, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 322, Total Loss: 0.9872450232505798, Accuracy Rate: 63.86%,            Test_Total_Loss: 3.3531079292297363, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 323, Total Loss: 1.025900959968567, Accuracy Rate: 63.30%,            Test_Total_Loss: 3.4396278858184814, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 324, Total Loss: 1.0213638544082642, Accuracy Rate: 63.20%,            Test_Total_Loss: 3.4017553329467773, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 325, Total Loss: 1.0009291172027588, Accuracy Rate: 64.43%,            Test_Total_Loss: 3.378980875015259, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 326, Total Loss: 1.0077629089355469, Accuracy Rate: 63.86%,            Test_Total_Loss: 3.426063299179077, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 327, Total Loss: 1.0120344161987305, Accuracy Rate: 63.44%,            Test_Total_Loss: 3.49365234375, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 328, Total Loss: 1.0245099067687988, Accuracy Rate: 63.30%,            Test_Total_Loss: 3.4435296058654785, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 329, Total Loss: 0.9816270470619202, Accuracy Rate: 64.38%,            Test_Total_Loss: 3.4512593746185303, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 330, Total Loss: 0.9826045036315918, Accuracy Rate: 64.85%,            Test_Total_Loss: 3.518768787384033, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 331, Total Loss: 0.9878054857254028, Accuracy Rate: 63.82%,            Test_Total_Loss: 3.4441092014312744, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 332, Total Loss: 0.9604713320732117, Accuracy Rate: 65.27%,            Test_Total_Loss: 3.53399658203125, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 333, Total Loss: 0.9400657415390015, Accuracy Rate: 65.88%,            Test_Total_Loss: 3.4746787548065186, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 334, Total Loss: 0.9716680645942688, Accuracy Rate: 63.63%,            Test_Total_Loss: 3.5006003379821777, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 335, Total Loss: 0.9655225276947021, Accuracy Rate: 64.61%,            Test_Total_Loss: 3.4763312339782715, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 336, Total Loss: 0.9700501561164856, Accuracy Rate: 65.04%,            Test_Total_Loss: 3.3826029300689697, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 337, Total Loss: 0.9857855439186096, Accuracy Rate: 64.94%,            Test_Total_Loss: 3.5015616416931152, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 338, Total Loss: 0.9804672598838806, Accuracy Rate: 64.38%,            Test_Total_Loss: 3.425386905670166, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 339, Total Loss: 0.9851494431495667, Accuracy Rate: 63.82%,            Test_Total_Loss: 3.5460586547851562, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 340, Total Loss: 0.9684494733810425, Accuracy Rate: 65.04%,            Test_Total_Loss: 3.5083389282226562, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 341, Total Loss: 1.0560957193374634, Accuracy Rate: 62.12%,            Test_Total_Loss: 3.5464537143707275, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 342, Total Loss: 1.0574579238891602, Accuracy Rate: 62.69%,            Test_Total_Loss: 3.459989547729492, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 343, Total Loss: 0.9796670079231262, Accuracy Rate: 64.57%,            Test_Total_Loss: 3.485183000564575, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 344, Total Loss: 1.0166584253311157, Accuracy Rate: 64.19%,            Test_Total_Loss: 3.5470917224884033, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 345, Total Loss: 1.0054678916931152, Accuracy Rate: 64.14%,            Test_Total_Loss: 3.4112284183502197, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 346, Total Loss: 0.9841499924659729, Accuracy Rate: 64.43%,            Test_Total_Loss: 3.505049705505371, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 347, Total Loss: 0.9519341588020325, Accuracy Rate: 65.27%,            Test_Total_Loss: 3.5060625076293945, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 348, Total Loss: 0.9500787258148193, Accuracy Rate: 64.76%,            Test_Total_Loss: 3.4787046909332275, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 349, Total Loss: 0.9445177316665649, Accuracy Rate: 65.37%,            Test_Total_Loss: 3.5459725856781006, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 350, Total Loss: 0.9217844605445862, Accuracy Rate: 65.93%,            Test_Total_Loss: 3.55503511428833, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 351, Total Loss: 0.9208601117134094, Accuracy Rate: 66.31%,            Test_Total_Loss: 3.593386173248291, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 352, Total Loss: 0.9239018559455872, Accuracy Rate: 65.27%,            Test_Total_Loss: 3.6511809825897217, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 353, Total Loss: 1.0335360765457153, Accuracy Rate: 63.20%,            Test_Total_Loss: 3.5915393829345703, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 354, Total Loss: 1.0311579704284668, Accuracy Rate: 62.78%,            Test_Total_Loss: 3.510533094406128, Test_Accuracy_Rate: 31.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355, Total Loss: 1.056984782218933, Accuracy Rate: 63.11%,            Test_Total_Loss: 3.648451805114746, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 356, Total Loss: 1.0113005638122559, Accuracy Rate: 62.45%,            Test_Total_Loss: 3.608086109161377, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 357, Total Loss: 0.9864400625228882, Accuracy Rate: 63.53%,            Test_Total_Loss: 3.658982038497925, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 358, Total Loss: 0.9512982368469238, Accuracy Rate: 65.55%,            Test_Total_Loss: 3.6276659965515137, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 359, Total Loss: 0.9485291838645935, Accuracy Rate: 65.23%,            Test_Total_Loss: 3.630568504333496, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 360, Total Loss: 0.9440909624099731, Accuracy Rate: 65.70%,            Test_Total_Loss: 3.604257583618164, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 361, Total Loss: 0.9414299726486206, Accuracy Rate: 65.41%,            Test_Total_Loss: 3.671841621398926, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 362, Total Loss: 0.9413637518882751, Accuracy Rate: 65.65%,            Test_Total_Loss: 3.6395061016082764, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 363, Total Loss: 0.9320732355117798, Accuracy Rate: 65.37%,            Test_Total_Loss: 3.6128756999969482, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 364, Total Loss: 0.9478156566619873, Accuracy Rate: 64.90%,            Test_Total_Loss: 3.6012847423553467, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 365, Total Loss: 0.9269100427627563, Accuracy Rate: 66.07%,            Test_Total_Loss: 3.6156609058380127, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 366, Total Loss: 0.9442204833030701, Accuracy Rate: 65.04%,            Test_Total_Loss: 3.659278392791748, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 367, Total Loss: 0.9589837193489075, Accuracy Rate: 64.71%,            Test_Total_Loss: 3.4493651390075684, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 368, Total Loss: 0.9407229423522949, Accuracy Rate: 65.88%,            Test_Total_Loss: 3.683852195739746, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 369, Total Loss: 0.9600421190261841, Accuracy Rate: 65.65%,            Test_Total_Loss: 3.5833921432495117, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 370, Total Loss: 0.9556285738945007, Accuracy Rate: 64.90%,            Test_Total_Loss: 3.705706834793091, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 371, Total Loss: 0.9445226192474365, Accuracy Rate: 65.41%,            Test_Total_Loss: 3.6641719341278076, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 372, Total Loss: 0.9449866414070129, Accuracy Rate: 64.71%,            Test_Total_Loss: 3.6266112327575684, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 373, Total Loss: 0.9438582062721252, Accuracy Rate: 65.13%,            Test_Total_Loss: 3.6338486671447754, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 374, Total Loss: 0.9657726883888245, Accuracy Rate: 65.23%,            Test_Total_Loss: 3.662477493286133, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 375, Total Loss: 0.9513103365898132, Accuracy Rate: 65.32%,            Test_Total_Loss: 3.6018173694610596, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 376, Total Loss: 0.9111170768737793, Accuracy Rate: 66.17%,            Test_Total_Loss: 3.7383291721343994, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 377, Total Loss: 0.905834972858429, Accuracy Rate: 65.93%,            Test_Total_Loss: 3.6054043769836426, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 378, Total Loss: 0.9067419767379761, Accuracy Rate: 66.21%,            Test_Total_Loss: 3.6451430320739746, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 379, Total Loss: 0.8941283226013184, Accuracy Rate: 66.96%,            Test_Total_Loss: 3.6030068397521973, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 380, Total Loss: 0.8912336826324463, Accuracy Rate: 66.92%,            Test_Total_Loss: 3.674891233444214, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 381, Total Loss: 0.8945913910865784, Accuracy Rate: 66.92%,            Test_Total_Loss: 3.8086347579956055, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 382, Total Loss: 0.8948009610176086, Accuracy Rate: 66.54%,            Test_Total_Loss: 3.775238275527954, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 383, Total Loss: 1.0389888286590576, Accuracy Rate: 63.67%,            Test_Total_Loss: 3.6411023139953613, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 384, Total Loss: 1.0927155017852783, Accuracy Rate: 61.61%,            Test_Total_Loss: 3.773301839828491, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 385, Total Loss: 1.0441471338272095, Accuracy Rate: 63.35%,            Test_Total_Loss: 3.5538487434387207, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 386, Total Loss: 1.037407398223877, Accuracy Rate: 62.45%,            Test_Total_Loss: 3.6586191654205322, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 387, Total Loss: 0.9861541390419006, Accuracy Rate: 63.49%,            Test_Total_Loss: 3.523202419281006, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 388, Total Loss: 0.9375698566436768, Accuracy Rate: 65.46%,            Test_Total_Loss: 3.5251338481903076, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 389, Total Loss: 0.932726263999939, Accuracy Rate: 65.37%,            Test_Total_Loss: 3.5369014739990234, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 390, Total Loss: 0.904766321182251, Accuracy Rate: 66.73%,            Test_Total_Loss: 3.526902914047241, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 391, Total Loss: 0.9022046327590942, Accuracy Rate: 65.79%,            Test_Total_Loss: 3.6538405418395996, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 392, Total Loss: 0.8742243051528931, Accuracy Rate: 67.72%,            Test_Total_Loss: 3.6356918811798096, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 393, Total Loss: 0.8837407231330872, Accuracy Rate: 66.26%,            Test_Total_Loss: 3.628800630569458, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 394, Total Loss: 0.8751601576805115, Accuracy Rate: 67.43%,            Test_Total_Loss: 3.602947950363159, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 395, Total Loss: 0.89634770154953, Accuracy Rate: 66.73%,            Test_Total_Loss: 3.442889928817749, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 396, Total Loss: 0.9854556322097778, Accuracy Rate: 64.66%,            Test_Total_Loss: 3.5632100105285645, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 397, Total Loss: 0.9670704007148743, Accuracy Rate: 64.80%,            Test_Total_Loss: 3.5109148025512695, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 398, Total Loss: 0.9667258262634277, Accuracy Rate: 64.19%,            Test_Total_Loss: 3.690871238708496, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 399, Total Loss: 0.9663024544715881, Accuracy Rate: 64.57%,            Test_Total_Loss: 3.6048450469970703, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 400, Total Loss: 0.9277949333190918, Accuracy Rate: 64.61%,            Test_Total_Loss: 3.552940607070923, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 401, Total Loss: 0.9341132044792175, Accuracy Rate: 64.94%,            Test_Total_Loss: 3.6321027278900146, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 402, Total Loss: 0.9149941802024841, Accuracy Rate: 65.93%,            Test_Total_Loss: 3.6828229427337646, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 403, Total Loss: 0.9077956080436707, Accuracy Rate: 65.79%,            Test_Total_Loss: 3.668152332305908, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 404, Total Loss: 0.893981397151947, Accuracy Rate: 66.87%,            Test_Total_Loss: 3.682563066482544, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 405, Total Loss: 0.9029560685157776, Accuracy Rate: 66.78%,            Test_Total_Loss: 3.707080602645874, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 406, Total Loss: 0.8839515447616577, Accuracy Rate: 66.26%,            Test_Total_Loss: 3.712183952331543, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 407, Total Loss: 0.9037907123565674, Accuracy Rate: 66.40%,            Test_Total_Loss: 3.7145700454711914, Test_Accuracy_Rate: 29.96%\n",
      "Epoch 408, Total Loss: 0.9374635219573975, Accuracy Rate: 64.00%,            Test_Total_Loss: 3.6457276344299316, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 409, Total Loss: 1.0640192031860352, Accuracy Rate: 62.55%,            Test_Total_Loss: 3.727055311203003, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 410, Total Loss: 1.1444454193115234, Accuracy Rate: 60.43%,            Test_Total_Loss: 3.6647815704345703, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 411, Total Loss: 1.0808677673339844, Accuracy Rate: 61.51%,            Test_Total_Loss: 3.581470012664795, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 412, Total Loss: 0.9999696016311646, Accuracy Rate: 64.57%,            Test_Total_Loss: 3.6759188175201416, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 413, Total Loss: 0.9482817649841309, Accuracy Rate: 65.27%,            Test_Total_Loss: 3.67805814743042, Test_Accuracy_Rate: 33.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414, Total Loss: 0.9683727622032166, Accuracy Rate: 64.66%,            Test_Total_Loss: 3.7239346504211426, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 415, Total Loss: 0.9411552548408508, Accuracy Rate: 65.74%,            Test_Total_Loss: 3.7334423065185547, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 416, Total Loss: 0.9210136532783508, Accuracy Rate: 66.59%,            Test_Total_Loss: 3.7077908515930176, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 417, Total Loss: 0.8834493160247803, Accuracy Rate: 67.62%,            Test_Total_Loss: 3.6663544178009033, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 418, Total Loss: 0.8872714042663574, Accuracy Rate: 66.82%,            Test_Total_Loss: 3.742820978164673, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 419, Total Loss: 0.8798612952232361, Accuracy Rate: 67.43%,            Test_Total_Loss: 3.694007396697998, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 420, Total Loss: 0.9036422371864319, Accuracy Rate: 67.01%,            Test_Total_Loss: 3.7044408321380615, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 421, Total Loss: 0.9189910292625427, Accuracy Rate: 66.07%,            Test_Total_Loss: 3.584312677383423, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 422, Total Loss: 1.04083251953125, Accuracy Rate: 63.63%,            Test_Total_Loss: 3.686962842941284, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 423, Total Loss: 1.250005841255188, Accuracy Rate: 58.27%,            Test_Total_Loss: 3.567425489425659, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 424, Total Loss: 1.0762343406677246, Accuracy Rate: 61.23%,            Test_Total_Loss: 3.56396222114563, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 425, Total Loss: 1.1382378339767456, Accuracy Rate: 60.76%,            Test_Total_Loss: 3.5854923725128174, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 426, Total Loss: 1.0860934257507324, Accuracy Rate: 62.88%,            Test_Total_Loss: 3.4925804138183594, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 427, Total Loss: 0.9673494696617126, Accuracy Rate: 65.13%,            Test_Total_Loss: 3.554569959640503, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 428, Total Loss: 0.9100151062011719, Accuracy Rate: 65.88%,            Test_Total_Loss: 3.638244152069092, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 429, Total Loss: 0.9365748763084412, Accuracy Rate: 66.12%,            Test_Total_Loss: 3.576231002807617, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 430, Total Loss: 0.9143055081367493, Accuracy Rate: 66.31%,            Test_Total_Loss: 3.6040103435516357, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 431, Total Loss: 0.889371931552887, Accuracy Rate: 67.11%,            Test_Total_Loss: 3.5751428604125977, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 432, Total Loss: 0.8865314722061157, Accuracy Rate: 67.20%,            Test_Total_Loss: 3.633068084716797, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 433, Total Loss: 0.8840994238853455, Accuracy Rate: 66.96%,            Test_Total_Loss: 3.6126983165740967, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 434, Total Loss: 0.8803451061248779, Accuracy Rate: 66.17%,            Test_Total_Loss: 3.683427572250366, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 435, Total Loss: 0.8580471873283386, Accuracy Rate: 67.39%,            Test_Total_Loss: 3.6954498291015625, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 436, Total Loss: 0.8530910015106201, Accuracy Rate: 67.25%,            Test_Total_Loss: 3.634899139404297, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 437, Total Loss: 0.8872106075286865, Accuracy Rate: 66.59%,            Test_Total_Loss: 3.7092721462249756, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 438, Total Loss: 0.8568214774131775, Accuracy Rate: 67.72%,            Test_Total_Loss: 3.658682346343994, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 439, Total Loss: 0.8649033308029175, Accuracy Rate: 66.78%,            Test_Total_Loss: 3.7409582138061523, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 440, Total Loss: 0.8559610247612, Accuracy Rate: 67.53%,            Test_Total_Loss: 3.761348247528076, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 441, Total Loss: 0.839998185634613, Accuracy Rate: 67.76%,            Test_Total_Loss: 3.803530216217041, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 442, Total Loss: 0.8381666541099548, Accuracy Rate: 68.89%,            Test_Total_Loss: 3.7949721813201904, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 443, Total Loss: 0.8429473638534546, Accuracy Rate: 67.53%,            Test_Total_Loss: 3.8636069297790527, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 444, Total Loss: 0.8724838495254517, Accuracy Rate: 66.73%,            Test_Total_Loss: 3.746898651123047, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 445, Total Loss: 0.8693102598190308, Accuracy Rate: 67.95%,            Test_Total_Loss: 3.740098714828491, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 446, Total Loss: 0.8729596138000488, Accuracy Rate: 67.20%,            Test_Total_Loss: 3.8781654834747314, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 447, Total Loss: 0.8953800201416016, Accuracy Rate: 66.02%,            Test_Total_Loss: 3.9584381580352783, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 448, Total Loss: 0.8933751583099365, Accuracy Rate: 66.54%,            Test_Total_Loss: 3.7581818103790283, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 449, Total Loss: 0.9172927141189575, Accuracy Rate: 65.32%,            Test_Total_Loss: 3.845339059829712, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 450, Total Loss: 0.883642315864563, Accuracy Rate: 65.88%,            Test_Total_Loss: 3.9723429679870605, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 451, Total Loss: 0.8723111152648926, Accuracy Rate: 66.96%,            Test_Total_Loss: 3.872323513031006, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 452, Total Loss: 0.8898895382881165, Accuracy Rate: 66.64%,            Test_Total_Loss: 3.7955992221832275, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 453, Total Loss: 0.9251601099967957, Accuracy Rate: 66.26%,            Test_Total_Loss: 3.9523417949676514, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 454, Total Loss: 0.9484540820121765, Accuracy Rate: 65.74%,            Test_Total_Loss: 3.7663753032684326, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 455, Total Loss: 0.972691535949707, Accuracy Rate: 64.76%,            Test_Total_Loss: 3.8336079120635986, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 456, Total Loss: 1.0218309164047241, Accuracy Rate: 62.69%,            Test_Total_Loss: 3.821535110473633, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 457, Total Loss: 0.9637313485145569, Accuracy Rate: 64.38%,            Test_Total_Loss: 3.928271770477295, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 458, Total Loss: 0.9382111430168152, Accuracy Rate: 65.23%,            Test_Total_Loss: 3.666588306427002, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 459, Total Loss: 0.9082277417182922, Accuracy Rate: 65.93%,            Test_Total_Loss: 3.7318100929260254, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 460, Total Loss: 0.8948177695274353, Accuracy Rate: 66.78%,            Test_Total_Loss: 3.863197088241577, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 461, Total Loss: 0.9239851832389832, Accuracy Rate: 65.46%,            Test_Total_Loss: 3.696584463119507, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 462, Total Loss: 0.8994690179824829, Accuracy Rate: 65.84%,            Test_Total_Loss: 3.8154163360595703, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 463, Total Loss: 0.9260477423667908, Accuracy Rate: 65.27%,            Test_Total_Loss: 3.8513004779815674, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 464, Total Loss: 0.9043061137199402, Accuracy Rate: 66.78%,            Test_Total_Loss: 3.8087849617004395, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 465, Total Loss: 0.87204509973526, Accuracy Rate: 67.53%,            Test_Total_Loss: 3.8686001300811768, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 466, Total Loss: 0.8605131506919861, Accuracy Rate: 67.48%,            Test_Total_Loss: 3.935103416442871, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 467, Total Loss: 0.8550185561180115, Accuracy Rate: 68.19%,            Test_Total_Loss: 3.875544548034668, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 468, Total Loss: 0.8428391218185425, Accuracy Rate: 68.37%,            Test_Total_Loss: 3.8831241130828857, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 469, Total Loss: 0.8395965695381165, Accuracy Rate: 68.19%,            Test_Total_Loss: 3.9027504920959473, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 470, Total Loss: 0.8308486342430115, Accuracy Rate: 68.14%,            Test_Total_Loss: 3.9243459701538086, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 471, Total Loss: 0.8341806530952454, Accuracy Rate: 68.14%,            Test_Total_Loss: 3.833967685699463, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 472, Total Loss: 0.8495349884033203, Accuracy Rate: 67.25%,            Test_Total_Loss: 3.7653162479400635, Test_Accuracy_Rate: 32.91%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473, Total Loss: 0.8756178617477417, Accuracy Rate: 66.78%,            Test_Total_Loss: 3.872957468032837, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 474, Total Loss: 0.860895574092865, Accuracy Rate: 67.11%,            Test_Total_Loss: 3.8655805587768555, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 475, Total Loss: 0.8602821230888367, Accuracy Rate: 67.25%,            Test_Total_Loss: 3.7887024879455566, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 476, Total Loss: 0.851980984210968, Accuracy Rate: 67.81%,            Test_Total_Loss: 3.8630359172821045, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 477, Total Loss: 0.8687626719474792, Accuracy Rate: 67.15%,            Test_Total_Loss: 3.7917842864990234, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 478, Total Loss: 0.9481485486030579, Accuracy Rate: 65.18%,            Test_Total_Loss: 3.967327356338501, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 479, Total Loss: 1.3710788488388062, Accuracy Rate: 57.28%,            Test_Total_Loss: 3.4420788288116455, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 480, Total Loss: 1.35552179813385, Accuracy Rate: 55.73%,            Test_Total_Loss: 3.637885570526123, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 481, Total Loss: 1.2009528875350952, Accuracy Rate: 59.02%,            Test_Total_Loss: 3.5445871353149414, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 482, Total Loss: 1.3647170066833496, Accuracy Rate: 56.25%,            Test_Total_Loss: 3.5119662284851074, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 483, Total Loss: 1.3029499053955078, Accuracy Rate: 57.14%,            Test_Total_Loss: 3.510411024093628, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 484, Total Loss: 1.22242271900177, Accuracy Rate: 58.51%,            Test_Total_Loss: 3.4118123054504395, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 485, Total Loss: 1.1250591278076172, Accuracy Rate: 61.56%,            Test_Total_Loss: 3.390756845474243, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 486, Total Loss: 1.0790857076644897, Accuracy Rate: 62.41%,            Test_Total_Loss: 3.425722599029541, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 487, Total Loss: 1.1301649808883667, Accuracy Rate: 61.75%,            Test_Total_Loss: 3.7535860538482666, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 488, Total Loss: 1.1727544069290161, Accuracy Rate: 59.96%,            Test_Total_Loss: 3.258631944656372, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 489, Total Loss: 1.2923548221588135, Accuracy Rate: 58.08%,            Test_Total_Loss: 3.364236354827881, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 490, Total Loss: 1.4809619188308716, Accuracy Rate: 54.65%,            Test_Total_Loss: 3.380669355392456, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 491, Total Loss: 1.2793430089950562, Accuracy Rate: 57.57%,            Test_Total_Loss: 3.407477855682373, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 492, Total Loss: 1.1583722829818726, Accuracy Rate: 60.95%,            Test_Total_Loss: 3.3056905269622803, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 493, Total Loss: 1.1019560098648071, Accuracy Rate: 63.06%,            Test_Total_Loss: 3.341836452484131, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 494, Total Loss: 1.0662891864776611, Accuracy Rate: 63.25%,            Test_Total_Loss: 3.3808212280273438, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 495, Total Loss: 1.0414458513259888, Accuracy Rate: 64.00%,            Test_Total_Loss: 3.4027628898620605, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 496, Total Loss: 1.0075187683105469, Accuracy Rate: 64.52%,            Test_Total_Loss: 3.35737681388855, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 497, Total Loss: 1.048693299293518, Accuracy Rate: 63.20%,            Test_Total_Loss: 3.3536887168884277, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 498, Total Loss: 1.0001925230026245, Accuracy Rate: 64.14%,            Test_Total_Loss: 3.3753585815429688, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 499, Total Loss: 0.9855883121490479, Accuracy Rate: 65.13%,            Test_Total_Loss: 3.353609561920166, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 500, Total Loss: 0.9576452374458313, Accuracy Rate: 65.51%,            Test_Total_Loss: 3.364286184310913, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 501, Total Loss: 0.9540031552314758, Accuracy Rate: 65.18%,            Test_Total_Loss: 3.4298088550567627, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 502, Total Loss: 0.9281800389289856, Accuracy Rate: 66.64%,            Test_Total_Loss: 3.4804067611694336, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 503, Total Loss: 0.9410902857780457, Accuracy Rate: 66.26%,            Test_Total_Loss: 3.464401960372925, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 504, Total Loss: 0.9323214292526245, Accuracy Rate: 66.31%,            Test_Total_Loss: 3.475083589553833, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 505, Total Loss: 0.9168793559074402, Accuracy Rate: 66.26%,            Test_Total_Loss: 3.448460102081299, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 506, Total Loss: 0.9218736290931702, Accuracy Rate: 66.68%,            Test_Total_Loss: 3.491973400115967, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 507, Total Loss: 0.9274148941040039, Accuracy Rate: 66.07%,            Test_Total_Loss: 3.496354103088379, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 508, Total Loss: 0.9207836985588074, Accuracy Rate: 65.98%,            Test_Total_Loss: 3.6150429248809814, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 509, Total Loss: 0.9378860592842102, Accuracy Rate: 65.74%,            Test_Total_Loss: 3.5020315647125244, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 510, Total Loss: 0.9390842318534851, Accuracy Rate: 66.40%,            Test_Total_Loss: 3.6240789890289307, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 511, Total Loss: 0.9480876922607422, Accuracy Rate: 65.41%,            Test_Total_Loss: 3.4946866035461426, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 512, Total Loss: 1.0037294626235962, Accuracy Rate: 64.71%,            Test_Total_Loss: 3.6218011379241943, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 513, Total Loss: 0.9661663770675659, Accuracy Rate: 65.84%,            Test_Total_Loss: 3.532529830932617, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 514, Total Loss: 0.9515663385391235, Accuracy Rate: 66.07%,            Test_Total_Loss: 3.354050397872925, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 515, Total Loss: 0.9783971905708313, Accuracy Rate: 65.51%,            Test_Total_Loss: 3.569528579711914, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 516, Total Loss: 0.9287005662918091, Accuracy Rate: 65.41%,            Test_Total_Loss: 3.537614345550537, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 517, Total Loss: 0.9299106597900391, Accuracy Rate: 66.96%,            Test_Total_Loss: 3.488276958465576, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 518, Total Loss: 0.9241637587547302, Accuracy Rate: 66.17%,            Test_Total_Loss: 3.550243854522705, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 519, Total Loss: 0.9659116268157959, Accuracy Rate: 64.47%,            Test_Total_Loss: 3.565387487411499, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 520, Total Loss: 0.9657570123672485, Accuracy Rate: 65.27%,            Test_Total_Loss: 3.582641124725342, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 521, Total Loss: 0.9690391421318054, Accuracy Rate: 64.10%,            Test_Total_Loss: 3.510488986968994, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 522, Total Loss: 0.928215742111206, Accuracy Rate: 65.37%,            Test_Total_Loss: 3.598848342895508, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 523, Total Loss: 0.8975048065185547, Accuracy Rate: 66.64%,            Test_Total_Loss: 3.634864568710327, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 524, Total Loss: 0.8980885744094849, Accuracy Rate: 66.07%,            Test_Total_Loss: 3.598651885986328, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 525, Total Loss: 0.8948914408683777, Accuracy Rate: 66.12%,            Test_Total_Loss: 3.608039617538452, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 526, Total Loss: 0.9055109024047852, Accuracy Rate: 66.26%,            Test_Total_Loss: 3.5970447063446045, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 527, Total Loss: 0.8929555416107178, Accuracy Rate: 67.11%,            Test_Total_Loss: 3.6894772052764893, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 528, Total Loss: 0.904403030872345, Accuracy Rate: 66.68%,            Test_Total_Loss: 3.6538615226745605, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 529, Total Loss: 0.8770931363105774, Accuracy Rate: 67.81%,            Test_Total_Loss: 3.6719584465026855, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 530, Total Loss: 0.882080078125, Accuracy Rate: 67.29%,            Test_Total_Loss: 3.694350004196167, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 531, Total Loss: 0.8757391571998596, Accuracy Rate: 67.76%,            Test_Total_Loss: 3.6640584468841553, Test_Accuracy_Rate: 34.60%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532, Total Loss: 0.8844109177589417, Accuracy Rate: 67.29%,            Test_Total_Loss: 3.6721208095550537, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 533, Total Loss: 0.8798748850822449, Accuracy Rate: 67.58%,            Test_Total_Loss: 3.6293020248413086, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 534, Total Loss: 0.910019040107727, Accuracy Rate: 66.35%,            Test_Total_Loss: 3.6999363899230957, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 535, Total Loss: 0.8766364455223083, Accuracy Rate: 66.59%,            Test_Total_Loss: 3.65165114402771, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 536, Total Loss: 0.8665140271186829, Accuracy Rate: 67.15%,            Test_Total_Loss: 3.7560787200927734, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 537, Total Loss: 0.879970371723175, Accuracy Rate: 66.59%,            Test_Total_Loss: 3.7073769569396973, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 538, Total Loss: 0.8859608173370361, Accuracy Rate: 66.35%,            Test_Total_Loss: 3.6982216835021973, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 539, Total Loss: 1.1579381227493286, Accuracy Rate: 61.98%,            Test_Total_Loss: 3.6968140602111816, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 540, Total Loss: 1.199712872505188, Accuracy Rate: 60.86%,            Test_Total_Loss: 3.473398208618164, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 541, Total Loss: 1.34006667137146, Accuracy Rate: 56.53%,            Test_Total_Loss: 3.487773895263672, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 542, Total Loss: 1.3787813186645508, Accuracy Rate: 55.64%,            Test_Total_Loss: 3.3315067291259766, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 543, Total Loss: 1.527012825012207, Accuracy Rate: 52.26%,            Test_Total_Loss: 3.2097880840301514, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 544, Total Loss: 1.4618819952011108, Accuracy Rate: 53.29%,            Test_Total_Loss: 3.0803005695343018, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 545, Total Loss: 1.403411865234375, Accuracy Rate: 54.93%,            Test_Total_Loss: 3.1182963848114014, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 546, Total Loss: 1.2845709323883057, Accuracy Rate: 57.00%,            Test_Total_Loss: 3.0533785820007324, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 547, Total Loss: 1.1942812204360962, Accuracy Rate: 60.34%,            Test_Total_Loss: 3.112753391265869, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 548, Total Loss: 1.1440128087997437, Accuracy Rate: 61.00%,            Test_Total_Loss: 3.1880104541778564, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 549, Total Loss: 1.111645221710205, Accuracy Rate: 61.37%,            Test_Total_Loss: 3.14582896232605, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 550, Total Loss: 1.0962735414505005, Accuracy Rate: 61.23%,            Test_Total_Loss: 3.173245668411255, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 551, Total Loss: 1.1060727834701538, Accuracy Rate: 61.47%,            Test_Total_Loss: 3.2739973068237305, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 552, Total Loss: 1.0949522256851196, Accuracy Rate: 62.12%,            Test_Total_Loss: 3.199773073196411, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 553, Total Loss: 1.0592575073242188, Accuracy Rate: 62.88%,            Test_Total_Loss: 3.180018901824951, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 554, Total Loss: 1.0926566123962402, Accuracy Rate: 62.31%,            Test_Total_Loss: 3.2403903007507324, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 555, Total Loss: 1.3441581726074219, Accuracy Rate: 55.78%,            Test_Total_Loss: 3.374067783355713, Test_Accuracy_Rate: 29.11%\n",
      "Epoch 556, Total Loss: 1.323232889175415, Accuracy Rate: 55.97%,            Test_Total_Loss: 3.3292784690856934, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 557, Total Loss: 1.1851316690444946, Accuracy Rate: 59.40%,            Test_Total_Loss: 3.2812860012054443, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 558, Total Loss: 1.1194102764129639, Accuracy Rate: 61.09%,            Test_Total_Loss: 3.1799871921539307, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 559, Total Loss: 1.1115425825119019, Accuracy Rate: 61.14%,            Test_Total_Loss: 3.1942214965820312, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 560, Total Loss: 1.0586210489273071, Accuracy Rate: 62.83%,            Test_Total_Loss: 3.1497366428375244, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 561, Total Loss: 1.023629069328308, Accuracy Rate: 63.63%,            Test_Total_Loss: 3.19647216796875, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 562, Total Loss: 1.009647250175476, Accuracy Rate: 64.29%,            Test_Total_Loss: 3.1918978691101074, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 563, Total Loss: 0.9953344464302063, Accuracy Rate: 64.47%,            Test_Total_Loss: 3.16715669631958, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 564, Total Loss: 0.975716233253479, Accuracy Rate: 63.72%,            Test_Total_Loss: 3.1781227588653564, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 565, Total Loss: 0.9562528133392334, Accuracy Rate: 65.70%,            Test_Total_Loss: 3.2545971870422363, Test_Accuracy_Rate: 37.55%\n",
      "Epoch 566, Total Loss: 0.9937623143196106, Accuracy Rate: 64.94%,            Test_Total_Loss: 3.2467358112335205, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 567, Total Loss: 1.0647928714752197, Accuracy Rate: 62.22%,            Test_Total_Loss: 3.3379790782928467, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 568, Total Loss: 1.0374057292938232, Accuracy Rate: 62.59%,            Test_Total_Loss: 3.3423562049865723, Test_Accuracy_Rate: 38.40%\n",
      "Epoch 569, Total Loss: 0.9761717915534973, Accuracy Rate: 64.43%,            Test_Total_Loss: 3.3156301975250244, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 570, Total Loss: 0.9597943425178528, Accuracy Rate: 65.84%,            Test_Total_Loss: 3.301149845123291, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 571, Total Loss: 0.9528538584709167, Accuracy Rate: 64.85%,            Test_Total_Loss: 3.358686685562134, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 572, Total Loss: 0.9393607974052429, Accuracy Rate: 65.41%,            Test_Total_Loss: 3.3353819847106934, Test_Accuracy_Rate: 39.24%\n",
      "Epoch 573, Total Loss: 0.9403972625732422, Accuracy Rate: 65.60%,            Test_Total_Loss: 3.456958770751953, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 574, Total Loss: 0.9481354355812073, Accuracy Rate: 64.61%,            Test_Total_Loss: 3.4350836277008057, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 575, Total Loss: 0.9411979913711548, Accuracy Rate: 64.43%,            Test_Total_Loss: 3.4322776794433594, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 576, Total Loss: 0.9543466567993164, Accuracy Rate: 65.88%,            Test_Total_Loss: 3.38273549079895, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 577, Total Loss: 0.9391546845436096, Accuracy Rate: 66.35%,            Test_Total_Loss: 3.4091944694519043, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 578, Total Loss: 0.9306979179382324, Accuracy Rate: 65.60%,            Test_Total_Loss: 3.502181053161621, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 579, Total Loss: 1.2702562808990479, Accuracy Rate: 58.55%,            Test_Total_Loss: 3.497603416442871, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 580, Total Loss: 1.3437175750732422, Accuracy Rate: 55.59%,            Test_Total_Loss: 3.3695480823516846, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 581, Total Loss: 1.24919593334198, Accuracy Rate: 58.18%,            Test_Total_Loss: 3.29856276512146, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 582, Total Loss: 1.1471070051193237, Accuracy Rate: 61.23%,            Test_Total_Loss: 3.268278121948242, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 583, Total Loss: 1.0987517833709717, Accuracy Rate: 62.59%,            Test_Total_Loss: 3.2045915126800537, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 584, Total Loss: 1.078942060470581, Accuracy Rate: 64.24%,            Test_Total_Loss: 3.3833138942718506, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 585, Total Loss: 1.0450831651687622, Accuracy Rate: 63.67%,            Test_Total_Loss: 3.407383441925049, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 586, Total Loss: 1.004819631576538, Accuracy Rate: 64.90%,            Test_Total_Loss: 3.4388785362243652, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 587, Total Loss: 1.0268349647521973, Accuracy Rate: 63.53%,            Test_Total_Loss: 3.4594736099243164, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 588, Total Loss: 1.0210676193237305, Accuracy Rate: 64.14%,            Test_Total_Loss: 3.4801840782165527, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 589, Total Loss: 1.0482670068740845, Accuracy Rate: 64.29%,            Test_Total_Loss: 3.465435266494751, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 590, Total Loss: 1.6452876329421997, Accuracy Rate: 56.95%,            Test_Total_Loss: 4.631242275238037, Test_Accuracy_Rate: 19.41%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591, Total Loss: 2.6526479721069336, Accuracy Rate: 34.02%,            Test_Total_Loss: 3.327413558959961, Test_Accuracy_Rate: 19.41%\n",
      "Epoch 592, Total Loss: 2.0724058151245117, Accuracy Rate: 37.17%,            Test_Total_Loss: 3.1205391883850098, Test_Accuracy_Rate: 22.36%\n",
      "Epoch 593, Total Loss: 2.123534917831421, Accuracy Rate: 34.54%,            Test_Total_Loss: 2.956885814666748, Test_Accuracy_Rate: 22.78%\n",
      "Epoch 594, Total Loss: 1.9163738489151, Accuracy Rate: 38.91%,            Test_Total_Loss: 2.831982135772705, Test_Accuracy_Rate: 28.69%\n",
      "Epoch 595, Total Loss: 1.7919459342956543, Accuracy Rate: 42.29%,            Test_Total_Loss: 2.839500904083252, Test_Accuracy_Rate: 29.11%\n",
      "Epoch 596, Total Loss: 1.725313663482666, Accuracy Rate: 43.52%,            Test_Total_Loss: 2.8318328857421875, Test_Accuracy_Rate: 26.16%\n",
      "Epoch 597, Total Loss: 1.6812361478805542, Accuracy Rate: 45.11%,            Test_Total_Loss: 2.7875068187713623, Test_Accuracy_Rate: 26.58%\n",
      "Epoch 598, Total Loss: 1.6584327220916748, Accuracy Rate: 46.01%,            Test_Total_Loss: 2.7785086631774902, Test_Accuracy_Rate: 28.27%\n",
      "Epoch 599, Total Loss: 1.6258970499038696, Accuracy Rate: 46.48%,            Test_Total_Loss: 2.7446811199188232, Test_Accuracy_Rate: 29.11%\n",
      "Epoch 600, Total Loss: 1.5961034297943115, Accuracy Rate: 47.84%,            Test_Total_Loss: 2.7778005599975586, Test_Accuracy_Rate: 29.96%\n",
      "Epoch 601, Total Loss: 1.5796258449554443, Accuracy Rate: 48.68%,            Test_Total_Loss: 2.7825560569763184, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 602, Total Loss: 1.5643823146820068, Accuracy Rate: 49.25%,            Test_Total_Loss: 2.772731304168701, Test_Accuracy_Rate: 29.11%\n",
      "Epoch 603, Total Loss: 1.5407475233078003, Accuracy Rate: 49.81%,            Test_Total_Loss: 2.767094373703003, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 604, Total Loss: 1.5177873373031616, Accuracy Rate: 49.72%,            Test_Total_Loss: 2.786569118499756, Test_Accuracy_Rate: 29.54%\n",
      "Epoch 605, Total Loss: 1.5088212490081787, Accuracy Rate: 50.47%,            Test_Total_Loss: 2.8246307373046875, Test_Accuracy_Rate: 29.11%\n",
      "Epoch 606, Total Loss: 1.4938218593597412, Accuracy Rate: 51.46%,            Test_Total_Loss: 2.804500102996826, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 607, Total Loss: 1.4737765789031982, Accuracy Rate: 52.44%,            Test_Total_Loss: 2.8122875690460205, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 608, Total Loss: 1.4586585760116577, Accuracy Rate: 52.40%,            Test_Total_Loss: 2.8238792419433594, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 609, Total Loss: 1.4533101320266724, Accuracy Rate: 52.40%,            Test_Total_Loss: 2.8652637004852295, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 610, Total Loss: 1.4248472452163696, Accuracy Rate: 53.48%,            Test_Total_Loss: 2.858062744140625, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 611, Total Loss: 1.4260778427124023, Accuracy Rate: 53.10%,            Test_Total_Loss: 2.8415839672088623, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 612, Total Loss: 1.406575322151184, Accuracy Rate: 54.23%,            Test_Total_Loss: 2.8677570819854736, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 613, Total Loss: 1.3860284090042114, Accuracy Rate: 54.65%,            Test_Total_Loss: 2.881680965423584, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 614, Total Loss: 1.3716316223144531, Accuracy Rate: 55.36%,            Test_Total_Loss: 2.926537036895752, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 615, Total Loss: 1.3520036935806274, Accuracy Rate: 55.17%,            Test_Total_Loss: 2.8824338912963867, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 616, Total Loss: 1.3428953886032104, Accuracy Rate: 56.48%,            Test_Total_Loss: 2.873842716217041, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 617, Total Loss: 1.3299930095672607, Accuracy Rate: 56.30%,            Test_Total_Loss: 2.977756977081299, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 618, Total Loss: 1.3354064226150513, Accuracy Rate: 56.39%,            Test_Total_Loss: 2.921560764312744, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 619, Total Loss: 1.3128769397735596, Accuracy Rate: 56.39%,            Test_Total_Loss: 2.8801982402801514, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 620, Total Loss: 1.3094769716262817, Accuracy Rate: 57.28%,            Test_Total_Loss: 2.8911592960357666, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 621, Total Loss: 1.2919656038284302, Accuracy Rate: 57.00%,            Test_Total_Loss: 2.9218602180480957, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 622, Total Loss: 1.3068112134933472, Accuracy Rate: 57.00%,            Test_Total_Loss: 2.882992744445801, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 623, Total Loss: 1.2880487442016602, Accuracy Rate: 57.89%,            Test_Total_Loss: 2.9337737560272217, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 624, Total Loss: 1.2713541984558105, Accuracy Rate: 58.93%,            Test_Total_Loss: 2.8791561126708984, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 625, Total Loss: 1.2666867971420288, Accuracy Rate: 58.04%,            Test_Total_Loss: 2.8365347385406494, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 626, Total Loss: 1.245937466621399, Accuracy Rate: 58.69%,            Test_Total_Loss: 2.889418601989746, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 627, Total Loss: 1.2388274669647217, Accuracy Rate: 58.41%,            Test_Total_Loss: 2.8647375106811523, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 628, Total Loss: 1.2361727952957153, Accuracy Rate: 58.04%,            Test_Total_Loss: 2.9042484760284424, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 629, Total Loss: 1.2330242395401, Accuracy Rate: 58.83%,            Test_Total_Loss: 2.9379098415374756, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 630, Total Loss: 1.2461931705474854, Accuracy Rate: 57.66%,            Test_Total_Loss: 2.917574167251587, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 631, Total Loss: 1.277062177658081, Accuracy Rate: 57.47%,            Test_Total_Loss: 2.9697682857513428, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 632, Total Loss: 1.2656193971633911, Accuracy Rate: 57.89%,            Test_Total_Loss: 2.9239394664764404, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 633, Total Loss: 1.2531099319458008, Accuracy Rate: 58.13%,            Test_Total_Loss: 2.9941060543060303, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 634, Total Loss: 1.2225501537322998, Accuracy Rate: 58.36%,            Test_Total_Loss: 2.964919090270996, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 635, Total Loss: 1.2030576467514038, Accuracy Rate: 59.73%,            Test_Total_Loss: 2.960063934326172, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 636, Total Loss: 1.1868869066238403, Accuracy Rate: 59.87%,            Test_Total_Loss: 2.983084201812744, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 637, Total Loss: 1.1758215427398682, Accuracy Rate: 60.48%,            Test_Total_Loss: 2.9528985023498535, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 638, Total Loss: 1.1993052959442139, Accuracy Rate: 58.88%,            Test_Total_Loss: 3.005587100982666, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 639, Total Loss: 1.1951344013214111, Accuracy Rate: 60.06%,            Test_Total_Loss: 2.991373300552368, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 640, Total Loss: 1.1837670803070068, Accuracy Rate: 60.43%,            Test_Total_Loss: 3.029388189315796, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 641, Total Loss: 1.1718640327453613, Accuracy Rate: 59.82%,            Test_Total_Loss: 3.1062588691711426, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 642, Total Loss: 1.1748390197753906, Accuracy Rate: 60.71%,            Test_Total_Loss: 3.1021406650543213, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 643, Total Loss: 1.2256317138671875, Accuracy Rate: 58.22%,            Test_Total_Loss: 3.050549268722534, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 644, Total Loss: 1.2660107612609863, Accuracy Rate: 57.05%,            Test_Total_Loss: 3.065659999847412, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 645, Total Loss: 1.2070198059082031, Accuracy Rate: 59.26%,            Test_Total_Loss: 3.0236878395080566, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 646, Total Loss: 1.220826268196106, Accuracy Rate: 58.27%,            Test_Total_Loss: 3.0905261039733887, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 647, Total Loss: 1.1752393245697021, Accuracy Rate: 59.49%,            Test_Total_Loss: 3.036020040512085, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 648, Total Loss: 1.1562367677688599, Accuracy Rate: 60.06%,            Test_Total_Loss: 3.0646462440490723, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 649, Total Loss: 1.1525267362594604, Accuracy Rate: 60.67%,            Test_Total_Loss: 3.088500499725342, Test_Accuracy_Rate: 34.60%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650, Total Loss: 1.1209639310836792, Accuracy Rate: 61.33%,            Test_Total_Loss: 3.133633613586426, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 651, Total Loss: 1.119465947151184, Accuracy Rate: 61.33%,            Test_Total_Loss: 3.16792368888855, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 652, Total Loss: 1.1066211462020874, Accuracy Rate: 61.23%,            Test_Total_Loss: 3.1468594074249268, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 653, Total Loss: 1.1027899980545044, Accuracy Rate: 61.94%,            Test_Total_Loss: 3.0943751335144043, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 654, Total Loss: 1.0892536640167236, Accuracy Rate: 61.98%,            Test_Total_Loss: 3.14272403717041, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 655, Total Loss: 1.0759299993515015, Accuracy Rate: 62.69%,            Test_Total_Loss: 3.196131467819214, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 656, Total Loss: 1.0980825424194336, Accuracy Rate: 62.08%,            Test_Total_Loss: 3.1910200119018555, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 657, Total Loss: 1.1248658895492554, Accuracy Rate: 61.09%,            Test_Total_Loss: 3.17488956451416, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 658, Total Loss: 1.1636626720428467, Accuracy Rate: 60.24%,            Test_Total_Loss: 3.1407623291015625, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 659, Total Loss: 1.1176645755767822, Accuracy Rate: 60.76%,            Test_Total_Loss: 3.101732015609741, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 660, Total Loss: 1.1116262674331665, Accuracy Rate: 61.42%,            Test_Total_Loss: 3.1383533477783203, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 661, Total Loss: 1.107875943183899, Accuracy Rate: 62.12%,            Test_Total_Loss: 3.13134765625, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 662, Total Loss: 1.0900226831436157, Accuracy Rate: 62.17%,            Test_Total_Loss: 3.128791093826294, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 663, Total Loss: 1.1242774724960327, Accuracy Rate: 61.42%,            Test_Total_Loss: 3.1128501892089844, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 664, Total Loss: 1.0978368520736694, Accuracy Rate: 61.98%,            Test_Total_Loss: 3.128807783126831, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 665, Total Loss: 1.0703874826431274, Accuracy Rate: 62.55%,            Test_Total_Loss: 3.0766360759735107, Test_Accuracy_Rate: 37.55%\n",
      "Epoch 666, Total Loss: 1.0695618391036987, Accuracy Rate: 62.50%,            Test_Total_Loss: 3.141329765319824, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 667, Total Loss: 1.0967265367507935, Accuracy Rate: 61.47%,            Test_Total_Loss: 3.1413748264312744, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 668, Total Loss: 1.2138949632644653, Accuracy Rate: 58.55%,            Test_Total_Loss: 3.0710761547088623, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 669, Total Loss: 1.2548000812530518, Accuracy Rate: 58.08%,            Test_Total_Loss: 3.2821309566497803, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 670, Total Loss: 1.1642262935638428, Accuracy Rate: 60.62%,            Test_Total_Loss: 3.2427310943603516, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 671, Total Loss: 1.126449465751648, Accuracy Rate: 60.95%,            Test_Total_Loss: 3.1926653385162354, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 672, Total Loss: 1.0823322534561157, Accuracy Rate: 63.06%,            Test_Total_Loss: 3.202955484390259, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 673, Total Loss: 1.0774956941604614, Accuracy Rate: 62.55%,            Test_Total_Loss: 3.1571404933929443, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 674, Total Loss: 1.0756208896636963, Accuracy Rate: 62.78%,            Test_Total_Loss: 3.1974477767944336, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 675, Total Loss: 1.0566548109054565, Accuracy Rate: 63.35%,            Test_Total_Loss: 3.283085346221924, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 676, Total Loss: 1.0368318557739258, Accuracy Rate: 63.86%,            Test_Total_Loss: 3.1951775550842285, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 677, Total Loss: 1.062261939048767, Accuracy Rate: 63.30%,            Test_Total_Loss: 3.1692240238189697, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 678, Total Loss: 1.0573326349258423, Accuracy Rate: 62.64%,            Test_Total_Loss: 3.163999557495117, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 679, Total Loss: 1.028281331062317, Accuracy Rate: 64.10%,            Test_Total_Loss: 3.177338123321533, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 680, Total Loss: 1.0530586242675781, Accuracy Rate: 63.49%,            Test_Total_Loss: 3.2243542671203613, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 681, Total Loss: 1.039505124092102, Accuracy Rate: 64.33%,            Test_Total_Loss: 3.37926983833313, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 682, Total Loss: 1.0395177602767944, Accuracy Rate: 63.53%,            Test_Total_Loss: 3.2806894779205322, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 683, Total Loss: 1.0314531326293945, Accuracy Rate: 64.43%,            Test_Total_Loss: 3.3325021266937256, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 684, Total Loss: 1.0597816705703735, Accuracy Rate: 62.83%,            Test_Total_Loss: 3.2295918464660645, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 685, Total Loss: 1.009700059890747, Accuracy Rate: 64.66%,            Test_Total_Loss: 3.3015406131744385, Test_Accuracy_Rate: 37.55%\n",
      "Epoch 686, Total Loss: 1.001409649848938, Accuracy Rate: 64.80%,            Test_Total_Loss: 3.3243653774261475, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 687, Total Loss: 1.0034196376800537, Accuracy Rate: 65.23%,            Test_Total_Loss: 3.3343148231506348, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 688, Total Loss: 1.017045021057129, Accuracy Rate: 64.94%,            Test_Total_Loss: 3.369758129119873, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 689, Total Loss: 1.0305726528167725, Accuracy Rate: 63.96%,            Test_Total_Loss: 3.362091541290283, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 690, Total Loss: 1.0559415817260742, Accuracy Rate: 63.49%,            Test_Total_Loss: 3.3926825523376465, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 691, Total Loss: 1.1664674282073975, Accuracy Rate: 61.09%,            Test_Total_Loss: 3.318462610244751, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 692, Total Loss: 1.0908591747283936, Accuracy Rate: 62.08%,            Test_Total_Loss: 3.3490214347839355, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 693, Total Loss: 1.1133086681365967, Accuracy Rate: 61.51%,            Test_Total_Loss: 3.3579952716827393, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 694, Total Loss: 1.051498293876648, Accuracy Rate: 63.72%,            Test_Total_Loss: 3.239861488342285, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 695, Total Loss: 1.0483258962631226, Accuracy Rate: 64.00%,            Test_Total_Loss: 3.3178441524505615, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 696, Total Loss: 1.09268319606781, Accuracy Rate: 62.97%,            Test_Total_Loss: 3.3401942253112793, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 697, Total Loss: 1.0585540533065796, Accuracy Rate: 63.67%,            Test_Total_Loss: 3.3108789920806885, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 698, Total Loss: 1.0227872133255005, Accuracy Rate: 65.08%,            Test_Total_Loss: 3.3714394569396973, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 699, Total Loss: 1.0094107389450073, Accuracy Rate: 64.52%,            Test_Total_Loss: 3.355255603790283, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 700, Total Loss: 0.9942719340324402, Accuracy Rate: 64.29%,            Test_Total_Loss: 3.3428897857666016, Test_Accuracy_Rate: 37.55%\n",
      "Epoch 701, Total Loss: 0.9968177676200867, Accuracy Rate: 64.29%,            Test_Total_Loss: 3.4559295177459717, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 702, Total Loss: 1.0051114559173584, Accuracy Rate: 65.32%,            Test_Total_Loss: 3.3655762672424316, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 703, Total Loss: 0.9938822388648987, Accuracy Rate: 65.13%,            Test_Total_Loss: 3.414074659347534, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 704, Total Loss: 0.9890691041946411, Accuracy Rate: 64.85%,            Test_Total_Loss: 3.4664387702941895, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 705, Total Loss: 0.9943885207176208, Accuracy Rate: 64.71%,            Test_Total_Loss: 3.3754923343658447, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 706, Total Loss: 1.0475326776504517, Accuracy Rate: 63.25%,            Test_Total_Loss: 3.4488186836242676, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 707, Total Loss: 1.0133322477340698, Accuracy Rate: 63.91%,            Test_Total_Loss: 3.471583843231201, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 708, Total Loss: 1.0168828964233398, Accuracy Rate: 64.80%,            Test_Total_Loss: 3.378247022628784, Test_Accuracy_Rate: 36.29%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709, Total Loss: 1.0582165718078613, Accuracy Rate: 63.67%,            Test_Total_Loss: 3.4057130813598633, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 710, Total Loss: 1.034250020980835, Accuracy Rate: 63.58%,            Test_Total_Loss: 3.505438804626465, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 711, Total Loss: 1.0140085220336914, Accuracy Rate: 64.29%,            Test_Total_Loss: 3.392421007156372, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 712, Total Loss: 1.0358648300170898, Accuracy Rate: 64.05%,            Test_Total_Loss: 3.446798801422119, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 713, Total Loss: 1.0265125036239624, Accuracy Rate: 63.67%,            Test_Total_Loss: 3.432777166366577, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 714, Total Loss: 0.9960617423057556, Accuracy Rate: 64.66%,            Test_Total_Loss: 3.465736150741577, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 715, Total Loss: 0.9784305691719055, Accuracy Rate: 65.27%,            Test_Total_Loss: 3.509094476699829, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 716, Total Loss: 0.9570907950401306, Accuracy Rate: 64.99%,            Test_Total_Loss: 3.5633349418640137, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 717, Total Loss: 0.9507703185081482, Accuracy Rate: 65.74%,            Test_Total_Loss: 3.482405424118042, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 718, Total Loss: 0.9439181089401245, Accuracy Rate: 65.84%,            Test_Total_Loss: 3.4956393241882324, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 719, Total Loss: 0.953171968460083, Accuracy Rate: 65.23%,            Test_Total_Loss: 3.4331350326538086, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 720, Total Loss: 0.9783916473388672, Accuracy Rate: 64.90%,            Test_Total_Loss: 3.4901955127716064, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 721, Total Loss: 0.9699548482894897, Accuracy Rate: 65.60%,            Test_Total_Loss: 3.4479150772094727, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 722, Total Loss: 0.9711815118789673, Accuracy Rate: 64.99%,            Test_Total_Loss: 3.564197063446045, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 723, Total Loss: 0.963361382484436, Accuracy Rate: 65.41%,            Test_Total_Loss: 3.501084804534912, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 724, Total Loss: 1.0145912170410156, Accuracy Rate: 64.14%,            Test_Total_Loss: 3.582990884780884, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 725, Total Loss: 1.0229536294937134, Accuracy Rate: 64.24%,            Test_Total_Loss: 3.4735145568847656, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 726, Total Loss: 1.017319679260254, Accuracy Rate: 64.00%,            Test_Total_Loss: 3.5165348052978516, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 727, Total Loss: 1.0221894979476929, Accuracy Rate: 64.14%,            Test_Total_Loss: 3.498392343521118, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 728, Total Loss: 1.0427440404891968, Accuracy Rate: 63.72%,            Test_Total_Loss: 3.4632670879364014, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 729, Total Loss: 1.0583562850952148, Accuracy Rate: 62.83%,            Test_Total_Loss: 3.499620199203491, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 730, Total Loss: 1.1449235677719116, Accuracy Rate: 60.76%,            Test_Total_Loss: 3.2879528999328613, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 731, Total Loss: 1.0769305229187012, Accuracy Rate: 62.45%,            Test_Total_Loss: 3.4791038036346436, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 732, Total Loss: 1.0012677907943726, Accuracy Rate: 65.18%,            Test_Total_Loss: 3.4800825119018555, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 733, Total Loss: 0.9848511219024658, Accuracy Rate: 64.85%,            Test_Total_Loss: 3.4734647274017334, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 734, Total Loss: 0.9907976984977722, Accuracy Rate: 64.52%,            Test_Total_Loss: 3.520643949508667, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 735, Total Loss: 0.9892547130584717, Accuracy Rate: 64.52%,            Test_Total_Loss: 3.4553184509277344, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 736, Total Loss: 0.9436531662940979, Accuracy Rate: 65.84%,            Test_Total_Loss: 3.4785704612731934, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 737, Total Loss: 0.9429888129234314, Accuracy Rate: 65.60%,            Test_Total_Loss: 3.542330503463745, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 738, Total Loss: 0.9366378784179688, Accuracy Rate: 65.98%,            Test_Total_Loss: 3.5395584106445312, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 739, Total Loss: 0.9517070651054382, Accuracy Rate: 65.55%,            Test_Total_Loss: 3.5012025833129883, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 740, Total Loss: 1.0437113046646118, Accuracy Rate: 63.39%,            Test_Total_Loss: 3.6435258388519287, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 741, Total Loss: 1.0455752611160278, Accuracy Rate: 63.67%,            Test_Total_Loss: 3.5007762908935547, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 742, Total Loss: 1.015208125114441, Accuracy Rate: 63.96%,            Test_Total_Loss: 3.539303779602051, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 743, Total Loss: 0.98328697681427, Accuracy Rate: 65.13%,            Test_Total_Loss: 3.5666520595550537, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 744, Total Loss: 0.9453322291374207, Accuracy Rate: 65.37%,            Test_Total_Loss: 3.4967827796936035, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 745, Total Loss: 0.9556848406791687, Accuracy Rate: 65.55%,            Test_Total_Loss: 3.5712926387786865, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 746, Total Loss: 1.0040804147720337, Accuracy Rate: 64.61%,            Test_Total_Loss: 3.5278992652893066, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 747, Total Loss: 1.0654767751693726, Accuracy Rate: 63.67%,            Test_Total_Loss: 3.5391228199005127, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 748, Total Loss: 1.050831913948059, Accuracy Rate: 64.14%,            Test_Total_Loss: 3.6064634323120117, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 749, Total Loss: 1.0370570421218872, Accuracy Rate: 63.11%,            Test_Total_Loss: 3.5144715309143066, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 750, Total Loss: 1.0266423225402832, Accuracy Rate: 64.47%,            Test_Total_Loss: 3.605806589126587, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 751, Total Loss: 1.0591976642608643, Accuracy Rate: 63.91%,            Test_Total_Loss: 3.681837558746338, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 752, Total Loss: 1.1248860359191895, Accuracy Rate: 62.69%,            Test_Total_Loss: 3.6635732650756836, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 753, Total Loss: 1.1495100259780884, Accuracy Rate: 61.56%,            Test_Total_Loss: 3.5489799976348877, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 754, Total Loss: 1.0703387260437012, Accuracy Rate: 63.86%,            Test_Total_Loss: 3.4809272289276123, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 755, Total Loss: 1.0379787683486938, Accuracy Rate: 64.05%,            Test_Total_Loss: 3.467046022415161, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 756, Total Loss: 1.0249481201171875, Accuracy Rate: 64.38%,            Test_Total_Loss: 3.5758047103881836, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 757, Total Loss: 1.014013409614563, Accuracy Rate: 64.66%,            Test_Total_Loss: 3.5467801094055176, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 758, Total Loss: 1.0079617500305176, Accuracy Rate: 65.46%,            Test_Total_Loss: 3.4889070987701416, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 759, Total Loss: 0.967197597026825, Accuracy Rate: 65.74%,            Test_Total_Loss: 3.506819248199463, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 760, Total Loss: 0.9638296961784363, Accuracy Rate: 65.55%,            Test_Total_Loss: 3.563488483428955, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 761, Total Loss: 0.9489290118217468, Accuracy Rate: 65.70%,            Test_Total_Loss: 3.6162846088409424, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 762, Total Loss: 0.9277216196060181, Accuracy Rate: 65.88%,            Test_Total_Loss: 3.5722293853759766, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 763, Total Loss: 1.1311280727386475, Accuracy Rate: 61.51%,            Test_Total_Loss: 3.485931873321533, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 764, Total Loss: 1.0955971479415894, Accuracy Rate: 62.50%,            Test_Total_Loss: 3.5427744388580322, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 765, Total Loss: 1.0918126106262207, Accuracy Rate: 62.03%,            Test_Total_Loss: 3.4650895595550537, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 766, Total Loss: 1.1426453590393066, Accuracy Rate: 61.28%,            Test_Total_Loss: 3.391794204711914, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 767, Total Loss: 1.0547200441360474, Accuracy Rate: 63.35%,            Test_Total_Loss: 3.462782382965088, Test_Accuracy_Rate: 33.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 768, Total Loss: 0.9950630068778992, Accuracy Rate: 64.19%,            Test_Total_Loss: 3.3826143741607666, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 769, Total Loss: 0.973121166229248, Accuracy Rate: 64.90%,            Test_Total_Loss: 3.4522993564605713, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 770, Total Loss: 0.9587992429733276, Accuracy Rate: 65.65%,            Test_Total_Loss: 3.459968090057373, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 771, Total Loss: 0.9363917112350464, Accuracy Rate: 66.78%,            Test_Total_Loss: 3.456517457962036, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 772, Total Loss: 0.923430323600769, Accuracy Rate: 66.35%,            Test_Total_Loss: 3.452338218688965, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 773, Total Loss: 1.0662145614624023, Accuracy Rate: 64.00%,            Test_Total_Loss: 3.4489645957946777, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 774, Total Loss: 1.2518894672393799, Accuracy Rate: 59.12%,            Test_Total_Loss: 3.4119458198547363, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 775, Total Loss: 1.108032464981079, Accuracy Rate: 61.70%,            Test_Total_Loss: 3.3671364784240723, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 776, Total Loss: 1.082392692565918, Accuracy Rate: 62.73%,            Test_Total_Loss: 3.337059736251831, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 777, Total Loss: 1.100155234336853, Accuracy Rate: 62.73%,            Test_Total_Loss: 3.3498647212982178, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 778, Total Loss: 1.1768734455108643, Accuracy Rate: 60.34%,            Test_Total_Loss: 3.35136342048645, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 779, Total Loss: 1.1562970876693726, Accuracy Rate: 60.90%,            Test_Total_Loss: 3.325666904449463, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 780, Total Loss: 1.0574486255645752, Accuracy Rate: 63.39%,            Test_Total_Loss: 3.3268680572509766, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 781, Total Loss: 1.0153566598892212, Accuracy Rate: 64.94%,            Test_Total_Loss: 3.3443527221679688, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 782, Total Loss: 0.9705011248588562, Accuracy Rate: 65.74%,            Test_Total_Loss: 3.409628391265869, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 783, Total Loss: 0.9477120637893677, Accuracy Rate: 66.49%,            Test_Total_Loss: 3.3860645294189453, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 784, Total Loss: 0.9538711309432983, Accuracy Rate: 65.93%,            Test_Total_Loss: 3.400949001312256, Test_Accuracy_Rate: 37.97%\n",
      "Epoch 785, Total Loss: 0.9312117695808411, Accuracy Rate: 65.93%,            Test_Total_Loss: 3.3908796310424805, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 786, Total Loss: 0.9249650239944458, Accuracy Rate: 65.93%,            Test_Total_Loss: 3.4181253910064697, Test_Accuracy_Rate: 39.66%\n",
      "Epoch 787, Total Loss: 0.9219820499420166, Accuracy Rate: 66.31%,            Test_Total_Loss: 3.436788558959961, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 788, Total Loss: 0.9210342764854431, Accuracy Rate: 66.78%,            Test_Total_Loss: 3.4367446899414062, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 789, Total Loss: 0.9572229981422424, Accuracy Rate: 64.99%,            Test_Total_Loss: 3.499495506286621, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 790, Total Loss: 1.0687919855117798, Accuracy Rate: 62.97%,            Test_Total_Loss: 3.491912603378296, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 791, Total Loss: 0.9921520948410034, Accuracy Rate: 64.24%,            Test_Total_Loss: 3.4781970977783203, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 792, Total Loss: 0.9701244235038757, Accuracy Rate: 65.32%,            Test_Total_Loss: 3.387922763824463, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 793, Total Loss: 0.941010594367981, Accuracy Rate: 65.70%,            Test_Total_Loss: 3.483205795288086, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 794, Total Loss: 0.9458714723587036, Accuracy Rate: 64.61%,            Test_Total_Loss: 3.396131753921509, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 795, Total Loss: 0.9378077387809753, Accuracy Rate: 65.37%,            Test_Total_Loss: 3.3974297046661377, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 796, Total Loss: 0.9214919209480286, Accuracy Rate: 66.82%,            Test_Total_Loss: 3.425515651702881, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 797, Total Loss: 0.9184095859527588, Accuracy Rate: 67.53%,            Test_Total_Loss: 3.388948917388916, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 798, Total Loss: 0.9051744341850281, Accuracy Rate: 66.31%,            Test_Total_Loss: 3.4396164417266846, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 799, Total Loss: 0.904196560382843, Accuracy Rate: 67.43%,            Test_Total_Loss: 3.415200710296631, Test_Accuracy_Rate: 37.97%\n",
      "Epoch 800, Total Loss: 0.8834952116012573, Accuracy Rate: 67.62%,            Test_Total_Loss: 3.4191105365753174, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 801, Total Loss: 0.8939608335494995, Accuracy Rate: 66.92%,            Test_Total_Loss: 3.466320514678955, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 802, Total Loss: 0.9139375686645508, Accuracy Rate: 67.25%,            Test_Total_Loss: 3.5757622718811035, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 803, Total Loss: 0.9639399647712708, Accuracy Rate: 66.02%,            Test_Total_Loss: 3.53760027885437, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 804, Total Loss: 0.9513522982597351, Accuracy Rate: 64.85%,            Test_Total_Loss: 3.683986186981201, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 805, Total Loss: 0.9599726796150208, Accuracy Rate: 65.55%,            Test_Total_Loss: 3.5302302837371826, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 806, Total Loss: 0.9307065010070801, Accuracy Rate: 66.73%,            Test_Total_Loss: 3.5935511589050293, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 807, Total Loss: 0.9150474071502686, Accuracy Rate: 65.88%,            Test_Total_Loss: 3.5369977951049805, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 808, Total Loss: 0.9024155735969543, Accuracy Rate: 66.59%,            Test_Total_Loss: 3.585273265838623, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 809, Total Loss: 0.8869950175285339, Accuracy Rate: 68.05%,            Test_Total_Loss: 3.607947826385498, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 810, Total Loss: 0.9019661545753479, Accuracy Rate: 66.12%,            Test_Total_Loss: 3.6258771419525146, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 811, Total Loss: 0.8974828720092773, Accuracy Rate: 67.01%,            Test_Total_Loss: 3.5917110443115234, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 812, Total Loss: 0.8892254829406738, Accuracy Rate: 67.34%,            Test_Total_Loss: 3.6106605529785156, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 813, Total Loss: 0.9076975584030151, Accuracy Rate: 66.35%,            Test_Total_Loss: 3.6660308837890625, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 814, Total Loss: 0.9664122462272644, Accuracy Rate: 64.71%,            Test_Total_Loss: 3.5216991901397705, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 815, Total Loss: 0.9378169178962708, Accuracy Rate: 65.32%,            Test_Total_Loss: 3.553896188735962, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 816, Total Loss: 0.9366900324821472, Accuracy Rate: 66.35%,            Test_Total_Loss: 3.673693895339966, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 817, Total Loss: 0.9387494325637817, Accuracy Rate: 65.74%,            Test_Total_Loss: 3.5249242782592773, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 818, Total Loss: 0.924418568611145, Accuracy Rate: 66.02%,            Test_Total_Loss: 3.6106057167053223, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 819, Total Loss: 0.9158025979995728, Accuracy Rate: 66.54%,            Test_Total_Loss: 3.5158236026763916, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 820, Total Loss: 0.8972348570823669, Accuracy Rate: 67.06%,            Test_Total_Loss: 3.588529586791992, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 821, Total Loss: 0.8821582794189453, Accuracy Rate: 67.15%,            Test_Total_Loss: 3.622948169708252, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 822, Total Loss: 0.8843649625778198, Accuracy Rate: 67.15%,            Test_Total_Loss: 3.551846504211426, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 823, Total Loss: 0.9211968779563904, Accuracy Rate: 66.17%,            Test_Total_Loss: 3.546072483062744, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 824, Total Loss: 0.8921073079109192, Accuracy Rate: 67.06%,            Test_Total_Loss: 3.6052000522613525, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 825, Total Loss: 0.8903061151504517, Accuracy Rate: 67.01%,            Test_Total_Loss: 3.6828560829162598, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 826, Total Loss: 0.876058042049408, Accuracy Rate: 67.15%,            Test_Total_Loss: 3.5712218284606934, Test_Accuracy_Rate: 35.86%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 827, Total Loss: 0.8771533370018005, Accuracy Rate: 67.34%,            Test_Total_Loss: 3.571251153945923, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 828, Total Loss: 0.8743841648101807, Accuracy Rate: 67.01%,            Test_Total_Loss: 3.6408843994140625, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 829, Total Loss: 0.8823041915893555, Accuracy Rate: 67.58%,            Test_Total_Loss: 3.577441453933716, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 830, Total Loss: 0.8789247274398804, Accuracy Rate: 66.96%,            Test_Total_Loss: 3.572653293609619, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 831, Total Loss: 0.9086599349975586, Accuracy Rate: 66.78%,            Test_Total_Loss: 3.588123321533203, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 832, Total Loss: 0.8981292843818665, Accuracy Rate: 66.96%,            Test_Total_Loss: 3.590162754058838, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 833, Total Loss: 0.8963637351989746, Accuracy Rate: 65.88%,            Test_Total_Loss: 3.640251636505127, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 834, Total Loss: 0.9175441861152649, Accuracy Rate: 65.98%,            Test_Total_Loss: 3.5771923065185547, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 835, Total Loss: 0.9184623956680298, Accuracy Rate: 66.45%,            Test_Total_Loss: 3.6128604412078857, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 836, Total Loss: 0.8844620585441589, Accuracy Rate: 67.62%,            Test_Total_Loss: 3.6784048080444336, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 837, Total Loss: 0.8606159090995789, Accuracy Rate: 67.20%,            Test_Total_Loss: 3.6682472229003906, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 838, Total Loss: 0.8573763370513916, Accuracy Rate: 67.90%,            Test_Total_Loss: 3.6522159576416016, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 839, Total Loss: 0.8346244096755981, Accuracy Rate: 67.86%,            Test_Total_Loss: 3.690117120742798, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 840, Total Loss: 0.837070643901825, Accuracy Rate: 68.14%,            Test_Total_Loss: 3.730760097503662, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 841, Total Loss: 0.8558200597763062, Accuracy Rate: 67.86%,            Test_Total_Loss: 3.8535943031311035, Test_Accuracy_Rate: 29.96%\n",
      "Epoch 842, Total Loss: 0.9107520580291748, Accuracy Rate: 66.59%,            Test_Total_Loss: 3.733421564102173, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 843, Total Loss: 0.9361238479614258, Accuracy Rate: 65.98%,            Test_Total_Loss: 3.6698946952819824, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 844, Total Loss: 0.923914909362793, Accuracy Rate: 66.17%,            Test_Total_Loss: 3.722862958908081, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 845, Total Loss: 0.8855795860290527, Accuracy Rate: 66.40%,            Test_Total_Loss: 3.722221851348877, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 846, Total Loss: 0.8779198527336121, Accuracy Rate: 67.29%,            Test_Total_Loss: 3.69655179977417, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 847, Total Loss: 0.8679251670837402, Accuracy Rate: 68.14%,            Test_Total_Loss: 3.6448631286621094, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 848, Total Loss: 0.8605578541755676, Accuracy Rate: 68.66%,            Test_Total_Loss: 3.6954240798950195, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 849, Total Loss: 0.8721884489059448, Accuracy Rate: 68.19%,            Test_Total_Loss: 3.7510101795196533, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 850, Total Loss: 0.9062982797622681, Accuracy Rate: 66.45%,            Test_Total_Loss: 3.7517035007476807, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 851, Total Loss: 0.9036296606063843, Accuracy Rate: 66.64%,            Test_Total_Loss: 3.620884418487549, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 852, Total Loss: 0.965386688709259, Accuracy Rate: 65.70%,            Test_Total_Loss: 3.8046085834503174, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 853, Total Loss: 0.9640651941299438, Accuracy Rate: 65.41%,            Test_Total_Loss: 3.59610915184021, Test_Accuracy_Rate: 37.55%\n",
      "Epoch 854, Total Loss: 0.9281882047653198, Accuracy Rate: 66.21%,            Test_Total_Loss: 3.6188149452209473, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 855, Total Loss: 0.9175606369972229, Accuracy Rate: 67.25%,            Test_Total_Loss: 3.5906219482421875, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 856, Total Loss: 0.8915800452232361, Accuracy Rate: 66.64%,            Test_Total_Loss: 3.652395248413086, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 857, Total Loss: 0.8733412623405457, Accuracy Rate: 67.48%,            Test_Total_Loss: 3.6240291595458984, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 858, Total Loss: 0.8740091323852539, Accuracy Rate: 66.78%,            Test_Total_Loss: 3.5969293117523193, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 859, Total Loss: 0.8591960668563843, Accuracy Rate: 67.90%,            Test_Total_Loss: 3.5784096717834473, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 860, Total Loss: 0.8858093619346619, Accuracy Rate: 66.64%,            Test_Total_Loss: 3.5952346324920654, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 861, Total Loss: 0.9039149284362793, Accuracy Rate: 67.01%,            Test_Total_Loss: 3.6083688735961914, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 862, Total Loss: 0.8612785935401917, Accuracy Rate: 67.34%,            Test_Total_Loss: 3.6006522178649902, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 863, Total Loss: 0.8601035475730896, Accuracy Rate: 67.81%,            Test_Total_Loss: 3.688277244567871, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 864, Total Loss: 0.8550322651863098, Accuracy Rate: 68.05%,            Test_Total_Loss: 3.644052267074585, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 865, Total Loss: 0.882558286190033, Accuracy Rate: 66.54%,            Test_Total_Loss: 3.6576859951019287, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 866, Total Loss: 0.8585649728775024, Accuracy Rate: 68.00%,            Test_Total_Loss: 3.7221860885620117, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 867, Total Loss: 0.8340301513671875, Accuracy Rate: 67.95%,            Test_Total_Loss: 3.670245409011841, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 868, Total Loss: 0.8356052041053772, Accuracy Rate: 68.42%,            Test_Total_Loss: 3.7510881423950195, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 869, Total Loss: 0.8381290435791016, Accuracy Rate: 68.70%,            Test_Total_Loss: 3.8723671436309814, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 870, Total Loss: 0.8602715134620667, Accuracy Rate: 68.23%,            Test_Total_Loss: 3.671320676803589, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 871, Total Loss: 0.8660186529159546, Accuracy Rate: 67.58%,            Test_Total_Loss: 3.597252130508423, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 872, Total Loss: 0.8529726266860962, Accuracy Rate: 67.86%,            Test_Total_Loss: 3.732896327972412, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 873, Total Loss: 0.8793464303016663, Accuracy Rate: 66.45%,            Test_Total_Loss: 3.762636423110962, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 874, Total Loss: 0.8652266263961792, Accuracy Rate: 67.48%,            Test_Total_Loss: 3.7754106521606445, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 875, Total Loss: 0.8612347841262817, Accuracy Rate: 67.72%,            Test_Total_Loss: 3.7801167964935303, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 876, Total Loss: 0.8708605766296387, Accuracy Rate: 66.82%,            Test_Total_Loss: 3.6678855419158936, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 877, Total Loss: 0.9241659641265869, Accuracy Rate: 65.98%,            Test_Total_Loss: 3.6410558223724365, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 878, Total Loss: 0.8893944025039673, Accuracy Rate: 66.92%,            Test_Total_Loss: 3.6031880378723145, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 879, Total Loss: 0.8889222145080566, Accuracy Rate: 66.78%,            Test_Total_Loss: 3.686251163482666, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 880, Total Loss: 0.8707007169723511, Accuracy Rate: 67.53%,            Test_Total_Loss: 3.6877081394195557, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 881, Total Loss: 0.8758690357208252, Accuracy Rate: 67.81%,            Test_Total_Loss: 3.707212448120117, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 882, Total Loss: 0.8474422097206116, Accuracy Rate: 68.05%,            Test_Total_Loss: 3.7072253227233887, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 883, Total Loss: 0.8379231095314026, Accuracy Rate: 67.67%,            Test_Total_Loss: 3.7686684131622314, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 884, Total Loss: 0.8367025256156921, Accuracy Rate: 67.76%,            Test_Total_Loss: 3.6889469623565674, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 885, Total Loss: 0.8288356065750122, Accuracy Rate: 68.33%,            Test_Total_Loss: 3.7210590839385986, Test_Accuracy_Rate: 33.76%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886, Total Loss: 0.8112862706184387, Accuracy Rate: 68.56%,            Test_Total_Loss: 3.6944046020507812, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 887, Total Loss: 0.8072459697723389, Accuracy Rate: 68.19%,            Test_Total_Loss: 3.692105531692505, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 888, Total Loss: 0.8145745992660522, Accuracy Rate: 69.17%,            Test_Total_Loss: 3.6896724700927734, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 889, Total Loss: 0.8114233613014221, Accuracy Rate: 68.98%,            Test_Total_Loss: 3.7641313076019287, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 890, Total Loss: 0.8268335461616516, Accuracy Rate: 68.28%,            Test_Total_Loss: 3.6998507976531982, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 891, Total Loss: 0.8146145343780518, Accuracy Rate: 68.75%,            Test_Total_Loss: 3.605520009994507, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 892, Total Loss: 0.824954628944397, Accuracy Rate: 68.75%,            Test_Total_Loss: 3.7399401664733887, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 893, Total Loss: 0.8560717105865479, Accuracy Rate: 67.29%,            Test_Total_Loss: 3.8214125633239746, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 894, Total Loss: 0.8878934979438782, Accuracy Rate: 66.96%,            Test_Total_Loss: 3.620502233505249, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 895, Total Loss: 0.939058244228363, Accuracy Rate: 65.88%,            Test_Total_Loss: 3.7399299144744873, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 896, Total Loss: 0.891523540019989, Accuracy Rate: 67.06%,            Test_Total_Loss: 3.667706251144409, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 897, Total Loss: 0.8701145052909851, Accuracy Rate: 67.48%,            Test_Total_Loss: 3.677849054336548, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 898, Total Loss: 0.8599379658699036, Accuracy Rate: 67.58%,            Test_Total_Loss: 3.684846878051758, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 899, Total Loss: 0.8624731302261353, Accuracy Rate: 67.48%,            Test_Total_Loss: 3.635702610015869, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 900, Total Loss: 0.8514677882194519, Accuracy Rate: 67.34%,            Test_Total_Loss: 3.648137331008911, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 901, Total Loss: 0.8397582769393921, Accuracy Rate: 67.34%,            Test_Total_Loss: 3.736417531967163, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 902, Total Loss: 0.9131773710250854, Accuracy Rate: 67.01%,            Test_Total_Loss: 3.6865079402923584, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 903, Total Loss: 0.9065961241722107, Accuracy Rate: 66.87%,            Test_Total_Loss: 3.7165303230285645, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 904, Total Loss: 0.887044370174408, Accuracy Rate: 67.25%,            Test_Total_Loss: 3.6634087562561035, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 905, Total Loss: 0.8574888706207275, Accuracy Rate: 68.84%,            Test_Total_Loss: 3.5334084033966064, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 906, Total Loss: 0.8892461657524109, Accuracy Rate: 68.09%,            Test_Total_Loss: 3.605496883392334, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 907, Total Loss: 1.3127446174621582, Accuracy Rate: 58.27%,            Test_Total_Loss: 3.6307930946350098, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 908, Total Loss: 1.106820821762085, Accuracy Rate: 62.55%,            Test_Total_Loss: 3.5791304111480713, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 909, Total Loss: 1.043979287147522, Accuracy Rate: 64.43%,            Test_Total_Loss: 3.5154671669006348, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 910, Total Loss: 0.9861006140708923, Accuracy Rate: 65.37%,            Test_Total_Loss: 3.4828221797943115, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 911, Total Loss: 0.9528555274009705, Accuracy Rate: 66.40%,            Test_Total_Loss: 3.494983196258545, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 912, Total Loss: 0.9017379283905029, Accuracy Rate: 66.59%,            Test_Total_Loss: 3.473839044570923, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 913, Total Loss: 0.8890207409858704, Accuracy Rate: 68.14%,            Test_Total_Loss: 3.4237959384918213, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 914, Total Loss: 0.8887186646461487, Accuracy Rate: 67.58%,            Test_Total_Loss: 3.4280850887298584, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 915, Total Loss: 0.8727499842643738, Accuracy Rate: 67.76%,            Test_Total_Loss: 3.526660680770874, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 916, Total Loss: 0.8508689999580383, Accuracy Rate: 68.09%,            Test_Total_Loss: 3.548859119415283, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 917, Total Loss: 0.8449646234512329, Accuracy Rate: 67.95%,            Test_Total_Loss: 3.6399128437042236, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 918, Total Loss: 0.8277794718742371, Accuracy Rate: 69.08%,            Test_Total_Loss: 3.5858535766601562, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 919, Total Loss: 0.8246525526046753, Accuracy Rate: 68.75%,            Test_Total_Loss: 3.652730703353882, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 920, Total Loss: 0.8176301121711731, Accuracy Rate: 68.52%,            Test_Total_Loss: 3.600473403930664, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 921, Total Loss: 0.8050297498703003, Accuracy Rate: 69.83%,            Test_Total_Loss: 3.634761095046997, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 922, Total Loss: 0.8119853734970093, Accuracy Rate: 68.70%,            Test_Total_Loss: 3.584019660949707, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 923, Total Loss: 0.8078802227973938, Accuracy Rate: 68.75%,            Test_Total_Loss: 3.684431552886963, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 924, Total Loss: 0.8052946925163269, Accuracy Rate: 69.08%,            Test_Total_Loss: 3.6456093788146973, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 925, Total Loss: 0.8048924207687378, Accuracy Rate: 69.69%,            Test_Total_Loss: 3.7658469676971436, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 926, Total Loss: 0.8053233623504639, Accuracy Rate: 68.56%,            Test_Total_Loss: 3.729065418243408, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 927, Total Loss: 0.8117127418518066, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.7355165481567383, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 928, Total Loss: 0.8075743317604065, Accuracy Rate: 69.27%,            Test_Total_Loss: 3.7062902450561523, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 929, Total Loss: 0.8232582807540894, Accuracy Rate: 68.66%,            Test_Total_Loss: 3.725510597229004, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 930, Total Loss: 0.8210309147834778, Accuracy Rate: 68.98%,            Test_Total_Loss: 3.739108085632324, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 931, Total Loss: 0.8214414119720459, Accuracy Rate: 68.28%,            Test_Total_Loss: 3.7433383464813232, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 932, Total Loss: 0.8150933384895325, Accuracy Rate: 68.56%,            Test_Total_Loss: 3.7487826347351074, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 933, Total Loss: 0.8123404383659363, Accuracy Rate: 68.23%,            Test_Total_Loss: 3.7554407119750977, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 934, Total Loss: 0.8492994904518127, Accuracy Rate: 68.19%,            Test_Total_Loss: 3.652761936187744, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 935, Total Loss: 0.8356836438179016, Accuracy Rate: 67.76%,            Test_Total_Loss: 3.6813454627990723, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 936, Total Loss: 0.8408277034759521, Accuracy Rate: 68.52%,            Test_Total_Loss: 3.6905529499053955, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 937, Total Loss: 0.848667323589325, Accuracy Rate: 68.75%,            Test_Total_Loss: 3.676356792449951, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 938, Total Loss: 0.8743178844451904, Accuracy Rate: 67.90%,            Test_Total_Loss: 3.5916192531585693, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 939, Total Loss: 0.9029760360717773, Accuracy Rate: 66.96%,            Test_Total_Loss: 3.5878379344940186, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 940, Total Loss: 0.8802129030227661, Accuracy Rate: 67.20%,            Test_Total_Loss: 3.6047940254211426, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 941, Total Loss: 0.9030738472938538, Accuracy Rate: 66.68%,            Test_Total_Loss: 3.5724499225616455, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 942, Total Loss: 0.9171918034553528, Accuracy Rate: 66.02%,            Test_Total_Loss: 3.6002330780029297, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 943, Total Loss: 0.8801628947257996, Accuracy Rate: 67.29%,            Test_Total_Loss: 3.670869827270508, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 944, Total Loss: 1.1118546724319458, Accuracy Rate: 62.69%,            Test_Total_Loss: 3.609058380126953, Test_Accuracy_Rate: 34.18%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 945, Total Loss: 0.9848794341087341, Accuracy Rate: 64.71%,            Test_Total_Loss: 3.5160610675811768, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 946, Total Loss: 0.9611571431159973, Accuracy Rate: 64.90%,            Test_Total_Loss: 3.5797903537750244, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 947, Total Loss: 0.9085771441459656, Accuracy Rate: 66.68%,            Test_Total_Loss: 3.6989946365356445, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 948, Total Loss: 0.8785964846611023, Accuracy Rate: 66.92%,            Test_Total_Loss: 3.670067310333252, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 949, Total Loss: 0.8972339034080505, Accuracy Rate: 67.39%,            Test_Total_Loss: 3.680784225463867, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 950, Total Loss: 0.8362017273902893, Accuracy Rate: 67.86%,            Test_Total_Loss: 3.6525933742523193, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 951, Total Loss: 0.8275428414344788, Accuracy Rate: 68.80%,            Test_Total_Loss: 3.6533758640289307, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 952, Total Loss: 0.8238808512687683, Accuracy Rate: 68.33%,            Test_Total_Loss: 3.6806461811065674, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 953, Total Loss: 0.8136405944824219, Accuracy Rate: 68.84%,            Test_Total_Loss: 3.6763768196105957, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 954, Total Loss: 0.8272940516471863, Accuracy Rate: 68.33%,            Test_Total_Loss: 3.689119338989258, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 955, Total Loss: 0.8511812686920166, Accuracy Rate: 68.00%,            Test_Total_Loss: 3.6573429107666016, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 956, Total Loss: 0.8370042443275452, Accuracy Rate: 68.14%,            Test_Total_Loss: 3.7320337295532227, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 957, Total Loss: 0.8368086814880371, Accuracy Rate: 68.80%,            Test_Total_Loss: 3.6634461879730225, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 958, Total Loss: 0.8220482468605042, Accuracy Rate: 68.56%,            Test_Total_Loss: 3.7594292163848877, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 959, Total Loss: 0.8174113035202026, Accuracy Rate: 68.23%,            Test_Total_Loss: 3.790672540664673, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 960, Total Loss: 0.8146365880966187, Accuracy Rate: 69.27%,            Test_Total_Loss: 3.7005410194396973, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 961, Total Loss: 0.8054819703102112, Accuracy Rate: 69.08%,            Test_Total_Loss: 3.808506965637207, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 962, Total Loss: 0.8059290647506714, Accuracy Rate: 68.89%,            Test_Total_Loss: 3.6905550956726074, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 963, Total Loss: 0.7934826016426086, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.704699993133545, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 964, Total Loss: 0.7882627248764038, Accuracy Rate: 69.22%,            Test_Total_Loss: 3.7055647373199463, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 965, Total Loss: 0.7894142270088196, Accuracy Rate: 68.98%,            Test_Total_Loss: 3.7087414264678955, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 966, Total Loss: 0.781264066696167, Accuracy Rate: 69.03%,            Test_Total_Loss: 3.650944709777832, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 967, Total Loss: 0.7748305201530457, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.6928465366363525, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 968, Total Loss: 0.7783756256103516, Accuracy Rate: 69.13%,            Test_Total_Loss: 3.740367889404297, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 969, Total Loss: 0.7831007838249207, Accuracy Rate: 69.50%,            Test_Total_Loss: 3.7144932746887207, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 970, Total Loss: 0.7838707566261292, Accuracy Rate: 69.50%,            Test_Total_Loss: 3.701484441757202, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 971, Total Loss: 0.7896533608436584, Accuracy Rate: 69.17%,            Test_Total_Loss: 3.6879897117614746, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 972, Total Loss: 0.8293277025222778, Accuracy Rate: 67.72%,            Test_Total_Loss: 3.6952595710754395, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 973, Total Loss: 0.8066130876541138, Accuracy Rate: 67.62%,            Test_Total_Loss: 3.781566858291626, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 974, Total Loss: 0.8343054056167603, Accuracy Rate: 69.03%,            Test_Total_Loss: 3.7693216800689697, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 975, Total Loss: 0.875572919845581, Accuracy Rate: 67.53%,            Test_Total_Loss: 3.662348985671997, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 976, Total Loss: 0.8779968619346619, Accuracy Rate: 67.62%,            Test_Total_Loss: 3.658051013946533, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 977, Total Loss: 0.9021443724632263, Accuracy Rate: 66.49%,            Test_Total_Loss: 3.687260866165161, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 978, Total Loss: 0.8859466910362244, Accuracy Rate: 66.87%,            Test_Total_Loss: 3.6334567070007324, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 979, Total Loss: 0.891780436038971, Accuracy Rate: 67.29%,            Test_Total_Loss: 3.62282133102417, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 980, Total Loss: 0.8448830842971802, Accuracy Rate: 67.48%,            Test_Total_Loss: 3.618884325027466, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 981, Total Loss: 0.8271798491477966, Accuracy Rate: 68.28%,            Test_Total_Loss: 3.6046175956726074, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 982, Total Loss: 0.8649839758872986, Accuracy Rate: 68.47%,            Test_Total_Loss: 3.690563678741455, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 983, Total Loss: 0.907645583152771, Accuracy Rate: 65.74%,            Test_Total_Loss: 3.6813220977783203, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 984, Total Loss: 0.8504399061203003, Accuracy Rate: 68.00%,            Test_Total_Loss: 3.8189730644226074, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 985, Total Loss: 0.8397861123085022, Accuracy Rate: 67.48%,            Test_Total_Loss: 3.588794231414795, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 986, Total Loss: 0.8172457218170166, Accuracy Rate: 69.03%,            Test_Total_Loss: 3.642862558364868, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 987, Total Loss: 0.7907423377037048, Accuracy Rate: 69.36%,            Test_Total_Loss: 3.630970001220703, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 988, Total Loss: 0.8024242520332336, Accuracy Rate: 68.70%,            Test_Total_Loss: 3.71628999710083, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 989, Total Loss: 0.8029540181159973, Accuracy Rate: 69.27%,            Test_Total_Loss: 3.6339755058288574, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 990, Total Loss: 0.7857431769371033, Accuracy Rate: 69.27%,            Test_Total_Loss: 3.633478879928589, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 991, Total Loss: 0.7841013669967651, Accuracy Rate: 69.97%,            Test_Total_Loss: 3.6941635608673096, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 992, Total Loss: 0.7941243052482605, Accuracy Rate: 69.17%,            Test_Total_Loss: 3.6538302898406982, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 993, Total Loss: 0.8062726259231567, Accuracy Rate: 69.31%,            Test_Total_Loss: 3.6415352821350098, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 994, Total Loss: 0.784347653388977, Accuracy Rate: 69.55%,            Test_Total_Loss: 3.6748547554016113, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 995, Total Loss: 0.7974271774291992, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.7928788661956787, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 996, Total Loss: 0.8899454474449158, Accuracy Rate: 67.20%,            Test_Total_Loss: 3.7500805854797363, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 997, Total Loss: 0.9318973422050476, Accuracy Rate: 66.73%,            Test_Total_Loss: 3.7183475494384766, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 998, Total Loss: 0.8705898523330688, Accuracy Rate: 68.52%,            Test_Total_Loss: 3.7666385173797607, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 999, Total Loss: 0.848950207233429, Accuracy Rate: 67.53%,            Test_Total_Loss: 3.8475489616394043, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1000, Total Loss: 0.8343818187713623, Accuracy Rate: 67.34%,            Test_Total_Loss: 3.74487566947937, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1001, Total Loss: 0.8265801668167114, Accuracy Rate: 68.33%,            Test_Total_Loss: 3.725984811782837, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1002, Total Loss: 0.8112235069274902, Accuracy Rate: 69.22%,            Test_Total_Loss: 3.686530828475952, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1003, Total Loss: 0.8059319257736206, Accuracy Rate: 69.03%,            Test_Total_Loss: 3.6910946369171143, Test_Accuracy_Rate: 32.91%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1004, Total Loss: 0.8006594181060791, Accuracy Rate: 68.89%,            Test_Total_Loss: 3.7172763347625732, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1005, Total Loss: 0.8031466007232666, Accuracy Rate: 68.56%,            Test_Total_Loss: 3.750380277633667, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1006, Total Loss: 0.8021087050437927, Accuracy Rate: 68.47%,            Test_Total_Loss: 3.774369955062866, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1007, Total Loss: 0.8294444680213928, Accuracy Rate: 68.09%,            Test_Total_Loss: 3.7891769409179688, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1008, Total Loss: 0.8299135565757751, Accuracy Rate: 68.42%,            Test_Total_Loss: 3.779980182647705, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1009, Total Loss: 0.8415237665176392, Accuracy Rate: 68.47%,            Test_Total_Loss: 3.8024420738220215, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1010, Total Loss: 0.8373105525970459, Accuracy Rate: 67.67%,            Test_Total_Loss: 3.668262243270874, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1011, Total Loss: 0.8499099016189575, Accuracy Rate: 67.58%,            Test_Total_Loss: 3.67539119720459, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1012, Total Loss: 0.925269365310669, Accuracy Rate: 66.59%,            Test_Total_Loss: 3.706198215484619, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1013, Total Loss: 0.8602110147476196, Accuracy Rate: 67.62%,            Test_Total_Loss: 3.801165819168091, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1014, Total Loss: 0.8493339419364929, Accuracy Rate: 68.28%,            Test_Total_Loss: 3.7449052333831787, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1015, Total Loss: 0.8252356052398682, Accuracy Rate: 68.52%,            Test_Total_Loss: 3.761746883392334, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1016, Total Loss: 0.8183125853538513, Accuracy Rate: 68.56%,            Test_Total_Loss: 3.7344212532043457, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1017, Total Loss: 0.805395245552063, Accuracy Rate: 69.17%,            Test_Total_Loss: 3.7723591327667236, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1018, Total Loss: 0.8005517721176147, Accuracy Rate: 68.66%,            Test_Total_Loss: 3.7307960987091064, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1019, Total Loss: 0.8074654340744019, Accuracy Rate: 68.70%,            Test_Total_Loss: 3.7329020500183105, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1020, Total Loss: 0.8259022831916809, Accuracy Rate: 68.84%,            Test_Total_Loss: 3.792834758758545, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1021, Total Loss: 0.8111638426780701, Accuracy Rate: 69.08%,            Test_Total_Loss: 3.745476245880127, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1022, Total Loss: 0.8232825398445129, Accuracy Rate: 68.84%,            Test_Total_Loss: 3.803784132003784, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1023, Total Loss: 0.818352997303009, Accuracy Rate: 68.56%,            Test_Total_Loss: 3.7392330169677734, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1024, Total Loss: 0.8135872483253479, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.7575669288635254, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1025, Total Loss: 0.7946878671646118, Accuracy Rate: 69.41%,            Test_Total_Loss: 3.7804598808288574, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1026, Total Loss: 0.7901591062545776, Accuracy Rate: 68.84%,            Test_Total_Loss: 3.8277366161346436, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1027, Total Loss: 0.7857017517089844, Accuracy Rate: 69.03%,            Test_Total_Loss: 3.7655582427978516, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1028, Total Loss: 0.7774652242660522, Accuracy Rate: 69.50%,            Test_Total_Loss: 3.8033385276794434, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1029, Total Loss: 0.7850126624107361, Accuracy Rate: 69.17%,            Test_Total_Loss: 3.8063409328460693, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1030, Total Loss: 0.7980077266693115, Accuracy Rate: 70.16%,            Test_Total_Loss: 3.681565761566162, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1031, Total Loss: 0.817242443561554, Accuracy Rate: 68.14%,            Test_Total_Loss: 3.7625479698181152, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1032, Total Loss: 1.0300108194351196, Accuracy Rate: 64.33%,            Test_Total_Loss: 3.7350778579711914, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1033, Total Loss: 1.0083928108215332, Accuracy Rate: 64.61%,            Test_Total_Loss: 3.782259225845337, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1034, Total Loss: 0.9295000433921814, Accuracy Rate: 65.70%,            Test_Total_Loss: 3.6011781692504883, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1035, Total Loss: 0.8637747168540955, Accuracy Rate: 68.19%,            Test_Total_Loss: 3.79620623588562, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 1036, Total Loss: 0.8831759095191956, Accuracy Rate: 67.67%,            Test_Total_Loss: 3.7489824295043945, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1037, Total Loss: 0.8946743607521057, Accuracy Rate: 67.76%,            Test_Total_Loss: 3.636178731918335, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1038, Total Loss: 0.9023373126983643, Accuracy Rate: 66.87%,            Test_Total_Loss: 3.603980541229248, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1039, Total Loss: 0.8475265502929688, Accuracy Rate: 67.90%,            Test_Total_Loss: 3.549989700317383, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1040, Total Loss: 0.8234196305274963, Accuracy Rate: 68.47%,            Test_Total_Loss: 3.551572322845459, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1041, Total Loss: 0.807087779045105, Accuracy Rate: 69.13%,            Test_Total_Loss: 3.5701980590820312, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1042, Total Loss: 0.7969351410865784, Accuracy Rate: 69.50%,            Test_Total_Loss: 3.5991549491882324, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1043, Total Loss: 0.7943389415740967, Accuracy Rate: 68.98%,            Test_Total_Loss: 3.59784197807312, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1044, Total Loss: 0.7787243127822876, Accuracy Rate: 68.98%,            Test_Total_Loss: 3.6994235515594482, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1045, Total Loss: 0.7896378040313721, Accuracy Rate: 69.78%,            Test_Total_Loss: 3.6726832389831543, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1046, Total Loss: 0.7875463366508484, Accuracy Rate: 69.41%,            Test_Total_Loss: 3.674830198287964, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1047, Total Loss: 0.7766789197921753, Accuracy Rate: 70.25%,            Test_Total_Loss: 3.664644956588745, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1048, Total Loss: 0.7861171960830688, Accuracy Rate: 69.88%,            Test_Total_Loss: 3.7891969680786133, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1049, Total Loss: 0.8032324910163879, Accuracy Rate: 68.19%,            Test_Total_Loss: 3.7457234859466553, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1050, Total Loss: 0.8079277276992798, Accuracy Rate: 69.03%,            Test_Total_Loss: 3.745868444442749, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1051, Total Loss: 0.797074556350708, Accuracy Rate: 69.17%,            Test_Total_Loss: 3.780545473098755, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1052, Total Loss: 0.7781651616096497, Accuracy Rate: 68.94%,            Test_Total_Loss: 3.752567768096924, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1053, Total Loss: 0.7667328119277954, Accuracy Rate: 69.83%,            Test_Total_Loss: 3.844383478164673, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1054, Total Loss: 0.7692311406135559, Accuracy Rate: 69.22%,            Test_Total_Loss: 3.7835192680358887, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1055, Total Loss: 0.7664725184440613, Accuracy Rate: 69.22%,            Test_Total_Loss: 3.8580844402313232, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1056, Total Loss: 0.7645063996315002, Accuracy Rate: 69.83%,            Test_Total_Loss: 3.7932980060577393, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1057, Total Loss: 0.765483021736145, Accuracy Rate: 69.60%,            Test_Total_Loss: 3.827361822128296, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1058, Total Loss: 0.7620920538902283, Accuracy Rate: 70.11%,            Test_Total_Loss: 3.802940845489502, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1059, Total Loss: 0.7685592770576477, Accuracy Rate: 69.55%,            Test_Total_Loss: 3.870697021484375, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1060, Total Loss: 0.7725354433059692, Accuracy Rate: 68.89%,            Test_Total_Loss: 3.848162889480591, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1061, Total Loss: 0.759061336517334, Accuracy Rate: 68.89%,            Test_Total_Loss: 3.9424023628234863, Test_Accuracy_Rate: 31.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1062, Total Loss: 0.7597872018814087, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.87094783782959, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1063, Total Loss: 0.7857764363288879, Accuracy Rate: 69.08%,            Test_Total_Loss: 3.92756724357605, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1064, Total Loss: 0.819530189037323, Accuracy Rate: 68.33%,            Test_Total_Loss: 3.804269552230835, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1065, Total Loss: 0.8246509432792664, Accuracy Rate: 68.37%,            Test_Total_Loss: 3.7971315383911133, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1066, Total Loss: 0.8243758082389832, Accuracy Rate: 68.33%,            Test_Total_Loss: 3.92637038230896, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1067, Total Loss: 0.9369551539421082, Accuracy Rate: 65.93%,            Test_Total_Loss: 3.7488088607788086, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1068, Total Loss: 0.9529916644096375, Accuracy Rate: 64.80%,            Test_Total_Loss: 3.723431348800659, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1069, Total Loss: 0.8927903175354004, Accuracy Rate: 67.34%,            Test_Total_Loss: 3.6208789348602295, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1070, Total Loss: 0.9057570099830627, Accuracy Rate: 67.11%,            Test_Total_Loss: 3.726158857345581, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1071, Total Loss: 0.920214056968689, Accuracy Rate: 66.54%,            Test_Total_Loss: 3.6868743896484375, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1072, Total Loss: 0.8684337735176086, Accuracy Rate: 68.00%,            Test_Total_Loss: 3.6649763584136963, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1073, Total Loss: 0.8227860331535339, Accuracy Rate: 68.00%,            Test_Total_Loss: 3.736999034881592, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1074, Total Loss: 0.7997841238975525, Accuracy Rate: 69.03%,            Test_Total_Loss: 3.7032346725463867, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1075, Total Loss: 0.8029319643974304, Accuracy Rate: 69.22%,            Test_Total_Loss: 3.708932399749756, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1076, Total Loss: 0.7984963059425354, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.790923833847046, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1077, Total Loss: 0.7966533303260803, Accuracy Rate: 68.94%,            Test_Total_Loss: 3.774754524230957, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1078, Total Loss: 0.7798730731010437, Accuracy Rate: 69.36%,            Test_Total_Loss: 3.7486677169799805, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1079, Total Loss: 0.800609827041626, Accuracy Rate: 69.31%,            Test_Total_Loss: 3.7703874111175537, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1080, Total Loss: 0.79460608959198, Accuracy Rate: 68.84%,            Test_Total_Loss: 3.794379472732544, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1081, Total Loss: 0.7864160537719727, Accuracy Rate: 69.08%,            Test_Total_Loss: 3.8148434162139893, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1082, Total Loss: 0.7689995169639587, Accuracy Rate: 70.25%,            Test_Total_Loss: 3.780517816543579, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1083, Total Loss: 0.7680182456970215, Accuracy Rate: 69.78%,            Test_Total_Loss: 3.814380407333374, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1084, Total Loss: 0.7629120349884033, Accuracy Rate: 69.92%,            Test_Total_Loss: 3.803678035736084, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1085, Total Loss: 0.7594466209411621, Accuracy Rate: 69.55%,            Test_Total_Loss: 3.8433175086975098, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1086, Total Loss: 0.7524654269218445, Accuracy Rate: 69.69%,            Test_Total_Loss: 3.8250832557678223, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1087, Total Loss: 0.7473261952400208, Accuracy Rate: 70.49%,            Test_Total_Loss: 3.896942138671875, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1088, Total Loss: 0.7511819005012512, Accuracy Rate: 69.92%,            Test_Total_Loss: 3.839207649230957, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1089, Total Loss: 0.7496772408485413, Accuracy Rate: 69.83%,            Test_Total_Loss: 3.837151288986206, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1090, Total Loss: 0.7545203566551208, Accuracy Rate: 69.88%,            Test_Total_Loss: 3.8402099609375, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1091, Total Loss: 0.7549706697463989, Accuracy Rate: 70.54%,            Test_Total_Loss: 3.897331953048706, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1092, Total Loss: 0.7583189606666565, Accuracy Rate: 69.55%,            Test_Total_Loss: 3.818955421447754, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1093, Total Loss: 0.7753824591636658, Accuracy Rate: 68.23%,            Test_Total_Loss: 3.863301992416382, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1094, Total Loss: 1.0188841819763184, Accuracy Rate: 64.57%,            Test_Total_Loss: 3.9728713035583496, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1095, Total Loss: 1.4007750749588013, Accuracy Rate: 56.16%,            Test_Total_Loss: 3.7411301136016846, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1096, Total Loss: 1.2067533731460571, Accuracy Rate: 59.92%,            Test_Total_Loss: 3.5321896076202393, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1097, Total Loss: 1.0976685285568237, Accuracy Rate: 61.84%,            Test_Total_Loss: 3.622361898422241, Test_Accuracy_Rate: 29.96%\n",
      "Epoch 1098, Total Loss: 1.098658561706543, Accuracy Rate: 62.36%,            Test_Total_Loss: 3.628391742706299, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1099, Total Loss: 1.0549616813659668, Accuracy Rate: 62.92%,            Test_Total_Loss: 3.5830864906311035, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1100, Total Loss: 0.9364198446273804, Accuracy Rate: 65.84%,            Test_Total_Loss: 3.5240261554718018, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1101, Total Loss: 0.883203387260437, Accuracy Rate: 67.15%,            Test_Total_Loss: 3.582266330718994, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1102, Total Loss: 0.853351891040802, Accuracy Rate: 68.19%,            Test_Total_Loss: 3.601870536804199, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1103, Total Loss: 0.8443116545677185, Accuracy Rate: 68.28%,            Test_Total_Loss: 3.6173248291015625, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1104, Total Loss: 0.8249813914299011, Accuracy Rate: 68.23%,            Test_Total_Loss: 3.697899580001831, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1105, Total Loss: 0.8093910813331604, Accuracy Rate: 68.47%,            Test_Total_Loss: 3.7649030685424805, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1106, Total Loss: 0.8081729412078857, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.729787826538086, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1107, Total Loss: 0.7975274920463562, Accuracy Rate: 68.84%,            Test_Total_Loss: 3.8214168548583984, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1108, Total Loss: 0.7842668890953064, Accuracy Rate: 68.75%,            Test_Total_Loss: 3.805814504623413, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1109, Total Loss: 0.8065096735954285, Accuracy Rate: 68.75%,            Test_Total_Loss: 3.7712061405181885, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1110, Total Loss: 0.8021714091300964, Accuracy Rate: 69.31%,            Test_Total_Loss: 3.8502328395843506, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1111, Total Loss: 0.8222646117210388, Accuracy Rate: 68.70%,            Test_Total_Loss: 3.839747428894043, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1112, Total Loss: 0.8124606609344482, Accuracy Rate: 69.08%,            Test_Total_Loss: 3.8065693378448486, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1113, Total Loss: 0.8020046949386597, Accuracy Rate: 69.03%,            Test_Total_Loss: 3.84356689453125, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1114, Total Loss: 0.7832050919532776, Accuracy Rate: 69.22%,            Test_Total_Loss: 3.8267931938171387, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1115, Total Loss: 0.7863363027572632, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.8330864906311035, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1116, Total Loss: 0.7858604788780212, Accuracy Rate: 69.22%,            Test_Total_Loss: 3.7002828121185303, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1117, Total Loss: 0.8322558999061584, Accuracy Rate: 67.58%,            Test_Total_Loss: 3.778029441833496, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1118, Total Loss: 0.838527262210846, Accuracy Rate: 68.42%,            Test_Total_Loss: 3.8762223720550537, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1119, Total Loss: 0.8447322845458984, Accuracy Rate: 67.34%,            Test_Total_Loss: 3.91390323638916, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1120, Total Loss: 0.7944158911705017, Accuracy Rate: 68.70%,            Test_Total_Loss: 3.8888638019561768, Test_Accuracy_Rate: 33.76%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1121, Total Loss: 0.8016143441200256, Accuracy Rate: 68.28%,            Test_Total_Loss: 3.938242197036743, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1122, Total Loss: 0.777615487575531, Accuracy Rate: 69.36%,            Test_Total_Loss: 3.9384021759033203, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1123, Total Loss: 0.7826899290084839, Accuracy Rate: 69.60%,            Test_Total_Loss: 3.9275028705596924, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1124, Total Loss: 0.7735030651092529, Accuracy Rate: 69.36%,            Test_Total_Loss: 3.9199726581573486, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1125, Total Loss: 0.7737315893173218, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.9436097145080566, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1126, Total Loss: 0.767228901386261, Accuracy Rate: 69.08%,            Test_Total_Loss: 3.919022798538208, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1127, Total Loss: 0.7684785723686218, Accuracy Rate: 69.13%,            Test_Total_Loss: 3.959352493286133, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1128, Total Loss: 0.7615859508514404, Accuracy Rate: 69.74%,            Test_Total_Loss: 3.931774139404297, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1129, Total Loss: 0.7636964321136475, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.9394478797912598, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1130, Total Loss: 0.7726079225540161, Accuracy Rate: 69.92%,            Test_Total_Loss: 3.9867947101593018, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1131, Total Loss: 0.7684414386749268, Accuracy Rate: 69.22%,            Test_Total_Loss: 3.946957588195801, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1132, Total Loss: 0.7804474830627441, Accuracy Rate: 69.60%,            Test_Total_Loss: 3.9735605716705322, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1133, Total Loss: 0.8518288135528564, Accuracy Rate: 67.62%,            Test_Total_Loss: 3.968435525894165, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1134, Total Loss: 0.8658743500709534, Accuracy Rate: 67.53%,            Test_Total_Loss: 3.933717727661133, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1135, Total Loss: 0.8779552578926086, Accuracy Rate: 66.17%,            Test_Total_Loss: 3.9938342571258545, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1136, Total Loss: 0.8917056322097778, Accuracy Rate: 66.17%,            Test_Total_Loss: 3.8413398265838623, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1137, Total Loss: 0.9139747619628906, Accuracy Rate: 66.96%,            Test_Total_Loss: 3.7991716861724854, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1138, Total Loss: 0.8829773664474487, Accuracy Rate: 67.67%,            Test_Total_Loss: 3.8237781524658203, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1139, Total Loss: 0.8433930277824402, Accuracy Rate: 68.42%,            Test_Total_Loss: 3.7782466411590576, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1140, Total Loss: 0.8005110025405884, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.9236788749694824, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1141, Total Loss: 0.8070635199546814, Accuracy Rate: 69.13%,            Test_Total_Loss: 3.9941534996032715, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1142, Total Loss: 0.8688151240348816, Accuracy Rate: 67.86%,            Test_Total_Loss: 3.787346839904785, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1143, Total Loss: 0.842513918876648, Accuracy Rate: 68.66%,            Test_Total_Loss: 3.7559335231781006, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1144, Total Loss: 0.8082472681999207, Accuracy Rate: 68.89%,            Test_Total_Loss: 3.860867738723755, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1145, Total Loss: 0.78824383020401, Accuracy Rate: 69.27%,            Test_Total_Loss: 3.8195114135742188, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1146, Total Loss: 0.7870388031005859, Accuracy Rate: 69.13%,            Test_Total_Loss: 3.894031047821045, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1147, Total Loss: 0.7749236822128296, Accuracy Rate: 69.64%,            Test_Total_Loss: 3.9190750122070312, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1148, Total Loss: 0.7588262557983398, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.8961329460144043, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1149, Total Loss: 0.7567084431648254, Accuracy Rate: 69.97%,            Test_Total_Loss: 3.907560348510742, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1150, Total Loss: 0.7578489780426025, Accuracy Rate: 69.64%,            Test_Total_Loss: 3.9824466705322266, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1151, Total Loss: 0.7480719089508057, Accuracy Rate: 70.91%,            Test_Total_Loss: 3.9118759632110596, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1152, Total Loss: 0.7581896781921387, Accuracy Rate: 69.50%,            Test_Total_Loss: 4.0301103591918945, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1153, Total Loss: 0.7546823620796204, Accuracy Rate: 70.11%,            Test_Total_Loss: 3.95266056060791, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1154, Total Loss: 0.7624112367630005, Accuracy Rate: 70.02%,            Test_Total_Loss: 3.7815544605255127, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1155, Total Loss: 0.809965193271637, Accuracy Rate: 69.36%,            Test_Total_Loss: 3.8508753776550293, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1156, Total Loss: 0.8161709308624268, Accuracy Rate: 68.23%,            Test_Total_Loss: 3.8696036338806152, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1157, Total Loss: 0.8852072358131409, Accuracy Rate: 67.34%,            Test_Total_Loss: 3.7219552993774414, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1158, Total Loss: 1.0813772678375244, Accuracy Rate: 61.89%,            Test_Total_Loss: 3.9899306297302246, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1159, Total Loss: 0.9674234986305237, Accuracy Rate: 65.65%,            Test_Total_Loss: 3.9272334575653076, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1160, Total Loss: 0.9021276235580444, Accuracy Rate: 67.53%,            Test_Total_Loss: 3.902214527130127, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1161, Total Loss: 0.8302782773971558, Accuracy Rate: 68.52%,            Test_Total_Loss: 3.846170663833618, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1162, Total Loss: 0.8165704011917114, Accuracy Rate: 69.36%,            Test_Total_Loss: 3.9477438926696777, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1163, Total Loss: 0.8055686354637146, Accuracy Rate: 69.97%,            Test_Total_Loss: 3.9749135971069336, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1164, Total Loss: 0.7762845754623413, Accuracy Rate: 69.92%,            Test_Total_Loss: 3.974245309829712, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1165, Total Loss: 0.7688038349151611, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.0572991371154785, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1166, Total Loss: 0.7657076120376587, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.039312362670898, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1167, Total Loss: 0.7863320708274841, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.1323442459106445, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1168, Total Loss: 0.979551374912262, Accuracy Rate: 65.18%,            Test_Total_Loss: 4.07601261138916, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 1169, Total Loss: 1.0131438970565796, Accuracy Rate: 64.29%,            Test_Total_Loss: 4.051605224609375, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1170, Total Loss: 0.9320589303970337, Accuracy Rate: 65.55%,            Test_Total_Loss: 4.089977264404297, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1171, Total Loss: 1.0219906568527222, Accuracy Rate: 65.08%,            Test_Total_Loss: 4.311746120452881, Test_Accuracy_Rate: 28.27%\n",
      "Epoch 1172, Total Loss: 1.162103533744812, Accuracy Rate: 61.84%,            Test_Total_Loss: 3.8917980194091797, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1173, Total Loss: 1.0088547468185425, Accuracy Rate: 65.88%,            Test_Total_Loss: 3.6629650592803955, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1174, Total Loss: 0.9962469339370728, Accuracy Rate: 64.71%,            Test_Total_Loss: 3.7692270278930664, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1175, Total Loss: 0.9332348704338074, Accuracy Rate: 66.31%,            Test_Total_Loss: 3.71295166015625, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1176, Total Loss: 0.9343802332878113, Accuracy Rate: 66.54%,            Test_Total_Loss: 3.863687038421631, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1177, Total Loss: 0.9243398904800415, Accuracy Rate: 67.11%,            Test_Total_Loss: 3.766690492630005, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1178, Total Loss: 0.8606855869293213, Accuracy Rate: 67.72%,            Test_Total_Loss: 3.7625083923339844, Test_Accuracy_Rate: 33.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1179, Total Loss: 0.817842423915863, Accuracy Rate: 69.08%,            Test_Total_Loss: 3.836672067642212, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1180, Total Loss: 0.7914656400680542, Accuracy Rate: 70.35%,            Test_Total_Loss: 3.881169557571411, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 1181, Total Loss: 0.7827299237251282, Accuracy Rate: 70.21%,            Test_Total_Loss: 3.8814198970794678, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1182, Total Loss: 0.7692199349403381, Accuracy Rate: 69.92%,            Test_Total_Loss: 3.9240856170654297, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1183, Total Loss: 0.7657368779182434, Accuracy Rate: 70.54%,            Test_Total_Loss: 3.9231581687927246, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1184, Total Loss: 0.7677276730537415, Accuracy Rate: 69.69%,            Test_Total_Loss: 3.9437406063079834, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1185, Total Loss: 0.7574145197868347, Accuracy Rate: 70.49%,            Test_Total_Loss: 3.937265396118164, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1186, Total Loss: 0.7591228485107422, Accuracy Rate: 69.88%,            Test_Total_Loss: 3.9345204830169678, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1187, Total Loss: 0.7655693292617798, Accuracy Rate: 69.64%,            Test_Total_Loss: 3.857869863510132, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1188, Total Loss: 0.7686518430709839, Accuracy Rate: 70.35%,            Test_Total_Loss: 3.859604835510254, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1189, Total Loss: 0.7563349008560181, Accuracy Rate: 70.02%,            Test_Total_Loss: 3.9248454570770264, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1190, Total Loss: 0.7483564019203186, Accuracy Rate: 70.30%,            Test_Total_Loss: 3.961289644241333, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1191, Total Loss: 0.7445893883705139, Accuracy Rate: 70.49%,            Test_Total_Loss: 3.958420515060425, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1192, Total Loss: 0.7502017021179199, Accuracy Rate: 70.11%,            Test_Total_Loss: 3.926511287689209, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1193, Total Loss: 0.7449843287467957, Accuracy Rate: 69.97%,            Test_Total_Loss: 3.989480495452881, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1194, Total Loss: 0.7456509470939636, Accuracy Rate: 70.02%,            Test_Total_Loss: 3.9901175498962402, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1195, Total Loss: 0.7658405303955078, Accuracy Rate: 69.78%,            Test_Total_Loss: 3.9746205806732178, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1196, Total Loss: 0.7534992098808289, Accuracy Rate: 70.82%,            Test_Total_Loss: 3.990116596221924, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1197, Total Loss: 0.7509217858314514, Accuracy Rate: 70.02%,            Test_Total_Loss: 3.9672906398773193, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1198, Total Loss: 0.7734205722808838, Accuracy Rate: 69.31%,            Test_Total_Loss: 4.025484085083008, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1199, Total Loss: 0.7673135995864868, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.087331771850586, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 1200, Total Loss: 0.8028426170349121, Accuracy Rate: 68.89%,            Test_Total_Loss: 3.9703733921051025, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1201, Total Loss: 0.9100615382194519, Accuracy Rate: 66.17%,            Test_Total_Loss: 3.9310245513916016, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1202, Total Loss: 0.882103443145752, Accuracy Rate: 67.67%,            Test_Total_Loss: 3.9417471885681152, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1203, Total Loss: 0.8474065661430359, Accuracy Rate: 68.70%,            Test_Total_Loss: 3.984143018722534, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1204, Total Loss: 0.8191608190536499, Accuracy Rate: 68.80%,            Test_Total_Loss: 4.03643798828125, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 1205, Total Loss: 0.816511332988739, Accuracy Rate: 68.42%,            Test_Total_Loss: 3.992546796798706, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1206, Total Loss: 0.813564121723175, Accuracy Rate: 68.28%,            Test_Total_Loss: 4.043475151062012, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1207, Total Loss: 0.7939119338989258, Accuracy Rate: 69.36%,            Test_Total_Loss: 4.066739559173584, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1208, Total Loss: 0.7745001912117004, Accuracy Rate: 69.27%,            Test_Total_Loss: 4.067333221435547, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1209, Total Loss: 0.7616379261016846, Accuracy Rate: 70.21%,            Test_Total_Loss: 4.102229595184326, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1210, Total Loss: 0.7594102025032043, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.1230082511901855, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1211, Total Loss: 0.7607376575469971, Accuracy Rate: 69.78%,            Test_Total_Loss: 4.123636722564697, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1212, Total Loss: 0.7578791379928589, Accuracy Rate: 70.49%,            Test_Total_Loss: 4.141711711883545, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1213, Total Loss: 0.7526654005050659, Accuracy Rate: 70.21%,            Test_Total_Loss: 4.101378440856934, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1214, Total Loss: 0.7578113079071045, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.126001358032227, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1215, Total Loss: 0.7507185935974121, Accuracy Rate: 69.92%,            Test_Total_Loss: 4.081543922424316, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1216, Total Loss: 0.7492438554763794, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.113430976867676, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1217, Total Loss: 0.7631955146789551, Accuracy Rate: 70.35%,            Test_Total_Loss: 4.088171005249023, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1218, Total Loss: 0.7570339441299438, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.117092132568359, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1219, Total Loss: 0.7520183324813843, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.128634452819824, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1220, Total Loss: 0.8522193431854248, Accuracy Rate: 68.42%,            Test_Total_Loss: 4.195884704589844, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1221, Total Loss: 1.021593451499939, Accuracy Rate: 64.85%,            Test_Total_Loss: 3.9972786903381348, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1222, Total Loss: 1.020095944404602, Accuracy Rate: 64.99%,            Test_Total_Loss: 3.977015256881714, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1223, Total Loss: 1.0060389041900635, Accuracy Rate: 63.49%,            Test_Total_Loss: 4.010143756866455, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1224, Total Loss: 0.9245785474777222, Accuracy Rate: 66.21%,            Test_Total_Loss: 3.9611024856567383, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1225, Total Loss: 0.9159005284309387, Accuracy Rate: 67.34%,            Test_Total_Loss: 3.9389431476593018, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1226, Total Loss: 0.8436185121536255, Accuracy Rate: 67.95%,            Test_Total_Loss: 4.012601375579834, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1227, Total Loss: 0.8196366429328918, Accuracy Rate: 70.16%,            Test_Total_Loss: 4.016468048095703, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 1228, Total Loss: 0.814709484577179, Accuracy Rate: 69.64%,            Test_Total_Loss: 4.018040657043457, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1229, Total Loss: 0.7795087695121765, Accuracy Rate: 69.36%,            Test_Total_Loss: 4.059311866760254, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1230, Total Loss: 0.7768672108650208, Accuracy Rate: 70.07%,            Test_Total_Loss: 4.047962188720703, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1231, Total Loss: 0.7607631683349609, Accuracy Rate: 69.41%,            Test_Total_Loss: 4.153429985046387, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1232, Total Loss: 0.7691754102706909, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.06109619140625, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1233, Total Loss: 0.7563016414642334, Accuracy Rate: 70.35%,            Test_Total_Loss: 4.129490852355957, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1234, Total Loss: 0.7532218098640442, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.139317512512207, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1235, Total Loss: 0.7548091411590576, Accuracy Rate: 68.98%,            Test_Total_Loss: 4.089392185211182, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1236, Total Loss: 0.7499054074287415, Accuracy Rate: 70.44%,            Test_Total_Loss: 4.147266864776611, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1237, Total Loss: 0.7531347274780273, Accuracy Rate: 69.31%,            Test_Total_Loss: 4.087264060974121, Test_Accuracy_Rate: 35.02%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1238, Total Loss: 0.7612481117248535, Accuracy Rate: 70.21%,            Test_Total_Loss: 4.154808521270752, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1239, Total Loss: 0.7682851552963257, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.10312032699585, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1240, Total Loss: 0.7441673278808594, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.154237270355225, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1241, Total Loss: 0.7439948320388794, Accuracy Rate: 70.44%,            Test_Total_Loss: 4.132643699645996, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1242, Total Loss: 0.7330299019813538, Accuracy Rate: 70.16%,            Test_Total_Loss: 4.137264728546143, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1243, Total Loss: 0.7399994134902954, Accuracy Rate: 70.39%,            Test_Total_Loss: 4.164755821228027, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1244, Total Loss: 0.7409594058990479, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.172380447387695, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1245, Total Loss: 0.745679497718811, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.169919967651367, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1246, Total Loss: 0.7461740374565125, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.183998107910156, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1247, Total Loss: 0.7388255596160889, Accuracy Rate: 70.35%,            Test_Total_Loss: 4.186257839202881, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1248, Total Loss: 0.7350762486457825, Accuracy Rate: 70.49%,            Test_Total_Loss: 4.234611988067627, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1249, Total Loss: 0.7443721890449524, Accuracy Rate: 70.44%,            Test_Total_Loss: 4.173864364624023, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1250, Total Loss: 0.7595272064208984, Accuracy Rate: 69.41%,            Test_Total_Loss: 4.1242289543151855, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1251, Total Loss: 0.7642481327056885, Accuracy Rate: 69.55%,            Test_Total_Loss: 4.223238945007324, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1252, Total Loss: 0.7954903244972229, Accuracy Rate: 68.75%,            Test_Total_Loss: 4.173007965087891, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1253, Total Loss: 0.8314146399497986, Accuracy Rate: 67.81%,            Test_Total_Loss: 4.101773262023926, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1254, Total Loss: 0.9721025824546814, Accuracy Rate: 65.93%,            Test_Total_Loss: 4.048166751861572, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1255, Total Loss: 1.0422217845916748, Accuracy Rate: 62.55%,            Test_Total_Loss: 4.046151161193848, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1256, Total Loss: 0.9896551370620728, Accuracy Rate: 64.94%,            Test_Total_Loss: 4.081741809844971, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1257, Total Loss: 0.9038221836090088, Accuracy Rate: 66.64%,            Test_Total_Loss: 4.033797264099121, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1258, Total Loss: 0.8750255107879639, Accuracy Rate: 67.53%,            Test_Total_Loss: 3.9753849506378174, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1259, Total Loss: 0.8655948042869568, Accuracy Rate: 67.25%,            Test_Total_Loss: 4.018374919891357, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1260, Total Loss: 0.9261323809623718, Accuracy Rate: 66.87%,            Test_Total_Loss: 4.06777811050415, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1261, Total Loss: 0.9318348169326782, Accuracy Rate: 66.26%,            Test_Total_Loss: 3.8797738552093506, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1262, Total Loss: 0.8932397365570068, Accuracy Rate: 67.11%,            Test_Total_Loss: 3.9428300857543945, Test_Accuracy_Rate: 29.96%\n",
      "Epoch 1263, Total Loss: 0.8699203133583069, Accuracy Rate: 67.25%,            Test_Total_Loss: 3.862945318222046, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1264, Total Loss: 0.8388785719871521, Accuracy Rate: 68.80%,            Test_Total_Loss: 3.928420305252075, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1265, Total Loss: 0.825955867767334, Accuracy Rate: 68.66%,            Test_Total_Loss: 3.8816845417022705, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1266, Total Loss: 0.8109992146492004, Accuracy Rate: 68.84%,            Test_Total_Loss: 3.9313464164733887, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1267, Total Loss: 0.8311829566955566, Accuracy Rate: 68.33%,            Test_Total_Loss: 3.9046058654785156, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1268, Total Loss: 0.8256812691688538, Accuracy Rate: 68.42%,            Test_Total_Loss: 3.9105770587921143, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1269, Total Loss: 0.8317767381668091, Accuracy Rate: 67.95%,            Test_Total_Loss: 3.933603286743164, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1270, Total Loss: 0.8881961107254028, Accuracy Rate: 66.07%,            Test_Total_Loss: 3.9014837741851807, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1271, Total Loss: 0.8982285261154175, Accuracy Rate: 67.67%,            Test_Total_Loss: 3.8192214965820312, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1272, Total Loss: 0.8711578845977783, Accuracy Rate: 67.81%,            Test_Total_Loss: 3.849539279937744, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1273, Total Loss: 0.8584175109863281, Accuracy Rate: 67.81%,            Test_Total_Loss: 3.8586630821228027, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1274, Total Loss: 0.8205806612968445, Accuracy Rate: 68.75%,            Test_Total_Loss: 3.776095151901245, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1275, Total Loss: 0.8170114755630493, Accuracy Rate: 69.22%,            Test_Total_Loss: 3.8491945266723633, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1276, Total Loss: 0.8132302761077881, Accuracy Rate: 68.47%,            Test_Total_Loss: 3.842214584350586, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1277, Total Loss: 0.8052114248275757, Accuracy Rate: 68.00%,            Test_Total_Loss: 3.8476085662841797, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1278, Total Loss: 0.8035464286804199, Accuracy Rate: 69.22%,            Test_Total_Loss: 3.8249173164367676, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1279, Total Loss: 0.8063049912452698, Accuracy Rate: 68.66%,            Test_Total_Loss: 3.9367728233337402, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1280, Total Loss: 0.7995750904083252, Accuracy Rate: 68.89%,            Test_Total_Loss: 3.953202247619629, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1281, Total Loss: 0.8003105521202087, Accuracy Rate: 68.14%,            Test_Total_Loss: 3.9782235622406006, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1282, Total Loss: 0.787979245185852, Accuracy Rate: 69.88%,            Test_Total_Loss: 3.9527394771575928, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1283, Total Loss: 0.8162356019020081, Accuracy Rate: 68.23%,            Test_Total_Loss: 4.016780853271484, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1284, Total Loss: 0.8105304837226868, Accuracy Rate: 68.37%,            Test_Total_Loss: 3.9500741958618164, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1285, Total Loss: 0.81697678565979, Accuracy Rate: 68.28%,            Test_Total_Loss: 3.9244890213012695, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1286, Total Loss: 0.8000978827476501, Accuracy Rate: 68.98%,            Test_Total_Loss: 3.961026668548584, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1287, Total Loss: 0.7937074899673462, Accuracy Rate: 68.80%,            Test_Total_Loss: 4.006772041320801, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1288, Total Loss: 0.7747706770896912, Accuracy Rate: 69.60%,            Test_Total_Loss: 3.9915690422058105, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1289, Total Loss: 0.7779998779296875, Accuracy Rate: 68.47%,            Test_Total_Loss: 4.024257183074951, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1290, Total Loss: 0.7866479158401489, Accuracy Rate: 69.27%,            Test_Total_Loss: 4.062974452972412, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1291, Total Loss: 0.7858992218971252, Accuracy Rate: 69.50%,            Test_Total_Loss: 4.0176310539245605, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1292, Total Loss: 0.7789202332496643, Accuracy Rate: 68.80%,            Test_Total_Loss: 4.068309783935547, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1293, Total Loss: 0.7770479321479797, Accuracy Rate: 68.80%,            Test_Total_Loss: 4.029988765716553, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1294, Total Loss: 0.7671235203742981, Accuracy Rate: 69.64%,            Test_Total_Loss: 3.9750466346740723, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1295, Total Loss: 0.780028760433197, Accuracy Rate: 68.61%,            Test_Total_Loss: 4.0893354415893555, Test_Accuracy_Rate: 34.18%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1296, Total Loss: 0.7947555184364319, Accuracy Rate: 68.33%,            Test_Total_Loss: 4.036797523498535, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1297, Total Loss: 0.7970995903015137, Accuracy Rate: 69.22%,            Test_Total_Loss: 4.11249303817749, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1298, Total Loss: 0.8026805520057678, Accuracy Rate: 69.03%,            Test_Total_Loss: 4.001789093017578, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1299, Total Loss: 0.8133202791213989, Accuracy Rate: 68.42%,            Test_Total_Loss: 4.073972225189209, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1300, Total Loss: 0.8074967861175537, Accuracy Rate: 69.03%,            Test_Total_Loss: 3.9381346702575684, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1301, Total Loss: 0.8295133709907532, Accuracy Rate: 68.00%,            Test_Total_Loss: 4.158470153808594, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1302, Total Loss: 0.8199644684791565, Accuracy Rate: 68.80%,            Test_Total_Loss: 4.077108383178711, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1303, Total Loss: 0.8156534433364868, Accuracy Rate: 68.75%,            Test_Total_Loss: 4.108548164367676, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1304, Total Loss: 0.8121014833450317, Accuracy Rate: 68.37%,            Test_Total_Loss: 4.023504734039307, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1305, Total Loss: 0.8136362433433533, Accuracy Rate: 68.28%,            Test_Total_Loss: 4.131540298461914, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1306, Total Loss: 0.8139781951904297, Accuracy Rate: 68.09%,            Test_Total_Loss: 4.045856475830078, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1307, Total Loss: 0.8211392164230347, Accuracy Rate: 68.56%,            Test_Total_Loss: 4.1090474128723145, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1308, Total Loss: 0.8732239007949829, Accuracy Rate: 66.68%,            Test_Total_Loss: 3.993899345397949, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1309, Total Loss: 0.8588771820068359, Accuracy Rate: 67.39%,            Test_Total_Loss: 4.083144664764404, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1310, Total Loss: 0.8619056940078735, Accuracy Rate: 67.01%,            Test_Total_Loss: 4.044177532196045, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1311, Total Loss: 0.8954154253005981, Accuracy Rate: 67.20%,            Test_Total_Loss: 4.069540977478027, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1312, Total Loss: 0.9264968037605286, Accuracy Rate: 65.60%,            Test_Total_Loss: 3.911895751953125, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1313, Total Loss: 0.9161872267723083, Accuracy Rate: 67.11%,            Test_Total_Loss: 3.911273956298828, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1314, Total Loss: 0.8889874219894409, Accuracy Rate: 67.01%,            Test_Total_Loss: 3.9254839420318604, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1315, Total Loss: 0.8796351552009583, Accuracy Rate: 68.05%,            Test_Total_Loss: 3.9565742015838623, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1316, Total Loss: 0.9074493050575256, Accuracy Rate: 67.20%,            Test_Total_Loss: 4.0156097412109375, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1317, Total Loss: 0.9566936492919922, Accuracy Rate: 65.08%,            Test_Total_Loss: 3.9244158267974854, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1318, Total Loss: 0.8661592602729797, Accuracy Rate: 67.15%,            Test_Total_Loss: 4.0528717041015625, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1319, Total Loss: 0.8593122959136963, Accuracy Rate: 66.96%,            Test_Total_Loss: 4.07691764831543, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1320, Total Loss: 0.8488302826881409, Accuracy Rate: 68.23%,            Test_Total_Loss: 3.922853469848633, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1321, Total Loss: 0.8235936760902405, Accuracy Rate: 68.37%,            Test_Total_Loss: 3.9986732006073, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1322, Total Loss: 0.8184482455253601, Accuracy Rate: 68.56%,            Test_Total_Loss: 4.030669689178467, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1323, Total Loss: 0.8028509020805359, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.110354423522949, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1324, Total Loss: 0.8149709701538086, Accuracy Rate: 68.28%,            Test_Total_Loss: 4.140085220336914, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1325, Total Loss: 0.812240481376648, Accuracy Rate: 68.94%,            Test_Total_Loss: 4.129790306091309, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1326, Total Loss: 0.7995540499687195, Accuracy Rate: 68.89%,            Test_Total_Loss: 4.164251804351807, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1327, Total Loss: 0.7921644449234009, Accuracy Rate: 69.31%,            Test_Total_Loss: 4.168861389160156, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1328, Total Loss: 0.783747673034668, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.135873794555664, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1329, Total Loss: 0.7816608548164368, Accuracy Rate: 69.03%,            Test_Total_Loss: 4.210943698883057, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1330, Total Loss: 0.7777906656265259, Accuracy Rate: 69.17%,            Test_Total_Loss: 4.209329128265381, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1331, Total Loss: 0.7671095728874207, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.150362968444824, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1332, Total Loss: 0.7716922163963318, Accuracy Rate: 68.61%,            Test_Total_Loss: 4.232624053955078, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1333, Total Loss: 0.7888545393943787, Accuracy Rate: 69.03%,            Test_Total_Loss: 4.251245498657227, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1334, Total Loss: 0.7801692485809326, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.255494117736816, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1335, Total Loss: 0.7852957844734192, Accuracy Rate: 68.61%,            Test_Total_Loss: 4.270478248596191, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1336, Total Loss: 0.770588219165802, Accuracy Rate: 68.75%,            Test_Total_Loss: 4.226382255554199, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1337, Total Loss: 0.76719069480896, Accuracy Rate: 69.41%,            Test_Total_Loss: 4.220737457275391, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1338, Total Loss: 0.7622092366218567, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.26227331161499, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1339, Total Loss: 0.7511417269706726, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.2628912925720215, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1340, Total Loss: 0.7761520147323608, Accuracy Rate: 69.22%,            Test_Total_Loss: 4.126516342163086, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1341, Total Loss: 0.8184365630149841, Accuracy Rate: 68.09%,            Test_Total_Loss: 4.205078125, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1342, Total Loss: 0.799673318862915, Accuracy Rate: 68.37%,            Test_Total_Loss: 4.153769493103027, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1343, Total Loss: 0.7913697361946106, Accuracy Rate: 69.22%,            Test_Total_Loss: 4.204441547393799, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1344, Total Loss: 0.8045966029167175, Accuracy Rate: 68.66%,            Test_Total_Loss: 4.217243194580078, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 1345, Total Loss: 0.81545490026474, Accuracy Rate: 68.05%,            Test_Total_Loss: 4.2100443840026855, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1346, Total Loss: 0.909916341304779, Accuracy Rate: 66.73%,            Test_Total_Loss: 4.014219284057617, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1347, Total Loss: 0.8794868588447571, Accuracy Rate: 66.21%,            Test_Total_Loss: 4.117527484893799, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1348, Total Loss: 0.8090238571166992, Accuracy Rate: 69.17%,            Test_Total_Loss: 4.103967189788818, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1349, Total Loss: 0.7830283045768738, Accuracy Rate: 69.27%,            Test_Total_Loss: 4.1003313064575195, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1350, Total Loss: 0.7739573121070862, Accuracy Rate: 69.13%,            Test_Total_Loss: 4.085838317871094, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1351, Total Loss: 0.7752653956413269, Accuracy Rate: 69.41%,            Test_Total_Loss: 4.02640962600708, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1352, Total Loss: 0.7578505277633667, Accuracy Rate: 69.36%,            Test_Total_Loss: 4.157331466674805, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1353, Total Loss: 0.7545844316482544, Accuracy Rate: 69.78%,            Test_Total_Loss: 4.1649699211120605, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1354, Total Loss: 0.7520941495895386, Accuracy Rate: 69.92%,            Test_Total_Loss: 4.185784816741943, Test_Accuracy_Rate: 35.44%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1355, Total Loss: 0.7491710782051086, Accuracy Rate: 70.30%,            Test_Total_Loss: 4.184274673461914, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1356, Total Loss: 0.7416712641716003, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.218967914581299, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1357, Total Loss: 0.7539017796516418, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.208115100860596, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1358, Total Loss: 0.7413233518600464, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.2339959144592285, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1359, Total Loss: 0.7475932836532593, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.237427234649658, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1360, Total Loss: 0.7528272867202759, Accuracy Rate: 69.41%,            Test_Total_Loss: 4.254763603210449, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1361, Total Loss: 0.7793819308280945, Accuracy Rate: 69.08%,            Test_Total_Loss: 4.229333400726318, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1362, Total Loss: 0.782844603061676, Accuracy Rate: 68.66%,            Test_Total_Loss: 4.180049896240234, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1363, Total Loss: 0.7768762707710266, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.188266277313232, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1364, Total Loss: 0.790257453918457, Accuracy Rate: 68.14%,            Test_Total_Loss: 4.177603244781494, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1365, Total Loss: 0.7775288224220276, Accuracy Rate: 69.55%,            Test_Total_Loss: 4.155460834503174, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1366, Total Loss: 0.7608779072761536, Accuracy Rate: 69.78%,            Test_Total_Loss: 4.18030309677124, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 1367, Total Loss: 0.7633591294288635, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.217719554901123, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1368, Total Loss: 0.7690781354904175, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.152212619781494, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1369, Total Loss: 0.7560573816299438, Accuracy Rate: 69.36%,            Test_Total_Loss: 4.092754364013672, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1370, Total Loss: 0.7599273324012756, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.149306297302246, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1371, Total Loss: 0.759462833404541, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.115653991699219, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1372, Total Loss: 0.75187748670578, Accuracy Rate: 68.84%,            Test_Total_Loss: 4.191883563995361, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1373, Total Loss: 0.7532039284706116, Accuracy Rate: 68.84%,            Test_Total_Loss: 4.214880466461182, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1374, Total Loss: 0.7689721584320068, Accuracy Rate: 69.41%,            Test_Total_Loss: 4.209221839904785, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1375, Total Loss: 0.7682802677154541, Accuracy Rate: 69.50%,            Test_Total_Loss: 4.171324729919434, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1376, Total Loss: 0.755940854549408, Accuracy Rate: 69.08%,            Test_Total_Loss: 4.129623889923096, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1377, Total Loss: 0.7480564117431641, Accuracy Rate: 69.22%,            Test_Total_Loss: 4.221863269805908, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1378, Total Loss: 0.7387152910232544, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.246547698974609, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1379, Total Loss: 0.7419290542602539, Accuracy Rate: 69.64%,            Test_Total_Loss: 4.282133102416992, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1380, Total Loss: 0.7441956996917725, Accuracy Rate: 70.07%,            Test_Total_Loss: 4.271877765655518, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1381, Total Loss: 0.7359436750411987, Accuracy Rate: 70.21%,            Test_Total_Loss: 4.310569763183594, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1382, Total Loss: 0.7956724762916565, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.3794050216674805, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1383, Total Loss: 0.8638001680374146, Accuracy Rate: 66.45%,            Test_Total_Loss: 4.1590657234191895, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1384, Total Loss: 0.8960496187210083, Accuracy Rate: 65.93%,            Test_Total_Loss: 4.00480842590332, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1385, Total Loss: 0.9918707013130188, Accuracy Rate: 64.47%,            Test_Total_Loss: 3.9857289791107178, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1386, Total Loss: 1.0012953281402588, Accuracy Rate: 64.29%,            Test_Total_Loss: 3.996044635772705, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1387, Total Loss: 1.0375392436981201, Accuracy Rate: 63.06%,            Test_Total_Loss: 4.056848526000977, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1388, Total Loss: 1.0239824056625366, Accuracy Rate: 64.38%,            Test_Total_Loss: 4.028366565704346, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1389, Total Loss: 0.9737870097160339, Accuracy Rate: 65.04%,            Test_Total_Loss: 3.8992369174957275, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1390, Total Loss: 0.8877897262573242, Accuracy Rate: 67.25%,            Test_Total_Loss: 3.8933300971984863, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1391, Total Loss: 0.9420598149299622, Accuracy Rate: 66.96%,            Test_Total_Loss: 3.865297794342041, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1392, Total Loss: 0.9617416858673096, Accuracy Rate: 66.02%,            Test_Total_Loss: 3.85654354095459, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1393, Total Loss: 0.8587820529937744, Accuracy Rate: 68.42%,            Test_Total_Loss: 3.905257225036621, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1394, Total Loss: 0.8416099548339844, Accuracy Rate: 68.14%,            Test_Total_Loss: 3.8751728534698486, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1395, Total Loss: 0.835580050945282, Accuracy Rate: 67.86%,            Test_Total_Loss: 3.887855052947998, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1396, Total Loss: 0.8237249851226807, Accuracy Rate: 68.05%,            Test_Total_Loss: 3.8689370155334473, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1397, Total Loss: 0.8256081938743591, Accuracy Rate: 68.42%,            Test_Total_Loss: 3.840130090713501, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1398, Total Loss: 0.809428870677948, Accuracy Rate: 68.80%,            Test_Total_Loss: 3.8615705966949463, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1399, Total Loss: 0.7878704071044922, Accuracy Rate: 68.75%,            Test_Total_Loss: 3.884549379348755, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1400, Total Loss: 0.7811620235443115, Accuracy Rate: 69.13%,            Test_Total_Loss: 3.9955196380615234, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1401, Total Loss: 0.791638195514679, Accuracy Rate: 69.08%,            Test_Total_Loss: 3.9370734691619873, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1402, Total Loss: 0.7870527505874634, Accuracy Rate: 68.84%,            Test_Total_Loss: 3.886186361312866, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1403, Total Loss: 0.7830567359924316, Accuracy Rate: 68.66%,            Test_Total_Loss: 3.9179887771606445, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1404, Total Loss: 0.7722263932228088, Accuracy Rate: 69.08%,            Test_Total_Loss: 3.9059383869171143, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1405, Total Loss: 0.7692240476608276, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.912168025970459, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1406, Total Loss: 0.774452805519104, Accuracy Rate: 68.52%,            Test_Total_Loss: 3.8723268508911133, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1407, Total Loss: 0.7900002002716064, Accuracy Rate: 69.41%,            Test_Total_Loss: 3.9989187717437744, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1408, Total Loss: 0.784393310546875, Accuracy Rate: 68.94%,            Test_Total_Loss: 3.963693857192993, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1409, Total Loss: 0.7827137112617493, Accuracy Rate: 68.70%,            Test_Total_Loss: 3.9037909507751465, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1410, Total Loss: 0.7882834672927856, Accuracy Rate: 69.13%,            Test_Total_Loss: 3.949125289916992, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1411, Total Loss: 0.7729165554046631, Accuracy Rate: 69.17%,            Test_Total_Loss: 3.9980201721191406, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1412, Total Loss: 0.7786271572113037, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.034684658050537, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1413, Total Loss: 0.7666192054748535, Accuracy Rate: 68.56%,            Test_Total_Loss: 4.032792568206787, Test_Accuracy_Rate: 35.44%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1414, Total Loss: 0.7718176245689392, Accuracy Rate: 69.50%,            Test_Total_Loss: 4.005251884460449, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1415, Total Loss: 0.7598044276237488, Accuracy Rate: 70.44%,            Test_Total_Loss: 3.947439670562744, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1416, Total Loss: 0.7623544931411743, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.059091091156006, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1417, Total Loss: 0.7713025808334351, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.049379825592041, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1418, Total Loss: 0.7735221982002258, Accuracy Rate: 69.13%,            Test_Total_Loss: 3.99381947517395, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1419, Total Loss: 0.770135760307312, Accuracy Rate: 69.17%,            Test_Total_Loss: 4.000482559204102, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1420, Total Loss: 0.7694846391677856, Accuracy Rate: 69.83%,            Test_Total_Loss: 4.026490688323975, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1421, Total Loss: 0.7953795790672302, Accuracy Rate: 68.89%,            Test_Total_Loss: 3.9146173000335693, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1422, Total Loss: 0.8058568835258484, Accuracy Rate: 68.94%,            Test_Total_Loss: 3.9407849311828613, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1423, Total Loss: 0.8371709585189819, Accuracy Rate: 68.05%,            Test_Total_Loss: 3.9196126461029053, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1424, Total Loss: 0.9258270859718323, Accuracy Rate: 65.93%,            Test_Total_Loss: 4.020589351654053, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1425, Total Loss: 0.9993588924407959, Accuracy Rate: 64.47%,            Test_Total_Loss: 4.015030860900879, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1426, Total Loss: 1.0463230609893799, Accuracy Rate: 63.25%,            Test_Total_Loss: 3.9545490741729736, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1427, Total Loss: 0.970794677734375, Accuracy Rate: 65.46%,            Test_Total_Loss: 3.9015307426452637, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1428, Total Loss: 0.8679329752922058, Accuracy Rate: 67.39%,            Test_Total_Loss: 3.907543897628784, Test_Accuracy_Rate: 37.13%\n",
      "Epoch 1429, Total Loss: 0.8295683860778809, Accuracy Rate: 68.66%,            Test_Total_Loss: 3.90360164642334, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1430, Total Loss: 0.8352718353271484, Accuracy Rate: 68.66%,            Test_Total_Loss: 3.8891353607177734, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1431, Total Loss: 0.828865647315979, Accuracy Rate: 68.66%,            Test_Total_Loss: 3.9911577701568604, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1432, Total Loss: 0.8152455687522888, Accuracy Rate: 68.14%,            Test_Total_Loss: 3.927652597427368, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1433, Total Loss: 0.7984857559204102, Accuracy Rate: 69.31%,            Test_Total_Loss: 3.9537792205810547, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1434, Total Loss: 0.7851847410202026, Accuracy Rate: 69.03%,            Test_Total_Loss: 3.8976874351501465, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1435, Total Loss: 0.7751067876815796, Accuracy Rate: 69.08%,            Test_Total_Loss: 3.93327260017395, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1436, Total Loss: 0.7754347324371338, Accuracy Rate: 69.31%,            Test_Total_Loss: 3.9450600147247314, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1437, Total Loss: 0.7770264744758606, Accuracy Rate: 69.41%,            Test_Total_Loss: 4.030636787414551, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1438, Total Loss: 0.7689573168754578, Accuracy Rate: 69.92%,            Test_Total_Loss: 4.051629543304443, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1439, Total Loss: 0.75571608543396, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.046454906463623, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1440, Total Loss: 0.7664880752563477, Accuracy Rate: 69.22%,            Test_Total_Loss: 3.978792905807495, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1441, Total Loss: 0.765753984451294, Accuracy Rate: 69.27%,            Test_Total_Loss: 4.056155681610107, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1442, Total Loss: 0.7578009963035583, Accuracy Rate: 70.11%,            Test_Total_Loss: 4.029139995574951, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1443, Total Loss: 0.7482759952545166, Accuracy Rate: 69.64%,            Test_Total_Loss: 4.0557541847229, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1444, Total Loss: 0.7969505786895752, Accuracy Rate: 68.94%,            Test_Total_Loss: 4.192899227142334, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1445, Total Loss: 0.8159605860710144, Accuracy Rate: 68.61%,            Test_Total_Loss: 4.150846481323242, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1446, Total Loss: 0.788363516330719, Accuracy Rate: 69.08%,            Test_Total_Loss: 4.114667892456055, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1447, Total Loss: 0.7737727761268616, Accuracy Rate: 69.13%,            Test_Total_Loss: 4.126451015472412, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1448, Total Loss: 0.7718097567558289, Accuracy Rate: 69.31%,            Test_Total_Loss: 4.027264595031738, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1449, Total Loss: 0.7699945569038391, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.022796154022217, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1450, Total Loss: 0.7652779221534729, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.112685680389404, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1451, Total Loss: 0.7675708532333374, Accuracy Rate: 68.56%,            Test_Total_Loss: 4.050023555755615, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1452, Total Loss: 0.7656200528144836, Accuracy Rate: 68.98%,            Test_Total_Loss: 4.0802130699157715, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1453, Total Loss: 0.764687180519104, Accuracy Rate: 69.03%,            Test_Total_Loss: 4.173210620880127, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1454, Total Loss: 0.757143497467041, Accuracy Rate: 69.13%,            Test_Total_Loss: 4.1285295486450195, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1455, Total Loss: 0.7520360350608826, Accuracy Rate: 69.27%,            Test_Total_Loss: 4.16526985168457, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1456, Total Loss: 0.7532092332839966, Accuracy Rate: 68.89%,            Test_Total_Loss: 4.184006214141846, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1457, Total Loss: 0.7701713442802429, Accuracy Rate: 69.08%,            Test_Total_Loss: 4.170085906982422, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1458, Total Loss: 0.7629184722900391, Accuracy Rate: 69.08%,            Test_Total_Loss: 4.138741970062256, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1459, Total Loss: 0.7530529499053955, Accuracy Rate: 69.36%,            Test_Total_Loss: 4.155580520629883, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1460, Total Loss: 0.7479994893074036, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.184709548950195, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1461, Total Loss: 0.7442484498023987, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.256383895874023, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1462, Total Loss: 0.7731165885925293, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.185967445373535, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1463, Total Loss: 0.7587064504623413, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.205187797546387, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1464, Total Loss: 0.7587308883666992, Accuracy Rate: 70.16%,            Test_Total_Loss: 4.15315580368042, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1465, Total Loss: 0.7570063471794128, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.221330642700195, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1466, Total Loss: 0.7522348761558533, Accuracy Rate: 69.22%,            Test_Total_Loss: 4.280984401702881, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1467, Total Loss: 0.777708888053894, Accuracy Rate: 69.31%,            Test_Total_Loss: 4.20531702041626, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1468, Total Loss: 0.8095338344573975, Accuracy Rate: 68.33%,            Test_Total_Loss: 4.1077494621276855, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1469, Total Loss: 0.78631991147995, Accuracy Rate: 69.78%,            Test_Total_Loss: 4.07252311706543, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1470, Total Loss: 0.7905055284500122, Accuracy Rate: 68.84%,            Test_Total_Loss: 4.1781792640686035, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1471, Total Loss: 0.7954612374305725, Accuracy Rate: 68.89%,            Test_Total_Loss: 4.302361965179443, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1472, Total Loss: 0.7702376842498779, Accuracy Rate: 68.33%,            Test_Total_Loss: 4.239683628082275, Test_Accuracy_Rate: 34.18%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1473, Total Loss: 0.7545607686042786, Accuracy Rate: 69.50%,            Test_Total_Loss: 4.277024269104004, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1474, Total Loss: 0.7894275188446045, Accuracy Rate: 68.42%,            Test_Total_Loss: 4.283023834228516, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1475, Total Loss: 0.7857649326324463, Accuracy Rate: 69.55%,            Test_Total_Loss: 4.269284248352051, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1476, Total Loss: 0.8469697833061218, Accuracy Rate: 68.33%,            Test_Total_Loss: 4.18035888671875, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1477, Total Loss: 0.8988521099090576, Accuracy Rate: 66.92%,            Test_Total_Loss: 4.157741546630859, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1478, Total Loss: 1.0428287982940674, Accuracy Rate: 63.11%,            Test_Total_Loss: 4.008432865142822, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1479, Total Loss: 0.9803405404090881, Accuracy Rate: 65.23%,            Test_Total_Loss: 3.952718734741211, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1480, Total Loss: 0.9118010401725769, Accuracy Rate: 65.13%,            Test_Total_Loss: 3.8279199600219727, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1481, Total Loss: 0.8572869300842285, Accuracy Rate: 68.94%,            Test_Total_Loss: 3.995511054992676, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1482, Total Loss: 0.8212396502494812, Accuracy Rate: 68.19%,            Test_Total_Loss: 4.082178115844727, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1483, Total Loss: 0.7959094643592834, Accuracy Rate: 69.41%,            Test_Total_Loss: 3.9965929985046387, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1484, Total Loss: 0.7765841484069824, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.9489119052886963, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1485, Total Loss: 0.775404691696167, Accuracy Rate: 69.31%,            Test_Total_Loss: 4.036132335662842, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1486, Total Loss: 0.7514554858207703, Accuracy Rate: 70.11%,            Test_Total_Loss: 4.104015827178955, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1487, Total Loss: 0.7486482262611389, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.040802955627441, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1488, Total Loss: 0.7486604452133179, Accuracy Rate: 69.97%,            Test_Total_Loss: 4.079704284667969, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1489, Total Loss: 0.7417265772819519, Accuracy Rate: 70.68%,            Test_Total_Loss: 4.096755027770996, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1490, Total Loss: 0.7336716055870056, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.184906959533691, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1491, Total Loss: 0.7398812174797058, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.117921352386475, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1492, Total Loss: 0.7370906472206116, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.168572425842285, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1493, Total Loss: 0.9517188668251038, Accuracy Rate: 66.73%,            Test_Total_Loss: 4.099720001220703, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1494, Total Loss: 1.066284418106079, Accuracy Rate: 62.73%,            Test_Total_Loss: 4.016263961791992, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1495, Total Loss: 1.0257065296173096, Accuracy Rate: 64.52%,            Test_Total_Loss: 4.056545734405518, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1496, Total Loss: 0.9503824710845947, Accuracy Rate: 65.70%,            Test_Total_Loss: 3.936182737350464, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1497, Total Loss: 0.8799419403076172, Accuracy Rate: 66.87%,            Test_Total_Loss: 3.8340792655944824, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1498, Total Loss: 0.8612983822822571, Accuracy Rate: 67.20%,            Test_Total_Loss: 3.908374309539795, Test_Accuracy_Rate: 36.71%\n",
      "Epoch 1499, Total Loss: 0.8242955803871155, Accuracy Rate: 68.28%,            Test_Total_Loss: 3.930103063583374, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1500, Total Loss: 0.8200166821479797, Accuracy Rate: 68.89%,            Test_Total_Loss: 3.9346044063568115, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1501, Total Loss: 0.8217489719390869, Accuracy Rate: 68.56%,            Test_Total_Loss: 3.872002124786377, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1502, Total Loss: 0.8175392150878906, Accuracy Rate: 68.33%,            Test_Total_Loss: 3.9860806465148926, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1503, Total Loss: 0.8109649419784546, Accuracy Rate: 68.80%,            Test_Total_Loss: 4.01928186416626, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1504, Total Loss: 0.7997739911079407, Accuracy Rate: 69.13%,            Test_Total_Loss: 3.905930280685425, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1505, Total Loss: 0.8008130192756653, Accuracy Rate: 68.28%,            Test_Total_Loss: 3.996622085571289, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1506, Total Loss: 0.7935577034950256, Accuracy Rate: 68.75%,            Test_Total_Loss: 3.8998565673828125, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1507, Total Loss: 0.7837305665016174, Accuracy Rate: 69.41%,            Test_Total_Loss: 3.977592706680298, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1508, Total Loss: 0.7729244232177734, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.973722457885742, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1509, Total Loss: 0.7644094824790955, Accuracy Rate: 69.83%,            Test_Total_Loss: 3.9697437286376953, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1510, Total Loss: 0.7584640383720398, Accuracy Rate: 70.21%,            Test_Total_Loss: 3.9784085750579834, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1511, Total Loss: 0.7581671476364136, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.065441608428955, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1512, Total Loss: 0.7596257328987122, Accuracy Rate: 69.64%,            Test_Total_Loss: 4.0140180587768555, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1513, Total Loss: 0.7609094977378845, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.031280040740967, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1514, Total Loss: 0.756396472454071, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.066257953643799, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1515, Total Loss: 0.752987265586853, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.100306510925293, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1516, Total Loss: 0.7444501519203186, Accuracy Rate: 70.39%,            Test_Total_Loss: 4.144604206085205, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1517, Total Loss: 0.7529258131980896, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.10805082321167, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1518, Total Loss: 0.7623776793479919, Accuracy Rate: 69.45%,            Test_Total_Loss: 3.9069674015045166, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1519, Total Loss: 0.7825020551681519, Accuracy Rate: 69.31%,            Test_Total_Loss: 3.985802173614502, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1520, Total Loss: 0.7987824082374573, Accuracy Rate: 68.66%,            Test_Total_Loss: 4.137732028961182, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1521, Total Loss: 0.7906549572944641, Accuracy Rate: 68.89%,            Test_Total_Loss: 4.275715351104736, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1522, Total Loss: 0.7946741580963135, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.186048984527588, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1523, Total Loss: 1.3802857398986816, Accuracy Rate: 58.22%,            Test_Total_Loss: 3.9091849327087402, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 1524, Total Loss: 1.1047481298446655, Accuracy Rate: 60.67%,            Test_Total_Loss: 3.9202404022216797, Test_Accuracy_Rate: 29.11%\n",
      "Epoch 1525, Total Loss: 0.9424571990966797, Accuracy Rate: 65.41%,            Test_Total_Loss: 3.884498119354248, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1526, Total Loss: 0.8748878836631775, Accuracy Rate: 66.49%,            Test_Total_Loss: 3.8987550735473633, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1527, Total Loss: 0.8250643610954285, Accuracy Rate: 68.75%,            Test_Total_Loss: 3.976987838745117, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 1528, Total Loss: 0.7949341535568237, Accuracy Rate: 69.13%,            Test_Total_Loss: 3.8124043941497803, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1529, Total Loss: 0.7961101531982422, Accuracy Rate: 69.55%,            Test_Total_Loss: 3.8592143058776855, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1530, Total Loss: 0.7758265733718872, Accuracy Rate: 69.55%,            Test_Total_Loss: 3.869692325592041, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1531, Total Loss: 0.7688661813735962, Accuracy Rate: 69.27%,            Test_Total_Loss: 3.9547247886657715, Test_Accuracy_Rate: 32.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1532, Total Loss: 0.7598644495010376, Accuracy Rate: 69.78%,            Test_Total_Loss: 3.905301570892334, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1533, Total Loss: 0.7591823935508728, Accuracy Rate: 69.83%,            Test_Total_Loss: 3.933668375015259, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1534, Total Loss: 0.7521177530288696, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.014835357666016, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1535, Total Loss: 0.7483314275741577, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.00590705871582, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1536, Total Loss: 0.7401536107063293, Accuracy Rate: 70.07%,            Test_Total_Loss: 4.016366958618164, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1537, Total Loss: 0.7466709613800049, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.025745868682861, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1538, Total Loss: 0.7435594797134399, Accuracy Rate: 69.78%,            Test_Total_Loss: 4.010174751281738, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1539, Total Loss: 0.7416053414344788, Accuracy Rate: 70.16%,            Test_Total_Loss: 4.040007591247559, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1540, Total Loss: 0.7431825995445251, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.068453788757324, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1541, Total Loss: 0.7386181950569153, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.136075496673584, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1542, Total Loss: 0.7424818873405457, Accuracy Rate: 69.78%,            Test_Total_Loss: 4.114339828491211, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1543, Total Loss: 0.7552405595779419, Accuracy Rate: 69.50%,            Test_Total_Loss: 4.114025592803955, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1544, Total Loss: 0.7690883278846741, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.110391139984131, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1545, Total Loss: 0.7662701606750488, Accuracy Rate: 68.47%,            Test_Total_Loss: 4.093638896942139, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1546, Total Loss: 0.7605316042900085, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.129701137542725, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1547, Total Loss: 0.753389835357666, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.1458048820495605, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1548, Total Loss: 0.7541724443435669, Accuracy Rate: 69.92%,            Test_Total_Loss: 4.180573463439941, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1549, Total Loss: 0.7784146070480347, Accuracy Rate: 69.03%,            Test_Total_Loss: 4.216648578643799, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1550, Total Loss: 0.7613655924797058, Accuracy Rate: 69.22%,            Test_Total_Loss: 4.14174747467041, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1551, Total Loss: 0.7749343514442444, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.110793113708496, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1552, Total Loss: 0.8193914294242859, Accuracy Rate: 67.95%,            Test_Total_Loss: 4.102603435516357, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1553, Total Loss: 0.8306070566177368, Accuracy Rate: 68.09%,            Test_Total_Loss: 4.201765060424805, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1554, Total Loss: 0.7862253189086914, Accuracy Rate: 69.27%,            Test_Total_Loss: 4.145617961883545, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1555, Total Loss: 0.7850525975227356, Accuracy Rate: 69.31%,            Test_Total_Loss: 4.162225723266602, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1556, Total Loss: 0.7726379036903381, Accuracy Rate: 68.89%,            Test_Total_Loss: 4.201329231262207, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1557, Total Loss: 0.7540004253387451, Accuracy Rate: 69.17%,            Test_Total_Loss: 4.198106288909912, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1558, Total Loss: 0.7570644617080688, Accuracy Rate: 69.64%,            Test_Total_Loss: 4.214725017547607, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1559, Total Loss: 0.7550944685935974, Accuracy Rate: 69.31%,            Test_Total_Loss: 4.249499797821045, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1560, Total Loss: 0.7454237937927246, Accuracy Rate: 69.97%,            Test_Total_Loss: 4.293212413787842, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1561, Total Loss: 0.7413756251335144, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.389097690582275, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1562, Total Loss: 0.7564114332199097, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.290703296661377, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1563, Total Loss: 0.7518923878669739, Accuracy Rate: 70.82%,            Test_Total_Loss: 4.339623928070068, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1564, Total Loss: 0.7464600205421448, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.297064781188965, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1565, Total Loss: 0.7431640028953552, Accuracy Rate: 69.78%,            Test_Total_Loss: 4.3437275886535645, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1566, Total Loss: 0.7600635290145874, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.3106279373168945, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1567, Total Loss: 0.8056647181510925, Accuracy Rate: 68.14%,            Test_Total_Loss: 4.327282428741455, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1568, Total Loss: 0.7976441979408264, Accuracy Rate: 68.89%,            Test_Total_Loss: 4.293466567993164, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1569, Total Loss: 0.7819559574127197, Accuracy Rate: 68.80%,            Test_Total_Loss: 4.2142438888549805, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1570, Total Loss: 0.7749893069267273, Accuracy Rate: 68.89%,            Test_Total_Loss: 4.233677864074707, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1571, Total Loss: 0.7661028504371643, Accuracy Rate: 69.17%,            Test_Total_Loss: 4.206601619720459, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1572, Total Loss: 0.7521255612373352, Accuracy Rate: 70.07%,            Test_Total_Loss: 4.136981010437012, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1573, Total Loss: 0.8147754669189453, Accuracy Rate: 68.47%,            Test_Total_Loss: 4.178293704986572, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1574, Total Loss: 0.842244565486908, Accuracy Rate: 67.81%,            Test_Total_Loss: 4.200728893280029, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1575, Total Loss: 0.8011618256568909, Accuracy Rate: 69.08%,            Test_Total_Loss: 4.21442174911499, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1576, Total Loss: 0.8105212450027466, Accuracy Rate: 69.36%,            Test_Total_Loss: 4.269372940063477, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1577, Total Loss: 0.9554259777069092, Accuracy Rate: 64.99%,            Test_Total_Loss: 4.394693851470947, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1578, Total Loss: 0.9256423115730286, Accuracy Rate: 65.74%,            Test_Total_Loss: 4.093066215515137, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1579, Total Loss: 0.8340579271316528, Accuracy Rate: 68.23%,            Test_Total_Loss: 3.9765892028808594, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1580, Total Loss: 0.8014858365058899, Accuracy Rate: 68.89%,            Test_Total_Loss: 4.161435604095459, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1581, Total Loss: 0.7884446978569031, Accuracy Rate: 69.13%,            Test_Total_Loss: 4.063776016235352, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1582, Total Loss: 0.7803707122802734, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.215842247009277, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1583, Total Loss: 0.7707167267799377, Accuracy Rate: 69.22%,            Test_Total_Loss: 4.1716485023498535, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1584, Total Loss: 0.762702465057373, Accuracy Rate: 69.64%,            Test_Total_Loss: 4.16945743560791, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1585, Total Loss: 0.7587893009185791, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.1837992668151855, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1586, Total Loss: 0.7550413608551025, Accuracy Rate: 69.55%,            Test_Total_Loss: 4.212204456329346, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1587, Total Loss: 0.7509167194366455, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.244126796722412, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1588, Total Loss: 0.7429050207138062, Accuracy Rate: 70.16%,            Test_Total_Loss: 4.246086597442627, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1589, Total Loss: 0.733643651008606, Accuracy Rate: 71.19%,            Test_Total_Loss: 4.333458423614502, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1590, Total Loss: 0.7386366128921509, Accuracy Rate: 70.30%,            Test_Total_Loss: 4.240998268127441, Test_Accuracy_Rate: 34.60%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1591, Total Loss: 0.741765022277832, Accuracy Rate: 70.39%,            Test_Total_Loss: 4.274364471435547, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1592, Total Loss: 0.7378396987915039, Accuracy Rate: 69.97%,            Test_Total_Loss: 4.396039009094238, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1593, Total Loss: 0.7694005966186523, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.341800212860107, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1594, Total Loss: 0.8324177861213684, Accuracy Rate: 67.95%,            Test_Total_Loss: 4.3349289894104, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1595, Total Loss: 0.8078568577766418, Accuracy Rate: 68.28%,            Test_Total_Loss: 4.331369876861572, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1596, Total Loss: 0.7891924381256104, Accuracy Rate: 68.98%,            Test_Total_Loss: 4.280277252197266, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1597, Total Loss: 0.7837364077568054, Accuracy Rate: 69.22%,            Test_Total_Loss: 4.344986915588379, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1598, Total Loss: 0.77956622838974, Accuracy Rate: 68.84%,            Test_Total_Loss: 4.296319961547852, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1599, Total Loss: 0.779944896697998, Accuracy Rate: 70.35%,            Test_Total_Loss: 4.342476844787598, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1600, Total Loss: 0.7632617354393005, Accuracy Rate: 69.83%,            Test_Total_Loss: 4.332607269287109, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1601, Total Loss: 0.7843673229217529, Accuracy Rate: 69.78%,            Test_Total_Loss: 4.3285603523254395, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1602, Total Loss: 0.8202518224716187, Accuracy Rate: 68.56%,            Test_Total_Loss: 4.297356128692627, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1603, Total Loss: 0.8457009196281433, Accuracy Rate: 67.90%,            Test_Total_Loss: 4.270237922668457, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1604, Total Loss: 0.8371840119361877, Accuracy Rate: 67.86%,            Test_Total_Loss: 4.389919281005859, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1605, Total Loss: 0.8148121237754822, Accuracy Rate: 68.09%,            Test_Total_Loss: 4.294651508331299, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1606, Total Loss: 0.7904196977615356, Accuracy Rate: 68.80%,            Test_Total_Loss: 4.2925238609313965, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1607, Total Loss: 0.7840720415115356, Accuracy Rate: 69.03%,            Test_Total_Loss: 4.445219993591309, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1608, Total Loss: 0.7682809829711914, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.320211410522461, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1609, Total Loss: 0.7464976906776428, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.382232189178467, Test_Accuracy_Rate: 30.38%\n",
      "Epoch 1610, Total Loss: 0.7449136972427368, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.428499221801758, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1611, Total Loss: 0.7344095706939697, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.445082187652588, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1612, Total Loss: 0.7260536551475525, Accuracy Rate: 70.30%,            Test_Total_Loss: 4.439963340759277, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1613, Total Loss: 0.7308255434036255, Accuracy Rate: 70.54%,            Test_Total_Loss: 4.480269908905029, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1614, Total Loss: 0.733376145362854, Accuracy Rate: 70.44%,            Test_Total_Loss: 4.474218368530273, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1615, Total Loss: 0.751572847366333, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.517767429351807, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1616, Total Loss: 0.7647159099578857, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.4841179847717285, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 1617, Total Loss: 0.7926238179206848, Accuracy Rate: 69.31%,            Test_Total_Loss: 4.3517374992370605, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1618, Total Loss: 0.8147303462028503, Accuracy Rate: 68.66%,            Test_Total_Loss: 4.393595218658447, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1619, Total Loss: 0.8188450932502747, Accuracy Rate: 67.90%,            Test_Total_Loss: 4.520549297332764, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1620, Total Loss: 0.7935739755630493, Accuracy Rate: 68.42%,            Test_Total_Loss: 4.4534831047058105, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1621, Total Loss: 0.7784258723258972, Accuracy Rate: 69.83%,            Test_Total_Loss: 4.502288341522217, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1622, Total Loss: 0.7725558280944824, Accuracy Rate: 68.98%,            Test_Total_Loss: 4.535516738891602, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1623, Total Loss: 0.7499062418937683, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.545855522155762, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1624, Total Loss: 0.7418522238731384, Accuracy Rate: 70.11%,            Test_Total_Loss: 4.57813835144043, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1625, Total Loss: 0.7400205135345459, Accuracy Rate: 69.92%,            Test_Total_Loss: 4.554620265960693, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1626, Total Loss: 0.7350372076034546, Accuracy Rate: 70.49%,            Test_Total_Loss: 4.564164638519287, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1627, Total Loss: 0.733999490737915, Accuracy Rate: 69.88%,            Test_Total_Loss: 4.583923816680908, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1628, Total Loss: 0.7272934913635254, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.586142063140869, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1629, Total Loss: 0.7348710298538208, Accuracy Rate: 70.16%,            Test_Total_Loss: 4.554396152496338, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1630, Total Loss: 0.736844003200531, Accuracy Rate: 69.64%,            Test_Total_Loss: 4.506166934967041, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1631, Total Loss: 0.7405499815940857, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.485743045806885, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1632, Total Loss: 0.7281144857406616, Accuracy Rate: 69.83%,            Test_Total_Loss: 4.603193283081055, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1633, Total Loss: 0.7386950850486755, Accuracy Rate: 70.35%,            Test_Total_Loss: 4.579866886138916, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1634, Total Loss: 0.7273907661437988, Accuracy Rate: 70.07%,            Test_Total_Loss: 4.503973007202148, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1635, Total Loss: 0.7320929765701294, Accuracy Rate: 69.83%,            Test_Total_Loss: 4.606690883636475, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1636, Total Loss: 0.7266104221343994, Accuracy Rate: 70.11%,            Test_Total_Loss: 4.629438400268555, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1637, Total Loss: 0.7415360808372498, Accuracy Rate: 69.69%,            Test_Total_Loss: 4.443222522735596, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1638, Total Loss: 0.7411997318267822, Accuracy Rate: 70.54%,            Test_Total_Loss: 4.551227569580078, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1639, Total Loss: 0.7269006967544556, Accuracy Rate: 70.21%,            Test_Total_Loss: 4.515481948852539, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1640, Total Loss: 0.7490755319595337, Accuracy Rate: 70.30%,            Test_Total_Loss: 4.513891220092773, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1641, Total Loss: 0.7452232241630554, Accuracy Rate: 69.31%,            Test_Total_Loss: 4.434452056884766, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1642, Total Loss: 0.7425483465194702, Accuracy Rate: 69.78%,            Test_Total_Loss: 4.487611293792725, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1643, Total Loss: 0.7390002608299255, Accuracy Rate: 70.07%,            Test_Total_Loss: 4.392449378967285, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1644, Total Loss: 0.7511107325553894, Accuracy Rate: 69.22%,            Test_Total_Loss: 4.417306900024414, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1645, Total Loss: 0.7439188361167908, Accuracy Rate: 69.55%,            Test_Total_Loss: 4.451622486114502, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1646, Total Loss: 0.7460272312164307, Accuracy Rate: 70.25%,            Test_Total_Loss: 4.324838161468506, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1647, Total Loss: 0.7485979795455933, Accuracy Rate: 70.30%,            Test_Total_Loss: 4.439409255981445, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1648, Total Loss: 0.794805645942688, Accuracy Rate: 69.27%,            Test_Total_Loss: 4.401570796966553, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1649, Total Loss: 0.8024933934211731, Accuracy Rate: 68.70%,            Test_Total_Loss: 4.296709060668945, Test_Accuracy_Rate: 31.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1650, Total Loss: 0.7850615978240967, Accuracy Rate: 69.08%,            Test_Total_Loss: 4.411296844482422, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1651, Total Loss: 0.7616278529167175, Accuracy Rate: 69.92%,            Test_Total_Loss: 4.34602165222168, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1652, Total Loss: 0.8306052088737488, Accuracy Rate: 68.37%,            Test_Total_Loss: 4.274631977081299, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1653, Total Loss: 0.7856627106666565, Accuracy Rate: 69.08%,            Test_Total_Loss: 4.434279918670654, Test_Accuracy_Rate: 36.29%\n",
      "Epoch 1654, Total Loss: 0.7927295565605164, Accuracy Rate: 69.03%,            Test_Total_Loss: 4.290724754333496, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1655, Total Loss: 0.8051433563232422, Accuracy Rate: 68.98%,            Test_Total_Loss: 4.35840368270874, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1656, Total Loss: 0.8002768754959106, Accuracy Rate: 68.66%,            Test_Total_Loss: 4.371464252471924, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1657, Total Loss: 0.8021088242530823, Accuracy Rate: 69.31%,            Test_Total_Loss: 4.380478858947754, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1658, Total Loss: 0.7491052746772766, Accuracy Rate: 69.92%,            Test_Total_Loss: 4.23622465133667, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1659, Total Loss: 0.7566245794296265, Accuracy Rate: 70.07%,            Test_Total_Loss: 4.380094051361084, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1660, Total Loss: 0.7912237048149109, Accuracy Rate: 68.89%,            Test_Total_Loss: 4.418092250823975, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1661, Total Loss: 0.7593099474906921, Accuracy Rate: 69.55%,            Test_Total_Loss: 4.497487545013428, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1662, Total Loss: 0.745804488658905, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.500785827636719, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1663, Total Loss: 0.7223032116889954, Accuracy Rate: 70.35%,            Test_Total_Loss: 4.492361068725586, Test_Accuracy_Rate: 35.44%\n",
      "Epoch 1664, Total Loss: 0.7288210988044739, Accuracy Rate: 69.83%,            Test_Total_Loss: 4.49215030670166, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1665, Total Loss: 0.728981077671051, Accuracy Rate: 70.16%,            Test_Total_Loss: 4.5161895751953125, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1666, Total Loss: 0.7201871275901794, Accuracy Rate: 70.16%,            Test_Total_Loss: 4.4983625411987305, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1667, Total Loss: 0.7204709649085999, Accuracy Rate: 70.58%,            Test_Total_Loss: 4.520657062530518, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1668, Total Loss: 0.7174258828163147, Accuracy Rate: 70.16%,            Test_Total_Loss: 4.58363676071167, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1669, Total Loss: 0.7167502045631409, Accuracy Rate: 70.35%,            Test_Total_Loss: 4.592092514038086, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1670, Total Loss: 0.7138355374336243, Accuracy Rate: 70.21%,            Test_Total_Loss: 4.588729381561279, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1671, Total Loss: 0.7134861350059509, Accuracy Rate: 69.83%,            Test_Total_Loss: 4.640589237213135, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1672, Total Loss: 0.7202856540679932, Accuracy Rate: 70.16%,            Test_Total_Loss: 4.628087043762207, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1673, Total Loss: 0.7148248553276062, Accuracy Rate: 70.54%,            Test_Total_Loss: 4.692643165588379, Test_Accuracy_Rate: 31.65%\n",
      "Epoch 1674, Total Loss: 0.715587854385376, Accuracy Rate: 70.77%,            Test_Total_Loss: 4.660012722015381, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1675, Total Loss: 0.7313727140426636, Accuracy Rate: 69.45%,            Test_Total_Loss: 4.604091644287109, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1676, Total Loss: 0.7373230457305908, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.54293155670166, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1677, Total Loss: 0.7618330121040344, Accuracy Rate: 69.36%,            Test_Total_Loss: 4.547318935394287, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1678, Total Loss: 0.7743493914604187, Accuracy Rate: 68.89%,            Test_Total_Loss: 4.57555627822876, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1679, Total Loss: 0.7901636362075806, Accuracy Rate: 68.89%,            Test_Total_Loss: 4.432056903839111, Test_Accuracy_Rate: 33.33%\n",
      "Epoch 1680, Total Loss: 0.7826469540596008, Accuracy Rate: 69.31%,            Test_Total_Loss: 4.515650749206543, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1681, Total Loss: 0.7917441725730896, Accuracy Rate: 69.50%,            Test_Total_Loss: 4.448497772216797, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1682, Total Loss: 0.8339021801948547, Accuracy Rate: 67.29%,            Test_Total_Loss: 4.533729553222656, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1683, Total Loss: 0.8648293018341064, Accuracy Rate: 66.82%,            Test_Total_Loss: 4.38948917388916, Test_Accuracy_Rate: 34.60%\n",
      "Epoch 1684, Total Loss: 0.8464770317077637, Accuracy Rate: 67.15%,            Test_Total_Loss: 4.600627899169922, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1685, Total Loss: 0.8185340762138367, Accuracy Rate: 67.53%,            Test_Total_Loss: 4.313640594482422, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1686, Total Loss: 1.00282621383667, Accuracy Rate: 64.61%,            Test_Total_Loss: 4.139027118682861, Test_Accuracy_Rate: 35.86%\n",
      "Epoch 1687, Total Loss: 1.10933256149292, Accuracy Rate: 63.49%,            Test_Total_Loss: 3.9810938835144043, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1688, Total Loss: 0.9900354146957397, Accuracy Rate: 63.67%,            Test_Total_Loss: 4.041893482208252, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1689, Total Loss: 0.9339626431465149, Accuracy Rate: 64.94%,            Test_Total_Loss: 3.9354147911071777, Test_Accuracy_Rate: 33.76%\n",
      "Epoch 1690, Total Loss: 0.8803952932357788, Accuracy Rate: 67.43%,            Test_Total_Loss: 4.046902179718018, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1691, Total Loss: 0.8317408561706543, Accuracy Rate: 68.89%,            Test_Total_Loss: 4.01936149597168, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1692, Total Loss: 0.8157252073287964, Accuracy Rate: 68.37%,            Test_Total_Loss: 3.980139970779419, Test_Accuracy_Rate: 34.18%\n",
      "Epoch 1693, Total Loss: 0.7830745577812195, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.052223205566406, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1694, Total Loss: 0.795530378818512, Accuracy Rate: 69.50%,            Test_Total_Loss: 4.300100803375244, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1695, Total Loss: 0.7959248423576355, Accuracy Rate: 68.75%,            Test_Total_Loss: 4.202641010284424, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1696, Total Loss: 0.7924216985702515, Accuracy Rate: 68.56%,            Test_Total_Loss: 4.2715253829956055, Test_Accuracy_Rate: 30.80%\n",
      "Epoch 1697, Total Loss: 0.7668781280517578, Accuracy Rate: 69.74%,            Test_Total_Loss: 4.151874542236328, Test_Accuracy_Rate: 35.02%\n",
      "Epoch 1698, Total Loss: 0.7613565325737, Accuracy Rate: 69.60%,            Test_Total_Loss: 4.096521854400635, Test_Accuracy_Rate: 32.07%\n",
      "Epoch 1699, Total Loss: 0.7452833652496338, Accuracy Rate: 70.35%,            Test_Total_Loss: 4.171240329742432, Test_Accuracy_Rate: 32.49%\n",
      "Epoch 1700, Total Loss: 0.7436180114746094, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.178414344787598, Test_Accuracy_Rate: 32.91%\n",
      "Epoch 1701, Total Loss: 0.7390062212944031, Accuracy Rate: 70.07%,            Test_Total_Loss: 4.219326496124268, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1702, Total Loss: 0.7331385016441345, Accuracy Rate: 70.21%,            Test_Total_Loss: 4.164287567138672, Test_Accuracy_Rate: 31.22%\n",
      "Epoch 1703, Total Loss: 0.7360318899154663, Accuracy Rate: 70.02%,            Test_Total_Loss: 4.205368995666504, Test_Accuracy_Rate: 34.18%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-5a7fee473c6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class test_model(Model):\n",
    "    def __init__(self):\n",
    "        super(test_model, self).__init__()\n",
    "        self.emb = Embedding(vocabulary_size+1, emb_dim,input_length=max_length\n",
    "                             ,trainable=True,name='embedding')\n",
    "        self.rnn = LSTM(hidden_dim,return_sequences=True,return_state=False,name='common_extract'\n",
    "                      ,trainable=True)\n",
    "        self.bn1 = BatchNormalization(name='bn1')\n",
    "        self.bn2 = BatchNormalization(name='bn2')\n",
    "        self.att = Attention(name='selfatt')\n",
    "        self.rnn2 = GRU(int(hidden_dim/2),name='common_extract2'\n",
    "                      ,trainable=True)\n",
    "        self.dn = Dense(int(hidden_dim/4),activation='selu',\n",
    "                        kernel_initializer=tf.keras.initializers.lecun_normal(), name='dn')\n",
    "        self.out = Dense(max(y_train)+1,activation='softmax',name='clf')\n",
    "    def call(self,x):\n",
    "        x1 = self.emb(x)\n",
    "        x = self.rnn(x1)\n",
    "        x = self.bn1(x)\n",
    "        #x = self.att([x,x1,x1])\n",
    "        x = self.rnn2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dn(x)\n",
    "        y2 = self.out(x)\n",
    "        return y2\n",
    "model = test_model()\n",
    "batch_size = 32\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train,y_train)).shuffle(X_train.shape[0]).batch(batch_size)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(batch_size)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy') \n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(x,yc):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_cat = model(x) #weight: pred_imp; 0/1binary: pred_imp2 。#pahse1: pred_imp; phase2; pred_imp2\n",
    "        loss = loss_object(yc, pred_cat)\n",
    "    trainable_variable = model.trainable_variables\n",
    "    gradients = tape.gradient(loss,trainable_variable)\n",
    "    optimizer.apply_gradients(zip(gradients,trainable_variable))\n",
    "    \n",
    "    train_loss(loss) #total_loss\n",
    "    train_accuracy(yc, pred_cat) #acc_rate\n",
    "\n",
    "@tf.function\n",
    "def test_step(x,yc):\n",
    "    pred_cat = model(x) #weight: pred_imp ; binary:pred_imp2\n",
    "    t_loss = loss_object2(yc, pred_cat)\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(yc, pred_cat)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for text, labels in train_ds:\n",
    "        train_step(text, labels)\n",
    "\n",
    "    for test_text, test_labels in valid_ds:\n",
    "        test_step(test_text, test_labels) #with restore words\n",
    "    \n",
    "    template = 'Epoch {}, Total Loss: {}, Accuracy Rate: {:5.2f}%,\\\n",
    "            Test_Total_Loss: {}, Test_Accuracy_Rate: {:5.2f}%'\n",
    "    print(template.format(epoch+1,train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100\n",
    "                        ))\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 182, 128), dtype=float32, numpy=\n",
       " array([[[ 0.05026103, -0.03759918, -0.0075792 , ..., -0.05715944,\n",
       "          -0.00929288,  0.07889917],\n",
       "         [ 0.04546504, -0.01330616, -0.08277178, ...,  0.0956112 ,\n",
       "           0.01197832,  0.04695564],\n",
       "         [ 0.05026103, -0.03759918, -0.0075792 , ..., -0.05715944,\n",
       "          -0.00929288,  0.07889917],\n",
       "         ...,\n",
       "         [-0.05260951,  0.04872397, -0.03031672, ...,  0.00519623,\n",
       "          -0.02559018, -0.03490983],\n",
       "         [-0.05260951,  0.04872397, -0.03031672, ...,  0.00519623,\n",
       "          -0.02559018, -0.03490983],\n",
       "         [-0.05260951,  0.04872397, -0.03031672, ...,  0.00519623,\n",
       "          -0.02559018, -0.03490983]],\n",
       " \n",
       "        [[-0.03128414,  0.03349531, -0.0571312 , ...,  0.00304027,\n",
       "           0.00620059, -0.00962268],\n",
       "         [ 0.05026103, -0.03759918, -0.0075792 , ..., -0.05715944,\n",
       "          -0.00929288,  0.07889917],\n",
       "         [ 0.09278463,  0.00644924,  0.01887746, ...,  0.08126906,\n",
       "          -0.07023067,  0.0265457 ],\n",
       "         ...,\n",
       "         [-0.05260951,  0.04872397, -0.03031672, ...,  0.00519623,\n",
       "          -0.02559018, -0.03490983],\n",
       "         [-0.05260951,  0.04872397, -0.03031672, ...,  0.00519623,\n",
       "          -0.02559018, -0.03490983],\n",
       "         [-0.05260951,  0.04872397, -0.03031672, ...,  0.00519623,\n",
       "          -0.02559018, -0.03490983]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 182, 1), dtype=float32, numpy=\n",
       " array([[[0.69214606],\n",
       "         [0.69168156],\n",
       "         [0.69193673],\n",
       "         [0.69214463],\n",
       "         [0.69228816],\n",
       "         [0.69238   ],\n",
       "         [0.69243497],\n",
       "         [0.69142264],\n",
       "         [0.6910454 ],\n",
       "         [0.6915591 ],\n",
       "         [0.6907061 ],\n",
       "         [0.6908472 ],\n",
       "         [0.6914505 ],\n",
       "         [0.69065773],\n",
       "         [0.69117075],\n",
       "         [0.6914711 ],\n",
       "         [0.69069684],\n",
       "         [0.6912888 ],\n",
       "         [0.6908518 ],\n",
       "         [0.6912708 ],\n",
       "         [0.69103026],\n",
       "         [0.68843305],\n",
       "         [0.6891277 ],\n",
       "         [0.6896567 ],\n",
       "         [0.6907783 ],\n",
       "         [0.69151527],\n",
       "         [0.6916537 ],\n",
       "         [0.69190425],\n",
       "         [0.69211817],\n",
       "         [0.69226897],\n",
       "         [0.692181  ],\n",
       "         [0.691805  ],\n",
       "         [0.69137156],\n",
       "         [0.69222045],\n",
       "         [0.69124943],\n",
       "         [0.69225097],\n",
       "         [0.69227904],\n",
       "         [0.69157606],\n",
       "         [0.6918166 ],\n",
       "         [0.692147  ],\n",
       "         [0.6915486 ],\n",
       "         [0.6916647 ],\n",
       "         [0.69223547],\n",
       "         [0.69164485],\n",
       "         [0.69141555],\n",
       "         [0.6924881 ],\n",
       "         [0.6920124 ],\n",
       "         [0.69275475],\n",
       "         [0.69215775],\n",
       "         [0.69284934],\n",
       "         [0.6922255 ],\n",
       "         [0.6928975 ],\n",
       "         [0.69225985],\n",
       "         [0.6929211 ],\n",
       "         [0.69227636],\n",
       "         [0.69293225],\n",
       "         [0.6922839 ],\n",
       "         [0.692937  ],\n",
       "         [0.692287  ],\n",
       "         [0.6929391 ],\n",
       "         [0.692288  ],\n",
       "         [0.6929401 ],\n",
       "         [0.6922887 ],\n",
       "         [0.6929403 ],\n",
       "         [0.6922889 ],\n",
       "         [0.69294035],\n",
       "         [0.69228894],\n",
       "         [0.69294035],\n",
       "         [0.69228894],\n",
       "         [0.6929404 ],\n",
       "         [0.6922255 ],\n",
       "         [0.6920951 ],\n",
       "         [0.6925165 ],\n",
       "         [0.6917709 ],\n",
       "         [0.69156945],\n",
       "         [0.691386  ],\n",
       "         [0.69115484],\n",
       "         [0.6909711 ],\n",
       "         [0.692018  ],\n",
       "         [0.6916751 ],\n",
       "         [0.6913805 ],\n",
       "         [0.69115597],\n",
       "         [0.69098824],\n",
       "         [0.69087255],\n",
       "         [0.69058853],\n",
       "         [0.69100726],\n",
       "         [0.69115746],\n",
       "         [0.6910864 ],\n",
       "         [0.6909703 ],\n",
       "         [0.69203633],\n",
       "         [0.69116414],\n",
       "         [0.69106287],\n",
       "         [0.69072855],\n",
       "         [0.69064367],\n",
       "         [0.6914714 ],\n",
       "         [0.6908736 ],\n",
       "         [0.6909024 ],\n",
       "         [0.69102526],\n",
       "         [0.6911241 ],\n",
       "         [0.69091326],\n",
       "         [0.69134736],\n",
       "         [0.69101906],\n",
       "         [0.6907902 ],\n",
       "         [0.6904172 ],\n",
       "         [0.69027954],\n",
       "         [0.69082373],\n",
       "         [0.6910589 ],\n",
       "         [0.69116855],\n",
       "         [0.6912194 ],\n",
       "         [0.6912408 ],\n",
       "         [0.69124705],\n",
       "         [0.69124687],\n",
       "         [0.6912437 ],\n",
       "         [0.6912401 ],\n",
       "         [0.69123644],\n",
       "         [0.69123393],\n",
       "         [0.6912321 ],\n",
       "         [0.6912305 ],\n",
       "         [0.6912292 ],\n",
       "         [0.69122887],\n",
       "         [0.69122803],\n",
       "         [0.69122785],\n",
       "         [0.69122756],\n",
       "         [0.6912274 ],\n",
       "         [0.6912273 ],\n",
       "         [0.69122726],\n",
       "         [0.69122726],\n",
       "         [0.6912272 ],\n",
       "         [0.69122714],\n",
       "         [0.69122726],\n",
       "         [0.6912272 ],\n",
       "         [0.6912272 ],\n",
       "         [0.6912272 ],\n",
       "         [0.6912272 ],\n",
       "         [0.6912272 ],\n",
       "         [0.6912272 ],\n",
       "         [0.6912272 ],\n",
       "         [0.69122714],\n",
       "         [0.6912272 ],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714],\n",
       "         [0.69122714]],\n",
       " \n",
       "        [[0.7123216 ],\n",
       "         [0.71249956],\n",
       "         [0.71163934],\n",
       "         [0.71159124],\n",
       "         [0.7120533 ],\n",
       "         [0.71126944],\n",
       "         [0.7113326 ],\n",
       "         [0.7118592 ],\n",
       "         [0.7123859 ],\n",
       "         [0.71261597],\n",
       "         [0.71252334],\n",
       "         [0.7122388 ],\n",
       "         [0.7125617 ],\n",
       "         [0.71287787],\n",
       "         [0.71208394],\n",
       "         [0.71236295],\n",
       "         [0.7121548 ],\n",
       "         [0.7122527 ],\n",
       "         [0.71198934],\n",
       "         [0.71013755],\n",
       "         [0.7111206 ],\n",
       "         [0.711564  ],\n",
       "         [0.7117813 ],\n",
       "         [0.71221834],\n",
       "         [0.7114817 ],\n",
       "         [0.71193933],\n",
       "         [0.71145505],\n",
       "         [0.7113308 ],\n",
       "         [0.71234506],\n",
       "         [0.712947  ],\n",
       "         [0.71295136],\n",
       "         [0.712799  ],\n",
       "         [0.7117362 ],\n",
       "         [0.71129745],\n",
       "         [0.7120154 ],\n",
       "         [0.7122224 ],\n",
       "         [0.712533  ],\n",
       "         [0.71211636],\n",
       "         [0.7126842 ],\n",
       "         [0.7116541 ],\n",
       "         [0.71173483],\n",
       "         [0.7123398 ],\n",
       "         [0.71253556],\n",
       "         [0.71256685],\n",
       "         [0.7125422 ],\n",
       "         [0.7125034 ],\n",
       "         [0.71246624],\n",
       "         [0.7124381 ],\n",
       "         [0.71241695],\n",
       "         [0.71240264],\n",
       "         [0.712394  ],\n",
       "         [0.71238756],\n",
       "         [0.712384  ],\n",
       "         [0.71238166],\n",
       "         [0.71238005],\n",
       "         [0.7123791 ],\n",
       "         [0.712379  ],\n",
       "         [0.7123786 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123785 ],\n",
       "         [0.7123783 ],\n",
       "         [0.71237826],\n",
       "         [0.7123783 ],\n",
       "         [0.7123783 ],\n",
       "         [0.71237826],\n",
       "         [0.7123784 ],\n",
       "         [0.71237826],\n",
       "         [0.71237826],\n",
       "         [0.71237826],\n",
       "         [0.71237826],\n",
       "         [0.71237826],\n",
       "         [0.71237826],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ],\n",
       "         [0.7123784 ]]], dtype=float32)>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class base_model_1(Model):\n",
    "    def __init__(self):\n",
    "        super(base_model_1, self).__init__()\n",
    "        self.mask = Masking(mask_value=0)\n",
    "        self.emb = Embedding(vocabulary_size, emb_dim,input_length=max_length\n",
    "                             ,trainable=True,name='embedding')\n",
    "        self.rnn1 = GRU(emb_dim,return_sequences=True,return_state=False,name='common_extract'\n",
    "                      ,trainable=True)\n",
    "        self.att = Attention(name='selfatt')\n",
    "        self.bn1 = BatchNormalization(name='bn1')\n",
    "        #self.fil = Dense(1,activation=onezero,name='filter_out')\n",
    "        self.fil = TimeDistributed(Dense(1,activation=onezero,kernel_initializer=init_w,bias_initializer=init_b, name='filter_out'),name='TD2') #relu/linear/step function\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.mask(x)\n",
    "        x1 = self.emb(x)\n",
    "        #x = self.att([x1,x1])\n",
    "        x = self.rnn1(x1)\n",
    "        x = self.bn1(x)\n",
    "        x = self.att([x,x1,x1])\n",
    "        y = self.fil(x)\n",
    "        return x1,y\n",
    "\n",
    "model1 = base_model_1()\n",
    "model1.load_weights(saveP1)\n",
    "model1(X_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_model_2(Model):\n",
    "    def __init__(self):\n",
    "        super(base_model_2, self).__init__()\n",
    "        self.mul = Multiply()\n",
    "        #self.rnn2 = Bidirectional(LSTM(int(hidden_dim),dropout=do,recurrent_dropout=do,name='lstm'))\n",
    "        self.rnn3 = GRU(int(hidden_dim/2))\n",
    "        self.bn2 = BatchNormalization(name='bn2')\n",
    "        self.dn = Dense(int(hidden_dim/4),activation='selu',\n",
    "                        kernel_initializer=tf.keras.initializers.lecun_normal(), name='dn')\n",
    "        self.out = Dense(max(y_train)+1,activation='softmax',name='clf')\n",
    "\n",
    "    def call(self,x1,y1):\n",
    "        x2 = self.mul([y1,x1])\n",
    "        x = self.rnn3(x2) #x2 #y1=weight|binary\n",
    "        x = self.bn2(x)\n",
    "        x = self.dn(x)\n",
    "        y2 = self.out(x)\n",
    "        return y2\n",
    "    \n",
    "model2 = base_model_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 182, 181), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 1., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_num = 2 #連續幾個才叫做連續，要改modify_idx看有幾個\n",
    "\n",
    "arr_len = max_length - seq_num + 1\n",
    "seq_arr = []\n",
    "for i in range(arr_len):\n",
    "    ori_np = np.array([0]*max_length)\n",
    "    modify_idx = [i,i+1] #要跟著seq_num改\n",
    "    ori_np[modify_idx]=1\n",
    "    seq_arr.append(ori_np)\n",
    "seq_arr = np.array(seq_arr)\n",
    "seq_mask = tf.Variable(seq_arr.T,dtype='float32')\n",
    "seq_mask = tf.expand_dims(seq_mask,axis=0)\n",
    "seq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_object1(predictions): #filter loss\n",
    "    mask = tf.math.logical_not(tf.math.equal(predictions, 0))\n",
    "    loss_ = tf.reduce_mean(predictions)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "def one_percentage(predictions): #1 num\n",
    "    mask = tf.math.logical_not(tf.math.equal(predictions, 0))\n",
    "    loss_ = tf.reduce_mean(predictions)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "def seq_loss(predictions):\n",
    "    mask = tf.math.logical_not(tf.math.equal(predictions, 0))\n",
    "    predictions = tf.squeeze(predictions,axis=-1)\n",
    "    results = tf.matmul(predictions,seq_mask)\n",
    "    results = tf.where(results==seq_num,1.0,0.0)\n",
    "    loss_ = tf.reduce_mean(results)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "def word_reproduce1(prediction_int):\n",
    "    return prediction_int\n",
    "def word_reproduce2(prediction_int):\n",
    "    return prediction_int\n",
    "loss_object2 = tf.keras.losses.SparseCategoricalCrossentropy() #clf loss\n",
    "\n",
    "# optimizer1 = tf.keras.optimizers.SGD(learning_rate=0.0044, momentum=0.5, nesterov=True)\n",
    "optimizer1 = tf.keras.optimizers.Nadam()#Adam(learning_rate=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss') #total_loss\n",
    "train_accloss = tf.keras.metrics.Mean(name='train_accloss')#loss_acc\n",
    "train_filloss = tf.keras.metrics.Mean(name='train_filloss') #loss_filter\n",
    "train_seqloss = tf.keras.metrics.Mean(name='train_seqloss') #loss_seq\n",
    "train_ones = tf.keras.metrics.Mean(name='train_ones') #ones_num\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy') #acc_rate\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss') #total_loss\n",
    "test_accloss = tf.keras.metrics.Mean(name='test_accloss')#loss_acc\n",
    "test_filloss = tf.keras.metrics.Mean(name='test_filloss') #loss_filter\n",
    "test_seqloss = tf.keras.metrics.Mean(name='test_seqloss')\n",
    "test_ones = tf.keras.metrics.Mean(name='test_ones') #ones_num\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0000001#0.000002 #pahse1: -0.1 / 0.0 ; phase2: 0.01~0.05~0.1 有1-就是希望1越多，沒1-就是希望0越多1越少\n",
    "beta = 1.0 #clf loss 越大越要求分好\n",
    "gamma = 0.0 #seqloss 越大越要求連續\n",
    "#-0.001 / 1.0 / 1.0\n",
    "\n",
    "@tf.function\n",
    "def train_step(x,yc):\n",
    "    with tf.GradientTape(persistent=False) as tape:\n",
    "        emb, pred_imp = model1(x)\n",
    "        #loss1 = alpha*loss_object1(pred_imp) #phase1\n",
    "        #pred_imp2 = tf.math.round(pred_imp)\n",
    "        #pred_imp3 = tf.clip_by_value(pred_imp,clip_value_max=1,clip_value_min=0)\n",
    "        pred_imp2 = tf.math.round(pred_imp)\n",
    "        loss1 = 1-loss_object1(pred_imp) #有1-就是希望1越多，沒1-就是希望0越多1越少 #pahse2:alpha*loss_object1(pred_imp) ; phase1: alpha*(1-loss_object1(pred_imp))\n",
    "        pred_cat = model2(emb,pred_imp2) #weight: pred_imp; 0/1binary: pred_imp2 。#pahse1: pred_imp; phase2; pred_imp2\n",
    "        loss2 = loss_object2(yc, pred_cat)\n",
    "        loss3 = 1-seq_loss(pred_imp2)\n",
    "        loss = alpha*loss1 + beta*loss2 + gamma*loss3\n",
    "    trainable_variable = model1.trainable_variables\n",
    "    trainable_variable.extend(model2.trainable_variables)\n",
    "    gradients = tape.gradient(loss,trainable_variable)\n",
    "    optimizer1.apply_gradients(zip(gradients,trainable_variable))\n",
    "    \n",
    "    train_loss(loss) #total_loss\n",
    "    train_filloss(loss1)\n",
    "    train_accloss(loss2)\n",
    "    train_seqloss(loss3) #loss_seq\n",
    "    train_accuracy(yc, pred_cat) #acc_rate\n",
    "    ones = one_percentage(pred_imp2) #pred_imp2\n",
    "    train_ones(ones) #ones_num\n",
    "    \n",
    "    \n",
    "@tf.function\n",
    "def test_step(x,yc):\n",
    "    emb, pred_imp = model1(x)\n",
    "    #loss1 = alpha*loss_object1(pred_imp) #phase1\n",
    "    #pred_imp2 = tf.math.round(pred_imp)\n",
    "    #pred_imp3 = tf.clip_by_value(pred_imp,clip_value_max=1,clip_value_min=0)\n",
    "    pred_imp2 = tf.math.round(pred_imp)\n",
    "    loss1 = loss_object1(pred_imp) #phase2\n",
    "    pred_cat = model2(emb,pred_imp2) #weight: pred_imp ; binary:pred_imp2\n",
    "    loss2 = loss_object2(yc, pred_cat)\n",
    "    loss3 = 1-seq_loss(pred_imp2)\n",
    "    #t_loss = loss1 + loss2\n",
    "    t_loss = alpha*loss1 + beta*loss2 + gamma*loss3\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_filloss(loss1)\n",
    "    test_accloss(loss2)\n",
    "    test_seqloss(loss3)\n",
    "    test_accuracy(yc, pred_cat)\n",
    "    t_ones = one_percentage(pred_imp2) #pred_imp2\n",
    "    test_ones(t_ones)\n",
    "\n",
    "    \n",
    "# word_idx_pd = pd.DataFrame(data=word_index,index=[\"ID\"]).T\n",
    "# word_idx_pd['ori_word'] = word_idx_pd.index \n",
    "# word_idx_pd = word_idx_pd.set_index([\"ID\"])\n",
    "# print(word_idx_pd)\n",
    "\n",
    "ep = -1\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def test_step_inf(x,yc,epoch):\n",
    "    #ori_text = getKeysByValues(list(x))\n",
    "    emb, pred_imp = model1(x)\n",
    "    #loss1 = alpha*loss_object1(pred_imp) #phase1\n",
    "    #pred_imp2 = tf.math.round(pred_imp)\n",
    "    #pred_imp3 = tf.clip_by_value(pred_imp,clip_value_max=1,clip_value_min=0)\n",
    "    pred_imp2 = tf.math.round(pred_imp)\n",
    "    #if ep != epoch:\n",
    "        #ep = epoch\n",
    "        #x = tf.dtypes.cast(x,dtype=tf.float32)\n",
    "    pred_imp3_ = tf.squeeze(pred_imp2)\n",
    "    pred_imp3 = tf.dtypes.cast(pred_imp3_,dtype=tf.int32)\n",
    "    used_x = tf.math.multiply(x,pred_imp3)\n",
    "    filter_emb = tf.math.multiply(emb,pred_imp2)\n",
    "        #word_reproduce1(x)\n",
    "        #word_reproduce2(used_x)\n",
    "        #used_text = getKeysByValues(list(used_x))\n",
    "        #tf.print(used_x)\n",
    "        #print(emb.numpy(),pred_imp, x,pred_imp3,used_x)\n",
    "        #int_ori_text.append(x)\n",
    "        #int_fil_text.append(used_x)\n",
    "    \n",
    "    loss1 = loss_object1(pred_imp) #phase2\n",
    "    pred_cat = model2(emb,pred_imp2) #weight: pred_imp ; binary:pred_imp2\n",
    "    loss2 = loss_object2(yc, pred_cat)\n",
    "    loss3 = 1-seq_loss(pred_imp2)\n",
    "    #t_loss = loss1 + loss2\n",
    "    t_loss = alpha*loss1 + beta*loss2 + gamma*loss3\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_filloss(loss1)\n",
    "    test_accloss(loss2)\n",
    "    test_seqloss(loss3)\n",
    "    test_accuracy(yc, pred_cat)\n",
    "    t_ones = one_percentage(pred_imp2) #pred_imp2\n",
    "    test_ones(t_ones)\n",
    "    return x, used_x, pred_cat, emb, filter_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Total Loss: 0.8048461675643921, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 69.15%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 2, Total Loss: 0.8320643305778503, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 68.14%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 3, Total Loss: 0.8325566053390503, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 68.33%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 4, Total Loss: 0.8727174401283264, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 67.90%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 5, Total Loss: 0.9017594456672668, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 66.64%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 6, Total Loss: 0.9170001149177551, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 66.07%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 7, Total Loss: 0.9004886746406555, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 66.73%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 8, Total Loss: 0.8528181910514832, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 68.00%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 9, Total Loss: 0.8446152210235596, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 67.90%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 10, Total Loss: 0.8383711576461792, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 68.56%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 11, Total Loss: 0.8207250833511353, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 68.28%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 12, Total Loss: 0.7937115430831909, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 68.89%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 13, Total Loss: 0.7880153656005859, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 69.97%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 14, Total Loss: 0.7830978035926819, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 69.08%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 15, Total Loss: 0.768572986125946, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 68.94%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n",
      "Epoch 16, Total Loss: 0.7659801244735718, Clf Loss: 0.0, Filter Loss: 0.0, Seq Loss: 0.0, Accuracy Rate: 69.50%, Ones Portion: 0.0,             Test_Total_Loss: 4.283809185028076, Test_Clf_Loss: 4.283809185028076, Test_Filter_Loss: 0.9998735189437866, TEST_Seq_Loss: 0.0, Test_Accuracy_Rate: 30.38%, Test_Ones_Portion: 1.0\n",
      "===MODEL WEIGHTS SAVED=== ./model/Hooklog/2019122601/model1 ./model/Hooklog/2019122601/model2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-6cfb3144826b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 2000\n",
    "DateID = '2019122601'\n",
    "\n",
    "saveP1 = './model/Hooklog/'+DateID+'/model1'\n",
    "saveP2 = './model/Hooklog/'+DateID+'/model2'\n",
    "train_loss_acc = []\n",
    "train_loss_filter = []\n",
    "train_loss_seq = []\n",
    "train_weighted_loss = []\n",
    "train_acc_rate = []\n",
    "train_ones_num = []\n",
    "\n",
    "test_loss_acc = []\n",
    "test_loss_filter = []\n",
    "test_loss_seq = []\n",
    "test_weighted_loss = []\n",
    "test_acc_rate = []\n",
    "test_ones_num = []\n",
    "\n",
    "int_ori_text =[]\n",
    "int_fil_text = []\n",
    "int_label = []\n",
    "pred_label = []\n",
    "ori_emb = []\n",
    "fil_emb = []\n",
    "epoch_num = []\n",
    "fam_lab = []\n",
    "pred_lab = []\n",
    "# signature_dict = {'att':model1.att}\n",
    "\n",
    "gc.collect()\n",
    "best_clf = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch = tf.convert_to_tensor(epoch, dtype=tf.int64)\n",
    "    for text, labels in train_ds:\n",
    "        train_step(text, labels)\n",
    "\n",
    "    for test_text, test_labels in valid_ds:\n",
    "        ori,fil,pred_cat,emb,filter_emb = test_step_inf(test_text, test_labels,epoch) #with restore words\n",
    "        ori_emb.append(emb.numpy()) #一個epoch有很多個batches\n",
    "        fil_emb.append(filter_emb.numpy()) #一個epoch有很多個batches\n",
    "        epoch_num.append(epoch.numpy()) #一個epoch一個數字\n",
    "        fam_lab.append(test_labels.numpy())#一個epoch有很多個batches\n",
    "        pred_lab.append(pred_cat.numpy())#一個epoch有很多個batches\n",
    "        #print(k2)\n",
    "        #test_step(test_text, test_labels) #original\n",
    "    int_ori_text.append(ori.numpy())\n",
    "    int_fil_text.append(fil.numpy())\n",
    "    int_label.append(test_labels.numpy()) #true label\n",
    "    pred_label.append(pred_cat.numpy())\n",
    "\n",
    "    \n",
    "    template = 'Epoch {}, Total Loss: {}, Clf Loss: {}, Filter Loss: {}, Seq Loss: {}, Accuracy Rate: {:5.2f}%, Ones Portion: {}, \\\n",
    "            Test_Total_Loss: {}, Test_Clf_Loss: {}, Test_Filter_Loss: {}, TEST_Seq_Loss: {}, Test_Accuracy_Rate: {:5.2f}%, Test_Ones_Portion: {}'\n",
    "    print(template.format(epoch+1,train_loss.result(),\n",
    "                          train_accloss.result(),train_filloss.result(),train_seqloss.result(),\n",
    "                        train_accuracy.result()*100,train_ones.result(),\n",
    "                        test_loss.result(),\n",
    "                        test_accloss.result(),test_filloss.result(),test_seqloss.result(),\n",
    "                        test_accuracy.result()*100,test_ones.result(),\n",
    "                        ))\n",
    "    #int_ori_text.append(word_reproduce1.result().numpy())\n",
    "    #int_fil_text.append(word_reproduce2.result().numpy())\n",
    "    train_loss_acc.append( train_accloss.result().numpy())\n",
    "    train_loss_filter.append( train_filloss.result().numpy())\n",
    "    train_loss_seq.append( train_seqloss.result().numpy())\n",
    "    train_weighted_loss.append( train_loss.result().numpy())\n",
    "    train_acc_rate.append( train_accuracy.result().numpy())\n",
    "    train_ones_num.append( train_ones.result().numpy())\n",
    "    \n",
    "    test_loss_acc.append( test_accloss.result().numpy())\n",
    "    test_loss_filter.append( test_filloss.result().numpy())\n",
    "    test_loss_seq.append( test_seqloss.result().numpy())\n",
    "    test_weighted_loss.append( test_loss.result().numpy())\n",
    "    test_acc_rate.append( test_accuracy.result().numpy())\n",
    "    test_ones_num.append( test_ones.result().numpy())\n",
    "    if best_clf<=test_accuracy.result()*100:\n",
    "        #tf.saved_model.save(model1,saveP1+'_all')\n",
    "        #model1.save(saveP1,save_format='h5')\n",
    "        #model2.save(saveP2,save_format='h5')\n",
    "        #tf.saved_model.save(model2,saveP2+'_all')\n",
    "        model1.save_weights(saveP1,save_format='tf')\n",
    "        model2.save_weights(saveP2,save_format='tf')\n",
    "        best_clf = test_accuracy.result()*100\n",
    "        print('===MODEL WEIGHTS SAVED===',saveP1,saveP2)\n",
    "    # Reset the metrics for the next epoch\n",
    "    #word_reproduce1.reset_states()\n",
    "    #word_reproduce2.reset_states()\n",
    "    train_loss.reset_states()\n",
    "    train_accloss.reset_states()\n",
    "    train_filloss.reset_states()\n",
    "    train_seqloss.reset_states()\n",
    "    train_ones.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    test_accloss.reset_states()\n",
    "    test_filloss.reset_states()\n",
    "    test_seqloss.reset_states()\n",
    "    test_ones.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "pred_lab2 = []\n",
    "for pred in pred_lab:\n",
    "    pred_lab2.append(list(np.argmax(pred,axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lab2 = []\n",
    "for pred in pred_lab:\n",
    "    pred_lab2.append(list(np.argmax(pred,axis=-1)))\n",
    "len(pred_lab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(ep)\n",
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP: 0\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 1\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 2\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 3\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 4\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 5\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 6\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 7\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 8\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 9\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 10\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 11\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 12\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 13\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 14\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n",
      "EP: 15\n",
      "samples#: 32\n",
      "samples#: 64\n",
      "samples#: 96\n",
      "samples#: 128\n",
      "samples#: 160\n",
      "samples#: 192\n",
      "samples#: 224\n",
      "samples#: 237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recent_ep = -1\n",
    "for ep, o_e, f_e, p_l, t_l in zip(epoch_num,ori_emb,fil_emb,pred_lab2,fam_lab): #take put each epoch\n",
    "    if ep != recent_ep:\n",
    "        whole_dict_ori = {}\n",
    "        whole_dict_fil = {}\n",
    "#         print('EP:',ep)\n",
    "        recent_ep = ep\n",
    "        count =  0\n",
    "    for oe,fe,pl,tl in zip(o_e, f_e, p_l, t_l): #take out each process\n",
    "        count+=1\n",
    "        try:\n",
    "            temp1 = whole_dict_ori[tl] #put by true label\n",
    "            temp1.append(oe)\n",
    "            temp2 = whole_dict_fil[tl]\n",
    "            temp2.append(fe)\n",
    "            whole_dict_ori[tl] = temp1 #original embedding\n",
    "            whole_dict_fil[tl] = temp2 #filter embedding\n",
    "        except KeyError:\n",
    "            temp1 = [oe]\n",
    "            temp2 = [fe]\n",
    "            whole_dict_ori[tl] = temp1 \n",
    "            whole_dict_fil[tl] = temp2\n",
    "#     print('samples#:',count)\n",
    "#             if tl == 2:\n",
    "#                 print([list(oe),])\n",
    "#                 break            \n",
    "#         except AttributeError:\n",
    "#             temp1 = list([oe,])\n",
    "#             temp2 = list([fe,])\n",
    "#             whole_dict_ori[tl] = temp1 #tl==2 有問題@@\n",
    "#             whole_dict_fil[tl] = temp2#tl==2 有問題@@\n",
    "#             if tl == 2:\n",
    "#                 print([list(oe),])\n",
    "#                 break           \n",
    "# count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 182, 128)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(whole_dict_fil[0][1:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02495082,  1.        ,  1.        , ..., -0.30626938,\n",
       "        -0.30626938, -0.30626938],\n",
       "       [-0.15718652, -0.42860642, -0.42860642, ...,  0.20198658,\n",
       "         0.20198658,  0.20198658],\n",
       "       [ 0.13184482,  0.4822547 ,  0.4822547 , ...,  0.04879938,\n",
       "         0.04879938,  0.04879938],\n",
       "       ...,\n",
       "       [-0.10193527,  0.24838135,  0.24838135, ...,  0.10296886,\n",
       "         0.10296886,  0.10296886],\n",
       "       [-0.04273163,  0.62445796,  0.62445796, ..., -0.32532468,\n",
       "        -0.32532468, -0.32532468],\n",
       "       [ 0.06389179, -0.11722743, -0.11722743, ..., -0.07050402,\n",
       "        -0.07050402, -0.07050402]], dtype=float32)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.expand_dims(whole_dict_fil[0][0],axis=0)\n",
    "cosine_similarity(test[0],np.array(whole_dict_fil[0][1:])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7: [array([[-0.05921727, -0.22285984, -0.28284442, ...,  0.16880853,\n",
       "           0.02067087,  0.13023372],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [ 0.03967505,  0.11265814,  0.13778308, ..., -0.16289778,\n",
       "           0.05585124,  0.5513557 ],\n",
       "         ...,\n",
       "         [ 0.25461718, -0.28900287,  0.17536671, ..., -0.01272231,\n",
       "          -0.0379485 ,  0.0289838 ],\n",
       "         [ 0.08445907,  0.2207433 , -0.27071974, ..., -0.02107348,\n",
       "           0.17360754, -0.25596514],\n",
       "         [-0.00855983, -0.48185816, -0.20542417, ...,  0.1833916 ,\n",
       "           0.11007628, -0.4651002 ]], dtype=float32),\n",
       "  array([[-0.05921727, -0.22285984, -0.28284442, ...,  0.16880853,\n",
       "           0.02067087,  0.13023372],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [ 0.03967505,  0.11265814,  0.13778308, ..., -0.16289778,\n",
       "           0.05585124,  0.5513557 ],\n",
       "         ...,\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903]], dtype=float32),\n",
       "  array([[-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [ 0.1538219 ,  0.08651822, -0.05850999, ...,  0.17292847,\n",
       "           0.0122154 , -0.16761358],\n",
       "         [ 0.10163565,  0.11445903, -0.09826617, ...,  0.22165859,\n",
       "          -0.40742022,  0.02286944],\n",
       "         ...,\n",
       "         [ 0.18049093,  0.04728775,  0.04551627, ..., -0.31711292,\n",
       "          -0.15048902,  0.13286895],\n",
       "         [ 0.18049093,  0.04728775,  0.04551627, ..., -0.31711292,\n",
       "          -0.15048902,  0.13286895],\n",
       "         [ 0.18049093,  0.04728775,  0.04551627, ..., -0.31711292,\n",
       "          -0.15048902,  0.13286895]], dtype=float32)],\n",
       " 4: [array([[-0.05921727, -0.22285984, -0.28284442, ...,  0.16880853,\n",
       "           0.02067087,  0.13023372],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [-0.1027265 , -0.27635664, -0.2477804 , ...,  0.17568733,\n",
       "          -0.4284471 ,  0.08035108],\n",
       "         ...,\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903]], dtype=float32)],\n",
       " 2: [array([[-0.05921727, -0.22285984, -0.28284442, ...,  0.16880853,\n",
       "           0.02067087,  0.13023372],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [ 0.03967505,  0.11265814,  0.13778308, ..., -0.16289778,\n",
       "           0.05585124,  0.5513557 ],\n",
       "         ...,\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903]], dtype=float32),\n",
       "  array([[-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         ...,\n",
       "         [ 0.1480549 ,  0.00072115, -0.14540538, ...,  0.39226478,\n",
       "          -0.012312  ,  0.022385  ],\n",
       "         [ 0.18049093,  0.04728775,  0.04551627, ..., -0.31711292,\n",
       "          -0.15048902,  0.13286895],\n",
       "         [ 0.18049093,  0.04728775,  0.04551627, ..., -0.31711292,\n",
       "          -0.15048902,  0.13286895]], dtype=float32)],\n",
       " 11: [array([[-0.05921727, -0.22285984, -0.28284442, ...,  0.16880853,\n",
       "           0.02067087,  0.13023372],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [ 0.06986269,  0.21129695,  0.04954508, ...,  0.01387486,\n",
       "          -0.11838187, -0.14526248],\n",
       "         ...,\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903]], dtype=float32)],\n",
       " 1: [array([[-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         ...,\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903]], dtype=float32)],\n",
       " 9: [array([[-0.05921727, -0.22285984, -0.28284442, ...,  0.16880853,\n",
       "           0.02067087,  0.13023372],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [ 0.03967505,  0.11265814,  0.13778308, ..., -0.16289778,\n",
       "           0.05585124,  0.5513557 ],\n",
       "         ...,\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903]], dtype=float32),\n",
       "  array([[-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [ 0.1538219 ,  0.08651822, -0.05850999, ...,  0.17292847,\n",
       "           0.0122154 , -0.16761358],\n",
       "         [ 0.10163565,  0.11445903, -0.09826617, ...,  0.22165859,\n",
       "          -0.40742022,  0.02286944],\n",
       "         ...,\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903]], dtype=float32),\n",
       "  array([[-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [-0.1027265 , -0.27635664, -0.2477804 , ...,  0.17568733,\n",
       "          -0.4284471 ,  0.08035108],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         ...,\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903]], dtype=float32)],\n",
       " 0: [array([[-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         [-0.1027265 , -0.27635664, -0.2477804 , ...,  0.17568733,\n",
       "          -0.4284471 ,  0.08035108],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         ...,\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903],\n",
       "         [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "          -0.05969607,  0.01120903]], dtype=float32)],\n",
       " 3: [array([[ 0.1538219 ,  0.08651822, -0.05850999, ...,  0.17292847,\n",
       "           0.0122154 , -0.16761358],\n",
       "         [-0.05921727, -0.22285984, -0.28284442, ...,  0.16880853,\n",
       "           0.02067087,  0.13023372],\n",
       "         [-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "          -0.22155283,  0.06595514],\n",
       "         ...,\n",
       "         [ 0.17120706, -0.07097782, -0.05042176, ...,  0.1894411 ,\n",
       "           0.01535186, -0.2281921 ],\n",
       "         [-0.16201018, -0.07334327,  0.01504868, ..., -0.07045063,\n",
       "          -0.6137164 ,  0.09685928],\n",
       "         [ 0.18049093,  0.04728775,  0.04551627, ..., -0.31711292,\n",
       "          -0.15048902,  0.13286895]], dtype=float32)]}"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_dict_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "         -0.22155283,  0.06595514],\n",
       "        [ 0.1538219 ,  0.08651822, -0.05850999, ...,  0.17292847,\n",
       "          0.0122154 , -0.16761358],\n",
       "        [ 0.10163565,  0.11445903, -0.09826617, ...,  0.22165859,\n",
       "         -0.40742022,  0.02286944],\n",
       "        ...,\n",
       "        [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "         -0.05969607,  0.01120903],\n",
       "        [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "         -0.05969607,  0.01120903],\n",
       "        [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "         -0.05969607,  0.01120903]], dtype=float32),\n",
       " array([[-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "         -0.22155283,  0.06595514],\n",
       "        [ 0.1538219 ,  0.08651822, -0.05850999, ...,  0.17292847,\n",
       "          0.0122154 , -0.16761358],\n",
       "        [ 0.10163565,  0.11445903, -0.09826617, ...,  0.22165859,\n",
       "         -0.40742022,  0.02286944],\n",
       "        ...,\n",
       "        [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "         -0.05969607,  0.01120903],\n",
       "        [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "         -0.05969607,  0.01120903],\n",
       "        [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "         -0.05969607,  0.01120903]], dtype=float32)]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_dict_ori[tl] = [fe,]\n",
    "whole_dict_ori[tl].append(fe)\n",
    "whole_dict_ori[tl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "         -0.22155283,  0.06595514],\n",
       "        [ 0.1538219 ,  0.08651822, -0.05850999, ...,  0.17292847,\n",
       "          0.0122154 , -0.16761358],\n",
       "        [ 0.10163565,  0.11445903, -0.09826617, ...,  0.22165859,\n",
       "         -0.40742022,  0.02286944],\n",
       "        ...,\n",
       "        [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "         -0.05969607,  0.01120903],\n",
       "        [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "         -0.05969607,  0.01120903],\n",
       "        [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "         -0.05969607,  0.01120903]], dtype=float32),\n",
       " array([[-0.04800555, -0.06131621, -0.07549804, ..., -0.10737687,\n",
       "         -0.22155283,  0.06595514],\n",
       "        [ 0.1538219 ,  0.08651822, -0.05850999, ...,  0.17292847,\n",
       "          0.0122154 , -0.16761358],\n",
       "        [ 0.10163565,  0.11445903, -0.09826617, ...,  0.22165859,\n",
       "         -0.40742022,  0.02286944],\n",
       "        ...,\n",
       "        [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "         -0.05969607,  0.01120903],\n",
       "        [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "         -0.05969607,  0.01120903],\n",
       "        [-0.01391036,  0.05648555,  0.05279909, ..., -0.00105697,\n",
       "         -0.05969607,  0.01120903]], dtype=float32)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fe,fe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_lab2 = list(np.argmax(pred_lab,axis=-1))\n",
    "len(epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ori_emb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_emb \n",
    "fil_emb \n",
    "epoch_num \n",
    "fam_lab \n",
    "pred_lab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/Hooklog/2019122601/losses_metrics.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train total loss</th>\n",
       "      <th>train acc loss</th>\n",
       "      <th>train filter loss</th>\n",
       "      <th>train seq loss</th>\n",
       "      <th>train acc rate</th>\n",
       "      <th>train ones num</th>\n",
       "      <th>test total loss</th>\n",
       "      <th>test acc loss</th>\n",
       "      <th>test filter loss</th>\n",
       "      <th>test seq loss</th>\n",
       "      <th>test acc rate</th>\n",
       "      <th>test ones num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [train total loss, train acc loss, train filter loss, train seq loss, train acc rate, train ones num, test total loss, test acc loss, test filter loss, test seq loss, test acc rate, test ones num]\n",
       "Index: []"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = './results/Hooklog/'+DateID+'/'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "saveR = save_dir + 'losses_metrics.xlsx'\n",
    "data = {'train total loss':train_weighted_loss, 'train acc loss':train_loss_acc,\n",
    "        'train filter loss':train_loss_filter,'train seq loss':train_loss_seq,\n",
    "        'train acc rate':train_acc_rate, 'train ones num':train_ones_num,\n",
    "        'test total loss':test_weighted_loss, 'test acc loss':test_loss_acc,\n",
    "        'test filter loss':test_loss_filter, 'test seq loss': test_loss_seq,\n",
    "        'test acc rate':test_acc_rate, 'test ones num':test_ones_num\n",
    "       }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(saveR)\n",
    "print(saveR)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            ori_word\n",
      "ID                                                  \n",
      "1               (DeleteFile, DeleteFile, DeleteFile)\n",
      "2                     (CopyFile, CopyFile, CopyFile)\n",
      "3               (RegOpenKey, RegOpenKey, RegOpenKey)\n",
      "4            (CloseHandle, CloseHandle, CloseHandle)\n",
      "5           (RegOpenKey, RegQueryValue, RegCloseKey)\n",
      "...                                              ...\n",
      "1373          (CloseHandle, RegSetValue, CreateFile)\n",
      "1374     (HttpSendRequest, CloseHandle, LoadLibrary)\n",
      "1375    (LoadLibrary, InternetOpen, InternetConnect)\n",
      "1376       (RegQueryValue, LoadLibrary, CloseHandle)\n",
      "1377  (LoadLibrary, CreateRemoteThread, CloseHandle)\n",
      "\n",
      "[1377 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "labelID_dict = pickle.load(open(\"./data/Hooklog/trace_picked_fam/fam_dict_enc.pkl\",\"rb\"))\n",
    "IDlabel_dict = {v: k for k, v in labelID_dict.items()}\n",
    "word_index = pickle.load(open(\"./data/Hooklog/trace_picked_fam/tri_dict_enc.pkl\",\"rb\"))\n",
    "word_idx_pd = pd.DataFrame(data=word_index,index=[\"ID\"]).T\n",
    "word_idx_pd['ori_word'] = word_idx_pd.index \n",
    "word_idx_pd = word_idx_pd.set_index([\"ID\"])\n",
    "print(word_idx_pd)\n",
    "\n",
    "pred_label = list(np.argmax(pred_label,axis=-1))\n",
    "def getKeysByValues(listOfValues,dfOfElements=word_idx_pd):\n",
    "    ori_word_li = []\n",
    "    for item in listOfValues:\n",
    "        key = item\n",
    "#         print(key)\n",
    "        if key == 0:\n",
    "            continue\n",
    "        ori_word = dfOfElements.loc[key][0]\n",
    "        ori_word_li.append(ori_word)\n",
    "    return ori_word_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1383",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1383",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-bcc0e65f2c67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#t = one sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetKeysByValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mword_ori_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_fil_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-3e7ee4253430>\u001b[0m in \u001b[0;36mgetKeysByValues\u001b[0;34m(listOfValues, dfOfElements)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mori_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfOfElements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mori_word_li\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mori_word_li\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no slices here, handle elsewhere\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3735\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3736\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3737\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1383"
     ]
    }
   ],
   "source": [
    "word_fil_text = []\n",
    "word_ori_text = []\n",
    "from tqdm import tqdm\n",
    "for text in tqdm(int_ori_text): #text = one epoch\n",
    "    temp = []\n",
    "    for t in text: #t = one sample\n",
    "        temp.append(getKeysByValues(list(t)))\n",
    "    word_ori_text.append(temp)\n",
    "for text in tqdm(int_fil_text):\n",
    "    temp = []\n",
    "    for t in text:\n",
    "        temp.append(getKeysByValues(list(t)))\n",
    "    word_fil_text.append(temp)\n",
    "\n",
    "save_path = save_dir + \"original_words.pkl\"\n",
    "pickle.dump(file=open(save_path,'wb'),obj=word_ori_text)\n",
    "save_path = save_dir + \"filterUsed_words.pkl\"\n",
    "pickle.dump(file=open(save_path,'wb'),obj=word_fil_text)\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:11, 174.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).emb.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).rnn1.state_spec\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).bn1.axis\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).bn1.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).bn1.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).bn1.moving_mean\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).bn1.moving_variance\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).rnn1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).rnn1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).rnn1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).fil.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).fil.layer.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "./results/Hooklog/2019122301/caseStudy_result.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch#</th>\n",
       "      <th>text_id</th>\n",
       "      <th>ori_text</th>\n",
       "      <th>filter_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>('RegOpenKey', 'LoadLibrary', 'LoadLibrary') (...</td>\n",
       "      <td>('RegOpenKey', 'LoadLibrary', 'LoadLibrary') (...</td>\n",
       "      <td>domaiq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...</td>\n",
       "      <td>installcore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...</td>\n",
       "      <td>browsefox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('RegOpenKey', 'LoadLibrary', 'LoadLibrary') (...</td>\n",
       "      <td>('RegOpenKey', 'LoadLibrary', 'LoadLibrary') (...</td>\n",
       "      <td>domaiq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...</td>\n",
       "      <td>installcore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...</td>\n",
       "      <td>installcore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>1998</td>\n",
       "      <td>2</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'RegOpenKey') (...</td>\n",
       "      <td>browsefox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>1999</td>\n",
       "      <td>0</td>\n",
       "      <td>('RegOpenKey', 'LoadLibrary', 'LoadLibrary') (...</td>\n",
       "      <td>('RegOpenKey', 'LoadLibrary', 'LoadLibrary') (...</td>\n",
       "      <td>domaiq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...</td>\n",
       "      <td>installcore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>1999</td>\n",
       "      <td>2</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...</td>\n",
       "      <td>('LoadLibrary', 'LoadLibrary', 'RegOpenKey') (...</td>\n",
       "      <td>browsefox</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch# text_id                                           ori_text  \\\n",
       "0         0       0  ('RegOpenKey', 'LoadLibrary', 'LoadLibrary') (...   \n",
       "1         0       1  ('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...   \n",
       "2         0       2  ('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...   \n",
       "3         1       0  ('RegOpenKey', 'LoadLibrary', 'LoadLibrary') (...   \n",
       "4         1       1  ('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...   \n",
       "...     ...     ...                                                ...   \n",
       "5995   1998       1  ('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...   \n",
       "5996   1998       2  ('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...   \n",
       "5997   1999       0  ('RegOpenKey', 'LoadLibrary', 'LoadLibrary') (...   \n",
       "5998   1999       1  ('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...   \n",
       "5999   1999       2  ('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...   \n",
       "\n",
       "                                            filter_text        label  \n",
       "0     ('RegOpenKey', 'LoadLibrary', 'LoadLibrary') (...       domaiq  \n",
       "1     ('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...  installcore  \n",
       "2     ('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...    browsefox  \n",
       "3     ('RegOpenKey', 'LoadLibrary', 'LoadLibrary') (...       domaiq  \n",
       "4     ('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...  installcore  \n",
       "...                                                 ...          ...  \n",
       "5995  ('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...  installcore  \n",
       "5996  ('LoadLibrary', 'LoadLibrary', 'RegOpenKey') (...    browsefox  \n",
       "5997  ('RegOpenKey', 'LoadLibrary', 'LoadLibrary') (...       domaiq  \n",
       "5998  ('LoadLibrary', 'LoadLibrary', 'LoadLibrary') ...  installcore  \n",
       "5999  ('LoadLibrary', 'LoadLibrary', 'RegOpenKey') (...    browsefox  \n",
       "\n",
       "[6000 rows x 5 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = ['epoch#','text_id','ori_text','filter_text','label','predict']\n",
    "df = pd.DataFrame(columns=col)\n",
    "for epoch,(text_ori,text_fil,labelID,predID) in tqdm(enumerate(zip(word_ori_text,word_fil_text,int_label,pred_label))):\n",
    "    for idx,(t_o,t_f,label,pred) in enumerate(zip(text_ori,text_fil,labelID,predID)):\n",
    "        temp = pd.Series([epoch,idx,\" \".join([str(x) for x in t_o]),\" \".join([str(x) for x in t_f]),IDlabel_dict[label],IDlabel_dict[pred]],col)\n",
    "        df = df.append(temp,ignore_index=True)\n",
    "\n",
    "save_path = save_dir + \"caseStudy_result.xlsx\"\n",
    "df.to_excel(save_path,index=False)\n",
    "print(save_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
