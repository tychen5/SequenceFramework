{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, csv\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import xlsxwriter\n",
    "import random\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import unicodedata as udata\n",
    "# import pause, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from distutils.dir_util import copy_tree\n",
    "import sklearn\n",
    "from sklearn.metrics import *\n",
    "import itertools as it\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import functools\n",
    "import spacy\n",
    "\n",
    "import tensorflow.keras.preprocessing.text as T\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_1 = (stopwords.words('english')) #Stopword\n",
    "with open('data/preprocess/stop_words.txt') as f:\n",
    "    stop_words_2 = f.read().splitlines() #stop_list1\n",
    "stop_words_3 = pickle.load(open('data/preprocess/stop_list2.pkl','rb'))\n",
    "stop_words_all = set(stop_words_1 + stop_words_2 + stop_words_3)\n",
    "len(stop_words_all) # stopwords#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196009"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenizer = spacy.load(\"en\")\n",
    "dil= r\"[!\\\"#$%&()*+,\\-\\./:;<\\=>?\\@[\\\\]\\^_`{|}~\\º]+\\ *\" \n",
    "dil_filter = 'ºÑ!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_\\`{|}~\\t\\n'\n",
    "glove = pd.read_table('./data/preprocess/glove.840B.300d.txt', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "glove_words = set(glove.index.tolist())\n",
    "len(glove_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprcess(text,tokenizers='nltk'):\n",
    "    '''\n",
    "    Input: string\n",
    "    Return: preprocessed list\n",
    "    '''\n",
    "    tokens = [i for i in text.split() if i not in stop_words_all]\n",
    "    tokens_str = ' '.join(tokens)\n",
    "    if tokenizers == 'keras':\n",
    "        tokens = T.text_to_word_sequence(tokens_str,filters=dil_filter) # lower casse & tokenize\n",
    "        tokens = list(filter(None, tokens))\n",
    "    elif tokenizers == 'nltk':\n",
    "#         tokens = re.sub(dil,\" \",tokens_str.lower())\n",
    "        tokens = [i for i in tokens_str.lower() if i not in dil_filter]\n",
    "        tokens = ''.join(tokens)\n",
    "        tokens = word_tokenize(tokens)\n",
    "        tokens = list(filter(None, tokens))\n",
    "#         tokens = [x.lower() for x in tokens]\n",
    "    elif tokenizers == 'spacy':\n",
    "#         tokens = re.sub(dil,\" \",tokens_str.lower())\n",
    "        tokens = [i for i in tokens_str.lower() if i not in dil_filter]\n",
    "        tokens = ''.join(tokens)\n",
    "        tokens = sp_tokenizer(tokens)\n",
    "        token_li = []\n",
    "        for token in tokens:\n",
    "            token_li.append(token.text)\n",
    "        tokens = list(filter(None, token_li))\n",
    "#         tokens = [x.lower() for x in token_li]\n",
    "    tokens_li = []\n",
    "    for token in tokens:\n",
    "        tokens_li.append(''.join([i for i in token if not i.isdigit()]))\n",
    "    tokens_str = ' '.join(tokens_li)\n",
    "    tokens_str = re.sub(dil,\" \",tokens_str.lower())\n",
    "    tokens_li = tokens_str.split()\n",
    "    tokens_li = list(filter(None, tokens_li))\n",
    "    ori_tokens_num = len(tokens_li)\n",
    "    tokens_li = [i for i in tokens_li if i in glove_words]\n",
    "    try:\n",
    "        rate = len(tokens_li)/ori_tokens_num\n",
    "    except ZeroDivisionError:\n",
    "        rate = np.nan\n",
    "    return tokens_li , rate#ori_tokens_num/len(tokens_li)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:02<03:16, 13.08s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e45cfcfee62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mnews_text_tok_keras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrate_keras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_preprcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mnews_text_tok_nltk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrate_nltk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_preprcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nltk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mnews_text_tok_spacy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrate_spacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_preprcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spacy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         train_df.loc[count] = [id_num,cat,news_text_ori,news_text_tok_num_ori,\n\u001b[1;32m     25\u001b[0m                                news_text_tok_keras,rate_keras,news_text_tok_nltk,rate_nltk,news_text_tok_spacy,rate_spacy]\n",
      "\u001b[0;32m<ipython-input-4-2e203f354942>\u001b[0m in \u001b[0;36mtext_preprcess\u001b[0;34m(text, tokenizers)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtoken_li\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtoken_li\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_li\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#         tokens = [x.lower() for x in token_li]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_df = pd.DataFrame(columns=['id','category','text','original_tokenize_num','preprocess_text','preprocss_tokenize_num'])\n",
    "train_df = pd.DataFrame(columns=['id','category','text','original_tokenize_num',\n",
    "                                 'text_keras','rate_keras','text_nltk','rate_nltk','text_apacy','rate_spacy'])\n",
    "train_dir = './data/20news-bydate-v3/20news-bydate-train/'\n",
    "cat_dir = next(os.walk(train_dir))[1]\n",
    "count = 0\n",
    "for cat in tqdm(cat_dir):\n",
    "    in_dir = train_dir + cat + '/'\n",
    "    news_list = next(os.walk(in_dir))[2]\n",
    "    news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "    news_list = list(filter(lambda f: f.endswith(\".txt\"), news_list))\n",
    "    for news_path in news_list:\n",
    "        id_num = news_path.split('/')[-1].split('.')[0]\n",
    "        with open(news_path,'r',encoding='latin1') as f:\n",
    "            news_text = f.read()\n",
    "            news_text = re.sub('\\n',' ',news_text)\n",
    "            news_text = re.sub('\\t',' ',news_text)\n",
    "            news_text_tok_ori = T.text_to_word_sequence(news_text,filters='',lower=False)\n",
    "            news_text_ori = \" \".join(news_text_tok_ori)\n",
    "            news_text_tok_num_ori = len(news_text_tok_ori)\n",
    "            news_text_tok_keras,rate_keras = text_preprcess(news_text,tokenizers='keras')\n",
    "            news_text_tok_nltk,rate_nltk = text_preprcess(news_text,tokenizers='nltk')\n",
    "            news_text_tok_spacy,rate_spacy = text_preprcess(news_text,tokenizers='spacy')\n",
    "        train_df.loc[count] = [id_num,cat,news_text_ori,news_text_tok_num_ori,\n",
    "                               news_text_tok_keras,rate_keras,news_text_tok_nltk,rate_nltk,news_text_tok_spacy,rate_spacy]\n",
    "        count+=1\n",
    "#         break\n",
    "    \n",
    "# train_df.loc[0] = [0,0,0]\n",
    "print(train_df.rate_keras.mean() , train_df.rate_nltk.mean(),train_df.rate_spacy.mean()  )\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* keras tokenizer: 95.95% tokens in glove\n",
    "* nltk tokenizer: 95.96% tokens in glove\n",
    "* spacy tokenizer: 96.05% tokens in glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(columns=['id','category','text','original_tokenize_num','preprocess_text','preprocss_tokenize_num'])\n",
    "train_dir = './data/20news-bydate-v3/20news-bydate-train/'\n",
    "cat_dir = next(os.walk(train_dir))[1]\n",
    "count = 0\n",
    "for cat in tqdm(cat_dir):\n",
    "    in_dir = train_dir + cat + '/'\n",
    "    news_list = next(os.walk(in_dir))[2]\n",
    "    news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "    news_list = list(filter(lambda f: f.endswith(\".txt\"), news_list))\n",
    "    for news_path in news_list:\n",
    "        id_num = news_path.split('/')[-1].split('.')[0]\n",
    "        with open(news_path,'r',encoding='latin1') as f:\n",
    "            news_text = f.read()\n",
    "            news_text = re.sub('\\n',' ',news_text)\n",
    "            news_text_tok_ori = T.text_to_word_sequence(news_text,filters='',lower=False)\n",
    "            news_text_ori = \" \".join(news_text_tok_ori)\n",
    "            news_text_tok_num_ori = len(news_text_tok_ori)\n",
    "#             news_text_tok_pre,rate_keras = text_preprcess(news_text,tokenizers='keras')\n",
    "#             news_text_tok_pre,rate_nltk = text_preprcess(news_text,tokenizers='nltk')\n",
    "            news_text_tok_pre,rate_spacy = text_preprcess(news_text,tokenizers='spacy')\n",
    "            news_text_pre = \" \".join(news_text_tok_pre)\n",
    "        train_df.loc[count] = [id_num,cat,news_text_ori,news_text_tok_num_ori,news_text_pre,len(news_text_tok_pre)]\n",
    "        count+=1\n",
    "        break\n",
    "    \n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprcess2(text):\n",
    "    '''\n",
    "    Goal: filter out punctuations、words not in GloVe\n",
    "    * cased\n",
    "    * spacy\n",
    "    * for RoBERTa + GloVe embedding\n",
    "    Input: text string\n",
    "    Return: preprcessed text string\n",
    "    '''\n",
    "    tokens = [i for i in text if i not in dil_filter] #濾掉符號\n",
    "    tokens = ''.join(tokens)\n",
    "    tokens = sp_tokenizer(tokens)\n",
    "    token_li = []\n",
    "    for token in tokens:\n",
    "        token_li.append(token.text)\n",
    "    tokens_li = [i for i in token_li if i in glove_words] #只留有在glove的字\n",
    "    tokens = list(filter(None, tokens_li))\n",
    "    tokens_str = ' '.join(tokens)\n",
    "    tokens_str = re.sub(dil,\" \",tokens_str) #在確認濾一次符號\n",
    "    return tokens_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/leoqaz12/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/leoqaz12/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "| dictionary: 50264 types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (decoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerSentenceEncoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\n",
    "roberta.cuda() #gpu\n",
    "# roberta.cpu()\n",
    "roberta.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 18857  =====\n",
      "==ERR== 19309  =====\n",
      "==ERR== 19148  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:31<09:56, 31.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 21139  =====\n",
      "==ERR== 21056  =====\n",
      "==ERR== 21194  =====\n",
      "==ERR== 21159  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:59<09:05, 30.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 21844  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [01:30<08:39, 30.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 22811  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [02:00<08:08, 30.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 23437  =====\n",
      "==ERR== 23490  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [02:42<08:27, 33.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 19698  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [03:54<07:37, 35.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 27445  =====\n",
      "==ERR== 27125  =====\n",
      "==ERR== 27502  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [04:21<06:34, 32.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 25988  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [04:48<05:41, 31.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 28469  =====\n",
      "==ERR== 28384  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [05:16<05:01, 30.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 25096  =====\n",
      "==ERR== 25469  =====\n",
      "==ERR== 25237  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [05:42<04:18, 28.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 17597  =====\n",
      "==ERR== 17601  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [06:08<03:43, 27.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 25604  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [06:29<03:01, 25.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 25564  =====\n",
      "==ERR== 24144  =====\n",
      "==ERR== 23980  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [07:03<02:49, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 26597  =====\n",
      "==ERR== 26784  =====\n",
      "==ERR== 26706  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [07:28<02:17, 27.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 18716  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [08:08<02:04, 31.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 20301  =====\n",
      "==ERR== 20416  =====\n",
      "==ERR== 20438  =====\n",
      "==ERR== 20199  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [08:42<01:36, 32.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 24750  =====\n",
      "==ERR== 24546  =====\n",
      "==ERR== 24744  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [09:11<01:02, 31.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ERR== 28128  =====\n",
      "==ERR== 28005  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:04<00:00, 28.83s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>tokens_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19305</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>My votes FWIW Team MVP Pat Verbeek He fans on ...</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19313</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Kovalev is too talented a player to play for R...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19122</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>New Jersey 1 0 23 Pittsburgh 2 3 16 First peri...</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19312</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>First of all the Penguins WILL win the cup aga...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>19115</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>You ca n't But good luck trying</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>20797</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>When the object of their belief is said to be ...</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>20537</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Koff You mean that as long as I put you to sle...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>20757</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Mr Connor 's assertion that more complex later...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>20677</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>excess stuff deleted I know of a similar incid...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>20719</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>The System refered to a moral system You shown...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id          category  \\\n",
       "0      19305  rec.sport.hockey   \n",
       "1      19313  rec.sport.hockey   \n",
       "2      19122  rec.sport.hockey   \n",
       "3      19312  rec.sport.hockey   \n",
       "4      19115  rec.sport.hockey   \n",
       "...      ...               ...   \n",
       "11309  20797       alt.atheism   \n",
       "11310  20537       alt.atheism   \n",
       "11311  20757       alt.atheism   \n",
       "11312  20677       alt.atheism   \n",
       "11313  20719       alt.atheism   \n",
       "\n",
       "                                         preprocess_text tokens_num  \n",
       "0      My votes FWIW Team MVP Pat Verbeek He fans on ...        514  \n",
       "1      Kovalev is too talented a player to play for R...        165  \n",
       "2      New Jersey 1 0 23 Pittsburgh 2 3 16 First peri...       1116  \n",
       "3      First of all the Penguins WILL win the cup aga...         68  \n",
       "4                        You ca n't But good luck trying          9  \n",
       "...                                                  ...        ...  \n",
       "11309  When the object of their belief is said to be ...        273  \n",
       "11310  Koff You mean that as long as I put you to sle...         25  \n",
       "11311  Mr Connor 's assertion that more complex later...        131  \n",
       "11312  excess stuff deleted I know of a similar incid...        142  \n",
       "11313  The System refered to a moral system You shown...         38  \n",
       "\n",
       "[11314 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(columns=['id','category','preprocess_text','tokens_num'])#,'preprocess_text','preprocss_tokenize_num'])\n",
    "train_dir = './data/20news-bydate-v3/20news-bydate-train/'\n",
    "cat_dir = next(os.walk(train_dir))[1]\n",
    "count = 0\n",
    "for cat in tqdm(cat_dir):\n",
    "    in_dir = train_dir + cat + '/'\n",
    "    news_list = next(os.walk(in_dir))[2]\n",
    "    news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "    news_list = list(filter(lambda f: f.endswith(\".txt\"), news_list))\n",
    "    for news_path in news_list:\n",
    "        id_num = news_path.split('/')[-1].split('.')[0]\n",
    "        with open(news_path,'r',encoding='latin1') as f:\n",
    "            news_text = f.read()\n",
    "            news_text = re.sub('\\n',' ',news_text)\n",
    "            news_text = re.sub('\\t',' ',news_text)\n",
    "            news_text_pre = text_preprcess2(news_text)\n",
    "        try:\n",
    "            doc = roberta.extract_features_aligned_to_words(news_text_pre)\n",
    "            tokens_num = len(doc)\n",
    "        except AssertionError:\n",
    "            tokens_num = np.nan\n",
    "        except Exception as e:\n",
    "            tokens_num_ = str(e)\n",
    "            try:\n",
    "                tokens_num = int(tokens_num_.split(':')[-1].split()[0])\n",
    "            except IndexError:\n",
    "                tokens_num = 0\n",
    "                print('==ERR==',id_num,tokens_num_,'=====')\n",
    "        train_df.loc[count] = [int(id_num),cat,news_text_pre,tokens_num]\n",
    "        count+=1        \n",
    "# GPU: 10:06\n",
    "# CPU: 75\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>tokens_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4948</td>\n",
       "      <td>26339</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>For your information Lankford is injured I thi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5099</td>\n",
       "      <td>26088</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>Mr Hernandez Ñ I apologize for the misundersta...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7090</td>\n",
       "      <td>25868</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>515 thus fitting in neatly with something else...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8440</td>\n",
       "      <td>26596</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>Send follow ups to Sorry about the header but ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id               category  \\\n",
       "4948  26339     rec.sport.baseball   \n",
       "5099  26088     rec.sport.baseball   \n",
       "7090  25868     talk.religion.misc   \n",
       "8440  26596  comp.sys.mac.hardware   \n",
       "\n",
       "                                        preprocess_text tokens_num  \n",
       "4948  For your information Lankford is injured I thi...        NaN  \n",
       "5099  Mr Hernandez Ñ I apologize for the misundersta...        NaN  \n",
       "7090  515 thus fitting in neatly with something else...        NaN  \n",
       "8440  Send follow ups to Sorry about the header but ...        NaN  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['tokens_num'].isnull()] #assertion err\n",
    "# train_df[train_df['id'] == 26339]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>tokens_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>18857</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>19309</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>19148</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>21139</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>21056</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>853</td>\n",
       "      <td>21194</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>881</td>\n",
       "      <td>21159</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>21844</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1761</td>\n",
       "      <td>22811</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2433</td>\n",
       "      <td>23437</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>23490</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3225</td>\n",
       "      <td>19698</td>\n",
       "      <td>sci.space</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3959</td>\n",
       "      <td>27445</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4215</td>\n",
       "      <td>27125</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4366</td>\n",
       "      <td>27502</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5022</td>\n",
       "      <td>25988</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5276</td>\n",
       "      <td>28469</td>\n",
       "      <td>sci.electronics</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5583</td>\n",
       "      <td>28384</td>\n",
       "      <td>sci.electronics</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5758</td>\n",
       "      <td>25096</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5991</td>\n",
       "      <td>25469</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6134</td>\n",
       "      <td>25237</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>17597</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6849</td>\n",
       "      <td>17601</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7122</td>\n",
       "      <td>25604</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7283</td>\n",
       "      <td>25564</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7318</td>\n",
       "      <td>24144</td>\n",
       "      <td>sci.med</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7681</td>\n",
       "      <td>23980</td>\n",
       "      <td>sci.med</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7879</td>\n",
       "      <td>26597</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8067</td>\n",
       "      <td>26784</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8070</td>\n",
       "      <td>26706</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8756</td>\n",
       "      <td>18716</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9127</td>\n",
       "      <td>20301</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9448</td>\n",
       "      <td>20416</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9454</td>\n",
       "      <td>20438</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9470</td>\n",
       "      <td>20199</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9774</td>\n",
       "      <td>24750</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9791</td>\n",
       "      <td>24546</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9902</td>\n",
       "      <td>24744</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10331</td>\n",
       "      <td>28128</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10735</td>\n",
       "      <td>28005</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                  category preprocess_text tokens_num\n",
       "150    18857          rec.sport.hockey                          0\n",
       "252    19309          rec.sport.hockey                          0\n",
       "318    19148          rec.sport.hockey                          0\n",
       "734    21139             comp.graphics                          0\n",
       "757    21056             comp.graphics                          0\n",
       "853    21194             comp.graphics                          0\n",
       "881    21159             comp.graphics                          0\n",
       "1222   21844        talk.politics.guns                          0\n",
       "1761   22811        talk.politics.misc                          0\n",
       "2433   23437     talk.politics.mideast                          0\n",
       "2500   23490     talk.politics.mideast                          0\n",
       "3225   19698                 sci.space                          0\n",
       "3959   27445  comp.sys.ibm.pc.hardware                          0\n",
       "4215   27125  comp.sys.ibm.pc.hardware                          0\n",
       "4366   27502  comp.sys.ibm.pc.hardware                          0\n",
       "5022   25988        rec.sport.baseball                          0\n",
       "5276   28469           sci.electronics                          0\n",
       "5583   28384           sci.electronics                          0\n",
       "5758   25096              misc.forsale                          0\n",
       "5991   25469              misc.forsale                          0\n",
       "6134   25237              misc.forsale                          0\n",
       "6700   17597           rec.motorcycles                          0\n",
       "6849   17601           rec.motorcycles                          0\n",
       "7122   25604        talk.religion.misc                          0\n",
       "7283   25564        talk.religion.misc                          0\n",
       "7318   24144                   sci.med                          0\n",
       "7681   23980                   sci.med                          0\n",
       "7879   26597     comp.sys.mac.hardware                          0\n",
       "8067   26784     comp.sys.mac.hardware                          0\n",
       "8070   26706     comp.sys.mac.hardware                          0\n",
       "8756   18716    soc.religion.christian                          0\n",
       "9127   20301            comp.windows.x                          0\n",
       "9448   20416            comp.windows.x                          0\n",
       "9454   20438            comp.windows.x                          0\n",
       "9470   20199            comp.windows.x                          0\n",
       "9774   24750   comp.os.ms-windows.misc                          0\n",
       "9791   24546   comp.os.ms-windows.misc                          0\n",
       "9902   24744   comp.os.ms-windows.misc                          0\n",
       "10331  28128                 rec.autos                          0\n",
       "10735  28005                 rec.autos                          0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['tokens_num'] == 0] #no text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(all_length):\n",
    "    '''\n",
    "    input: length list of elements e.g.[1,1,1,3,5,9,4,2,1,3,54,78,5...]\n",
    "    output1: mean、std、mode、min、q1、median(q2)、q3、max、iqr、outlier、far out\n",
    "    output2: statistics graph、10%~90% form\n",
    "    '''\n",
    "    stat_dict = {}\n",
    "    stat_dict['mean'] = np.mean(all_length)\n",
    "    stat_dict['std'] = np.std(all_length)\n",
    "    stat_dict['mode'] = np.argmax(np.bincount(all_length))\n",
    "    stat_dict['min'] = np.min(all_length)\n",
    "    stat_dict['q1'] = np.quantile(all_length,0.25)\n",
    "    stat_dict['median'] = np.quantile(all_length,0.5)\n",
    "    stat_dict['q3'] = np.quantile(all_length,0.75)\n",
    "    stat_dict['max'] = np.max(all_length)\n",
    "    stat_dict['iqr'] = stat_dict['q3'] - stat_dict['q1']\n",
    "    stat_dict['outlier'] = stat_dict['q3'] + 1.5*stat_dict['iqr']\n",
    "    stat_dict['far_out'] = stat_dict['q3'] + 3*stat_dict['iqr']\n",
    "    for i in [10,20,30,40,50,60,70,80,90,100]:\n",
    "        stat_dict[str(i)+'%'] = np.percentile(all_length,i)\n",
    "    return pd.DataFrame.from_dict(stat_dict,orient='index',columns=['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>190.810168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>544.026512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mode</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q1</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>median</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q3</td>\n",
       "      <td>166.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>12700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iqr</td>\n",
       "      <td>123.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>outlier</td>\n",
       "      <td>350.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>far_out</td>\n",
       "      <td>535.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10%</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20%</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30%</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40%</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60%</td>\n",
       "      <td>109.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70%</td>\n",
       "      <td>143.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80%</td>\n",
       "      <td>199.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90%</td>\n",
       "      <td>330.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100%</td>\n",
       "      <td>12700.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               length\n",
       "mean       190.810168\n",
       "std        544.026512\n",
       "mode        41.000000\n",
       "min          0.000000\n",
       "q1          43.000000\n",
       "median      84.000000\n",
       "q3         166.000000\n",
       "max      12700.000000\n",
       "iqr        123.000000\n",
       "outlier    350.500000\n",
       "far_out    535.000000\n",
       "10%         23.000000\n",
       "20%         36.000000\n",
       "30%         50.000000\n",
       "40%         65.000000\n",
       "50%         84.000000\n",
       "60%        109.000000\n",
       "70%        143.000000\n",
       "80%        199.000000\n",
       "90%        330.100000\n",
       "100%     12700.000000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk = train_df.tokens_num.tolist()\n",
    "kk = [x for x in kk if str(x) != 'nan']\n",
    "basic_statistics(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>107.035535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>91.436267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mode</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q1</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>median</td>\n",
       "      <td>78.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q3</td>\n",
       "      <td>144.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>498.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iqr</td>\n",
       "      <td>103.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>outlier</td>\n",
       "      <td>298.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>far_out</td>\n",
       "      <td>453.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10%</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20%</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30%</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40%</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>78.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60%</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70%</td>\n",
       "      <td>126.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80%</td>\n",
       "      <td>166.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90%</td>\n",
       "      <td>239.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100%</td>\n",
       "      <td>498.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "mean     107.035535\n",
       "std       91.436267\n",
       "mode      41.000000\n",
       "min        3.000000\n",
       "q1        41.000000\n",
       "median    78.000000\n",
       "q3       144.000000\n",
       "max      498.000000\n",
       "iqr      103.000000\n",
       "outlier  298.500000\n",
       "far_out  453.000000\n",
       "10%       22.000000\n",
       "20%       35.000000\n",
       "30%       47.000000\n",
       "40%       61.000000\n",
       "50%       78.000000\n",
       "60%       99.000000\n",
       "70%      126.000000\n",
       "80%      166.000000\n",
       "90%      239.000000\n",
       "100%     498.000000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#只留下>=3 tokens & <=512 tokens的文章\n",
    "train_df_select = train_df[(train_df['tokens_num'] >=3) & (train_df['tokens_num']<=512)]\n",
    "basic_statistics(train_df_select.tokens_num.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-f66bce8866c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features_aligned_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'26339'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/roberta/hub_interface.py\u001b[0m in \u001b[0;36mextract_features_aligned_to_words\u001b[0;34m(self, sentence, return_all_hiddens)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mspacy_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mspacy_toks_ws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_with_ws\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0malignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignment_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_bpe_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbpe_toks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy_toks_ws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# extract features and align them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/roberta/alignment_utils.py\u001b[0m in \u001b[0;36malign_bpe_to_words\u001b[0;34m(roberta, bpe_tokens, other_tokens)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mbpe_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'<s>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mbpe_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpe_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# create alignment from every word to a list of BPE tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "roberta.extract_features_aligned_to_words(train_df[train_df['id'] == '26339'].preprocess_text.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>tokens_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, category, preprocess_text, tokens_num]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['tokens_num'].apply(lambda x: type(x)==str)]\n",
    "# train_df[type(train_df.tokens_num) is str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(25) is int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(series):\n",
    "    if type(series) is not int:\n",
    "        try:\n",
    "            return int(series.split(':')[-1].split()[0])\n",
    "        except:\n",
    "            print(series,type(series))\n",
    "            return series\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n",
      " <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "train_df['tokens_num'] = train_df.tokens_num.apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My votes FWIW Team MVP Pat Verbeek He fans on 25 of goal mouth feeds but he still has 36 goals after a terrible start and has been an examplary sp team captain throughout a tough couple of seasons Honorable mention Nick Kypreos and Mark Janssens Probably more appropriate in the unsung heroes category than MVP but Kypreos 17 goals 320 PIM has been the hardest working player on the team and Janssens is underrated as a defensive center and checker I guess I place a greater emphasis on hard work than skill when determining value Biggest surprise Geoff Sanderson He had 13 goals and 31 points last season as a center then moved to left wing and has so far put up 45 goals and 80 points He now has a new Whaler record 21 power play goals most all coming from the right wing faceoff circle his garden spot Honorable mention Andrew Cassels and Terry Yake The kiddie quartet of Sanderson Poulin Nylander and Petrovicky have been attracting the most attention but Cassels is just 23 and will score close to 90 points this season He has quite nicely assumed the role of number one center on the team and works very well with Sanderson Yake bounced around the minors for a number of seasons but is still 24 and will put up about 20 goals and 50 points this season Yake like Sanderson started performing better offensively once he was converted from center to wing although lefty Sanderson went to the left wing and righty Yake went to the right side Biggest disappointment Hands down John Cullen Cullen had a disasterous 77 point season last year his first full season after The Trade Cullen started the season off of summer back surgery and fell flat on his face appropriate since he spent all of his Whaler career flat on his ass and whining about it Cullen scored just 9 point on 19 games was a clubhouse malcontent commanded the powerplay to a 9 success percentage 21 with Sanderson and sulked his way out of town Worst of all his 4 year 4 M contract had three years left to run so no one would give up any more than the 2nd round draft pick the Maple Leafs offered to Hartford Honorable mention Steve also subpar after signing a 3 year 21 M contract Eric Weinrich who showed flashes of competence but overall has played poorly Jim McKenzie who was a much better hockey player two seasons ago than he is now and Frank Pietrangelo who only seemed to play well when Sean Burke was out for an extended period and he got to make a number of starts in a row'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.preprocess_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2550'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ValueError: tokens exceeds maximum length: 2550 > 512'.split(':')[-1].split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(all_length):\n",
    "    '''\n",
    "    input: length list of elements e.g.[1,1,1,3,5,9,4,2,1,3,54,78,5...]\n",
    "    output1: mean、std、mode、min、q1、median(q2)、q3、max、iqr、outlier、far out\n",
    "    output2: statistics graph、10%~90% form\n",
    "    '''\n",
    "    stat_dict = {}\n",
    "    stat_dict['mean'] = np.mean(all_length)\n",
    "    stat_dict['std'] = np.std(all_length)\n",
    "    stat_dict['mode'] = np.argmax(np.bincount(all_length))\n",
    "    stat_dict['min'] = np.min(all_length)\n",
    "    stat_dict['q1'] = np.quantile(all_length,0.25)\n",
    "    stat_dict['median'] = np.quantile(all_length,0.5)\n",
    "    stat_dict['q3'] = np.quantile(all_length,0.75)\n",
    "    stat_dict['max'] = np.max(all_length)\n",
    "    stat_dict['iqr'] = stat_dict['q3'] - stat_dict['q1']\n",
    "    stat_dict['outlier'] = stat_dict['q3'] + 1.5*stat_dict['iqr']\n",
    "    stat_dict['far_out'] = stat_dict['q3'] + 3*stat_dict['iqr']\n",
    "    for i in [10,20,30,40,50,60,70,80,90,100]:\n",
    "        stat_dict[str(i)+'%'] = np.percentile(all_length,i)\n",
    "    return pd.DataFrame.from_dict(stat_dict,orient='index',columns=['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>144.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>214.881008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mode</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q1</td>\n",
       "      <td>23.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>median</td>\n",
       "      <td>63.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q3</td>\n",
       "      <td>157.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>912.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iqr</td>\n",
       "      <td>133.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>outlier</td>\n",
       "      <td>358.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>far_out</td>\n",
       "      <td>558.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10%</td>\n",
       "      <td>15.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20%</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30%</td>\n",
       "      <td>25.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40%</td>\n",
       "      <td>37.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>63.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60%</td>\n",
       "      <td>77.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70%</td>\n",
       "      <td>112.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80%</td>\n",
       "      <td>189.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90%</td>\n",
       "      <td>379.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100%</td>\n",
       "      <td>912.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "mean     144.450000\n",
       "std      214.881008\n",
       "mode      23.000000\n",
       "min        0.000000\n",
       "q1        23.750000\n",
       "median    63.500000\n",
       "q3       157.500000\n",
       "max      912.000000\n",
       "iqr      133.750000\n",
       "outlier  358.125000\n",
       "far_out  558.750000\n",
       "10%       15.600000\n",
       "20%       23.000000\n",
       "30%       25.400000\n",
       "40%       37.400000\n",
       "50%       63.500000\n",
       "60%       77.600000\n",
       "70%      112.200000\n",
       "80%      189.800000\n",
       "90%      379.300000\n",
       "100%     912.000000"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_num_li = train_df.preprocss_tokenize_num.tolist()\n",
    "basic_statistics(tokens_num_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just',\n",
       " 'what',\n",
       " 'do',\n",
       " 'gay',\n",
       " 'people',\n",
       " 'do',\n",
       " 'that',\n",
       " 'straight',\n",
       " 'people',\n",
       " \"don't?\",\n",
       " 'absolutely',\n",
       " 'nothing.',\n",
       " \"i'm\",\n",
       " 'a',\n",
       " 'very',\n",
       " 'straight(as',\n",
       " 'an',\n",
       " 'arrow),',\n",
       " '17-year',\n",
       " 'old',\n",
       " 'male',\n",
       " 'that',\n",
       " 'is',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bsa.',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'care',\n",
       " 'what',\n",
       " 'gay',\n",
       " 'people',\n",
       " 'do',\n",
       " 'among',\n",
       " 'each',\n",
       " 'other,',\n",
       " 'as',\n",
       " 'long',\n",
       " 'as',\n",
       " 'they',\n",
       " \"don't\",\n",
       " 'make',\n",
       " 'passes',\n",
       " 'at',\n",
       " 'me',\n",
       " 'or',\n",
       " 'anything.',\n",
       " 'at',\n",
       " 'my',\n",
       " 'summer',\n",
       " 'camp',\n",
       " 'where',\n",
       " 'i',\n",
       " 'work,',\n",
       " 'my',\n",
       " 'boss',\n",
       " 'is',\n",
       " 'gay.',\n",
       " 'not',\n",
       " 'in',\n",
       " 'a',\n",
       " \"'pansy'\",\n",
       " 'way',\n",
       " 'of',\n",
       " 'gay',\n",
       " '(i',\n",
       " 'know',\n",
       " 'a',\n",
       " 'few),',\n",
       " 'but',\n",
       " 'just',\n",
       " \"'one\",\n",
       " 'of',\n",
       " 'the',\n",
       " \"guys'.\",\n",
       " 'he',\n",
       " \"doesn't\",\n",
       " 'push',\n",
       " 'anything',\n",
       " 'on',\n",
       " 'me,',\n",
       " 'and',\n",
       " 'we',\n",
       " 'give',\n",
       " 'him',\n",
       " 'the',\n",
       " 'same',\n",
       " 'respect',\n",
       " 'back,',\n",
       " 'due',\n",
       " 'to',\n",
       " 'his',\n",
       " 'position.',\n",
       " 'if',\n",
       " 'anything,',\n",
       " 'the',\n",
       " 'bsa',\n",
       " 'has',\n",
       " 'taught',\n",
       " 'me,',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know,',\n",
       " 'tolerance',\n",
       " 'or',\n",
       " 'something.',\n",
       " 'before',\n",
       " 'i',\n",
       " 'met',\n",
       " 'this',\n",
       " 'guy,',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'all',\n",
       " 'gays',\n",
       " 'were',\n",
       " \"'faries'.\",\n",
       " 'so,',\n",
       " 'the',\n",
       " 'bsa',\n",
       " 'has',\n",
       " 'taught',\n",
       " 'me',\n",
       " 'to',\n",
       " 'be',\n",
       " 'an',\n",
       " 'antibigot.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/20news-bydate-v3/20news-bydate-train/alt.atheism/20556.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20748.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20810.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20574.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20692.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20705.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20519.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20565.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20835.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20649.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20684.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20872.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20739.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20547.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20578.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20704.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20932.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20528.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20769.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20869.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20549.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20776.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20936.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20980.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20780.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20515.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20575.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20766.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20750.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20562.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20700.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20594.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20972.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20801.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20910.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20756.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20883.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20539.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20687.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20852.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20945.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20734.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20865.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20694.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20897.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20833.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20966.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20808.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20816.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20793.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20893.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20926.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20975.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20914.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20952.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20913.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20879.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20925.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20666.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20958.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20760.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20881.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20640.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20834.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20600.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20727.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20842.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20768.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20829.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20871.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20741.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20886.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20927.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20652.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20679.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20586.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20527.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20976.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20613.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20794.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20559.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20595.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20959.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20744.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20541.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20717.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20589.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20725.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20526.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20598.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20802.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20674.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20901.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20667.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20538.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20577.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20693.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20853.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20938.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20545.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20823.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20859.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20803.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20921.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20861.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20832.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20622.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20680.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20696.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20988.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20747.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20805.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20627.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20858.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20977.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20686.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20800.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20664.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20792.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20698.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20795.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20770.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20688.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20683.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20807.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20531.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20764.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20553.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20954.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20827.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20767.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20916.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20896.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20782.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20970.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20939.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20955.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20894.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20646.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20951.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20819.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20946.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20979.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20905.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20847.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20864.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20570.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20611.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20870.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20779.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20635.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20900.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20665.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20580.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20670.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20840.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20787.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20962.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20701.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20542.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20956.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20737.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20612.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20743.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20522.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20918.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20618.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20814.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20763.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20851.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20728.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20662.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20745.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20517.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20609.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20676.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20839.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20848.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20891.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20825.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20950.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20758.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20911.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20751.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20902.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20775.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20548.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20535.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20623.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20604.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20762.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20566.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20690.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20711.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20552.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20655.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20581.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20984.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20681.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20607.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20788.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20532.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20774.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20544.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20678.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20812.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20981.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20944.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20536.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20620.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20647.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20785.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20804.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20878.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20843.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20659.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20935.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20672.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20551.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20629.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20703.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20986.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20720.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20621.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20749.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20917.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20746.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20930.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20707.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20880.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20657.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20540.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20671.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20656.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20636.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20815.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20772.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20862.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20963.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20850.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20663.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20571.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20799.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20860.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20929.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20601.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20875.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20854.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20765.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20989.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20637.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20985.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20948.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20730.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20715.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20919.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20863.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20983.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20721.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20754.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20633.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20709.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20759.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20806.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20550.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20777.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20937.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20895.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20661.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20658.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20568.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20898.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20723.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20753.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20731.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20824.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20738.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20923.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20564.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20884.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20726.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20903.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20610.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20855.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20874.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20922.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20521.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20625.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20596.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20836.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20887.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20933.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20818.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20890.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20991.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20628.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20543.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20994.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20591.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20642.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20826.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20588.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20654.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20987.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20846.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20668.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20561.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20587.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20602.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20899.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20811.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20648.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20634.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20892.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20572.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20554.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20724.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20638.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20789.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20632.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20906.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20590.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20877.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20742.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20710.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20631.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20817.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20888.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20830.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20771.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20733.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20518.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20904.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20828.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20982.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20974.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20555.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20920.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20856.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20644.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20606.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20608.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20845.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20524.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20821.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20791.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20953.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20689.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20708.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20626.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20643.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20691.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20786.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20943.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20530.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20809.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20957.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20908.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20978.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20928.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20716.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20961.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20605.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20857.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20889.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20660.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20712.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20567.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20624.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20844.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20702.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20778.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20569.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20560.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20582.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20729.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20682.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20735.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20614.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20573.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20558.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20973.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20713.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20740.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20675.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20736.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20523.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20617.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20706.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20576.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20525.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20968.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20866.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20603.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20534.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20516.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20924.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20971.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20761.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20599.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20650.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20820.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20563.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20990.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20597.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20585.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20755.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20868.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20992.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20949.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20876.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20993.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20615.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20630.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20645.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20533.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20867.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20685.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20639.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20907.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20882.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20813.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20695.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20641.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20838.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20722.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20699.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20967.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20520.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20781.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20885.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20837.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20960.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20796.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20841.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20969.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20546.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20557.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20942.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20784.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20912.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20790.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20940.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20529.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20909.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20941.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20583.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20752.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20965.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20593.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20915.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20616.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20592.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20653.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20697.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20714.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20579.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20831.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20783.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20773.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20873.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20651.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20931.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20947.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20718.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20798.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20669.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20619.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20732.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20584.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20673.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20964.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20822.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20934.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20849.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20797.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20537.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20757.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20677.txt',\n",
       " './data/20news-bydate-v3/20news-bydate-train/alt.atheism/20719.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_list = next(os.walk(in_dir))[2]\n",
    "news_list = [os.path.join(in_dir, f) for f in news_list]\n",
    "news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/leoqaz12/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/leoqaz12/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "| dictionary: 50264 types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (decoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerSentenceEncoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\n",
    "roberta.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ori = 'I have posted disp135.zip to alt.binaries.pictures.utilities ****** You may distribute this program freely for non-commercial use if no fee is gained. ****** There is no warranty. The author is not responsible for any damage caused by this program. Important changes since version 1,30: Fix bugs in file management system (file displaying). Improve file management system (more user-friendly). Fix bug in XPM version 3 reading. Fix bugs in TARGA reading/writng. Fix bug in GEM/IMG reading. Add support for PCX and GEM/IMG writing. Auto-skip macbinary header. (1) Introduction: This program can let you READ, WRITE and DISPLAY images with different formats. It also let you do some special effects(ROTATION, DITHERING ....) on image. Its main purpose is to let you convert image among different formts. Include simple file management system. Support \\'slide show\\'. There is NO LIMIT on image size. Currently this program supports 8, 15, 16, 24 bits display. If you want to use HiColor or TrueColor, you must have VESA driver. If you want to modify video driver, please read section (8). (2) Hardware Requirement: PC 386 or better. MSDOS 3,3 or higher. min amount of ram is 4M bytes(Maybe less memory will also work). (I recommend min 8M bytes for better performance). Hard disk for swapping(virtual memory). The following description is borrowed from DJGPP. Supported Wares: * Up to 128M of extended memory (expanded under VCPI) * Up to 128M of disk space used for swapping * SuperVGA 256-color mode up to 1024x768 * 80387 * XMS & VDISK memory allocation strategies * VCPI programs, such as QEMM, DESQview, and 386MAX Unsupported: * DPMI * Microsoft Windows Features: 80387 emulator, 32-bit unix-ish environment, flat memory model, SVGA graphics. (3) Installation: Video drivers, emu387 and go32.exe are borrowed from DJGPP. (If you use Western Digital VGA chips, read readme.wd) (This GO32.EXE is a modified version for vesa and is COMPLETELY compatible with original version) + *** But some people report that this go32.exe is not compatible with + other DJGPP programs in their system. If you encounter this problem, + DON\\'T put go32.exe within search path. *** Please read runme.bat for how to run this program. If you choose xxxxx.grn as video driver, add \\'nc 256\\' to environment GO32. For example, go32=driver x:/xxxxx/xxxxx.grn nc 256 If you don\\'t have 80x87, add \\'emu x:/xxxxx/emu387\\' to environment GO32. For example, go32=driver x:/xxxxx/xxxxx.grd emu x:/xxxxx/emu387 **** Notes: 1. I only test tr8900.grn, et4000.grn and vesa.grn. Other drivers are not tested. 2. I have modified et4000.grn to support 8, 15, 16, 24 bits display. You don\\'t need to use vesa driver. If et4000.grn doesn\\'t work, please try vesa.grn. 3. For those who want to use HiColor or TrueColor display, please use vesa.grn(except et4000 users). You can find vesa BIOS driver from : wuarchive.wustl.edu: /mirrors/msdos/graphics godzilla.cgl.rmit.oz.au: /kjb/MGL (4) Command Line Switch: + Usage : display [-d|--display initial_display_type] + [-s|--sort sort_method] + [-h|-?] Display type: 8(SVGA,default), 15, 16(HiColor), 24(TrueColor) + Sort method: \\'name\\', \\'ext\\' (5) Function Key: F2 : Change disk drive + CTRL-A -- CTRL-Z : change disk drive. F3 : Change filename mask (See match.doc) F4 : Change parameters F5 : Some effects on picture, eg. flip, rotate .... F7 : Make Directory t : Tag file + : Tag group files (See match.doc) T : Tag all files u : Untag file - : Untag group files (See match.doc) U : Untag all files Ins : Change display type (8,15,16,24) in \\'read\\' & \\'screen\\' menu. F6,m,M : Move file(s) F8,d,D : Delete file(s) r,R : Rename file c,C : Copy File(s) z,Z : Display first 10 bytes in Ascii, Hex and Dec modes. + f,F : Display disk free space. Page Up/Down : Move one page TAB : Change processing target. Arrow keys, Home, End, Page Up, Page Down: Scroll image. Home: Left Most. End: Right Most. Page Up: Top Most. Page Down: Bottom Most. in \\'screen\\' & \\'effect\\' menu : Left,Right arrow: Change display type(8, 15, 16, 24 bits) s,S : Slide Show. ESCAPE to terminate. ALT-X : Quit program without prompting. + ALT-A : Reread directory. Escape : Abort function and return. (6) Support Format: Read: GIF(.gif), Japan MAG(.mag), Japan PIC(.pic), Sun Raster(.ras), Jpeg(.jpg), XBM(.xbm), Utah RLE(.rle), PBM(.pbm), PGM(.pgm), PPM(.ppm), PM(.pm), PCX(.pcx), Japan MKI(.mki), Tiff(.tif), Targa(.tga), XPM(.xpm), Mac Paint(.mac), GEM/IMG(.img), IFF/ILBM(.lbm), Window BMP(.bmp), QRT ray tracing(.qrt), Mac PICT(.pct), VIS(.vis), PDS(.pds), VIKING(.vik), VICAR(.vic), FITS(.fit), Usenix FACE(.fac). the extensions in () are standard extensions. Write: GIF, Sun Raster, Jpeg, XBM, PBM, PGM, PPM, PM, Tiff, Targa, XPM, Mac Paint, Ascii, Laser Jet, IFF/ILBM, Window BMP, + Mac PICT, VIS, FITS, FACE, PCX, GEM/IMG. All Read/Write support full color(8 bits), grey scale, b/w dither, and 24 bits image, if allowed for that format. (7) Detail: Initialization: Set default display type to highest display type. Find allowable screen resolution(for .grn video driver only). 1. When you run this program, you will enter \\'read\\' menu. Whthin this menu you can press any function key except F5. If you move or copy files, you will enter \\'write\\' menu. the \\'write\\' menu is much like \\'read\\' menu, but only allow you to change directory. + The header line in \\'read\\' menu includes \"(d:xx,f:xx,t:xx)\". + d : display type. f: number of files. t: number of tagged files. pressing SPACE in \\'read\\' menu will let you select which format to use for reading current file. pressing RETURN in \\'read\\' menu will let you reading current file. This program will automatically determine which format this file is. The procedure is: First, check magic number. If fail, check standard extension. Still fail, report error. pressing s or S in \\'read\\' menu will do \\'Slide Show\\'. If delay time is 0, program will wait until you hit a key (except ESCAPE). If any error occurs, program will make a beep. ESCAPE to terminate. pressing Ins in \\'read\\' menu will change display type. pressing ALT-X in \\'read\\' menu will quit program without prompting. 2. Once image file is successfully read, you will enter \\'screen\\' menu. Within this menu F5 is turn on. You can do special effect on image. pressing RETURN: show image. in graphic mode, press RETURN, SPACE or ESCAPE to return to text mode. pressing TAB: change processing target. This program allows you to do special effects on 8-bit or 24-bit image. pressing Left,Right arrow: change display type. 8, 15, 16, 24 bits. pressing SPACE: save current image to file. B/W Dither: save as black/white image(1 bit). Grey Scale: save as grey image(8 bits). Full Color: save as color image(8 bits). True Color: save as 24-bit image. This program will ask you some questions if you want to write image to file. Some questions are format-dependent. Finally This program will prompt you a filename. If you want to save file under another directory other than current directory, please press SPACE. after pressing SPACE, you will enter \\'write2\\' menu. You can change directory to what you want. Then, pressing SPACE: this program will prompt you \\'original\\' filename. pressing RETURN: this program will prompt you \\'selected\\' filename (filename under bar). 3. This program supports 8, 15, 16, 24 bits display. 4. This Program is MEMORY GREEDY. If you don\\'t have enough memory, the performance is poor. 5. If you want to save 8 bits image : try GIF then TIFF(LZW) then TARGA then Sun Raster then BMP then ... If you want to save 24 bits image (lossless): try TIFF(LZW) or TARGA or ILBM or Sun Raster (No one is better for true 24bits image) 6. I recommend Jpeg for storing 24 bits images, even 8 bits images. 7. Not all subroutines are fully tested 8. This document is not well written. If you have any PROBLEM, SUGGESTION, COMMENT about this program, Please send to u7711501@bicmos.ee.nctu.edu.tw (140,113,11,13). I need your suggestion to improve this program. (There is NO anonymous ftp on this site) (8) Tech. information: Program (user interface and some subroutines) written by Jih-Shin Ho. Some subroutines are borrowed from XV(2,21) and PBMPLUS(dec 91). Tiff(V3,2) and Jpeg(V4) reading/writing are through public domain libraries. Compiled with DJGPP. You can get whole DJGPP package from SIMTEL20 or mirror sites. For example, wuarchive.wustl.edu: /mirrors/msdos/djgpp (9) For Thoese who want to modify video driver: 1. get GRX source code from SIMTEL20 or mirror sites. 2. For HiColor and TrueColor: 15 bits : # of colors is set to 32768. 16 bits : # of colors is set to 0xc010. 24 bits : # of colors is set to 0xc018. Acknowledgment: I would like to thank the authors of XV and PBMPLUS for their permission to let me use their subroutines. Also I will thank the authors who write Tiff and Jpeg libraries. Thank DJ. Without DJGPP I can\\'t do any thing on PC.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tokens exceeds maximum length: 2550 > 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d50bf0dfd79e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(len(tokens) , tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#roberta.decode(tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features_aligned_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_ori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/roberta/hub_interface.py\u001b[0m in \u001b[0;36mextract_features_aligned_to_words\u001b[0;34m(self, sentence, return_all_hiddens)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# extract features and align them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_toks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0maligned_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignment_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_features_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malignment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/roberta/hub_interface.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, tokens, return_all_hiddens)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             raise ValueError('tokens exceeds maximum length: {} > {}'.format(\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             ))\n\u001b[1;32m     84\u001b[0m         features, extra = self.model(\n",
      "\u001b[0;31mValueError\u001b[0m: tokens exceeds maximum length: 2550 > 512"
     ]
    }
   ],
   "source": [
    "# tokens_ori = train_df.loc[3,'preprocess_text']#'how\\'s are you'#\n",
    "#tokens = roberta.encode(tokens_ori)\n",
    "# assert tokens.tolist() == [0, 31414, 232, 328, 2]\n",
    "# assert roberta.decode(tokens) == 'Hello world!'\n",
    "#print(len(tokens) , tokens)\n",
    "#roberta.decode(tokens)\n",
    "doc = roberta.extract_features_aligned_to_words(tokens_ori)\n",
    "print(len(doc))\n",
    "for tok in doc:\n",
    "    print('{:10}{} (...)'.format(str(tok), tok.vector[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, ['I', 'said', ',', '\"', 'hello', 'RoBERTa', '.', '\"'])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = 'I said, \"hello RoBERTa.\"'\n",
    "tokens = sp_tokenizer(tokens)\n",
    "token_li = []\n",
    "for token in tokens:\n",
    "    token_li.append(token.text)\n",
    "len(token_li) , token_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tokens exceeds maximum length: 592 > 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-7a2d631178ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast_layer_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_layer_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# == torch.Size([1, 5, 1024])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/roberta/hub_interface.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, tokens, return_all_hiddens)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             raise ValueError('tokens exceeds maximum length: {} > {}'.format(\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             ))\n\u001b[1;32m     84\u001b[0m         features, extra = self.model(\n",
      "\u001b[0;31mValueError\u001b[0m: tokens exceeds maximum length: 592 > 512"
     ]
    }
   ],
   "source": [
    "last_layer_features = roberta.extract_features(tokens)\n",
    "print(last_layer_features.size())# == torch.Size([1, 5, 1024])\n",
    "\n",
    "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)\n",
    "assert len(all_layers) == 25\n",
    "assert torch.all(all_layers[-1] == last_layer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 19, 1024)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer_features.detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 898823/898823 [00:06<00:00, 134563.64B/s]\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 545019.70B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0925 15:40:29.729804 139833327294272 tokenization_utils.py:665] Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "590"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(train_df.loc[0,'text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
